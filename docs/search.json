[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Apresentação",
    "section": "",
    "text": "Este é um website baseado no Quarto da empresa Posit.\nTemplates úteis: https://drganghe.github.io/quarto-academic-site-examples.html\nSite oficial do Quarto: https://quarto.org/docs/websites.\nPara rodar códigos em Python, é necessário instalar as bibliotecas usando o reticulate e ter o jupyter instalado.\nExemplo:\nreticulate::py_require(\"matplotlib\")\n\nAlém disso, cuidado ao setar engine jupyter que daí os códigos R não serão rodados.\nEngine Conflicts:\n\nKnitr vs. Jupyter: If you explicitly set the engine to jupyter in your YAML, R code chunks will not be executed, and only Python (or the kernel’s language) will run. For mixed R and Python, engine: knitr is typically used, relying on reticulate for Python execution.",
    "crumbs": [
      "Sobre"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre",
    "section": "",
    "text": "This is a Quarto website.\nTemplates úteis: https://drganghe.github.io/quarto-academic-site-examples.html\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\nTo add a plotly graph:\nytd library(plotly) fig &lt;- plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length) fig}"
  },
  {
    "objectID": "LGN.html",
    "href": "LGN.html",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "Este é uma página da LGN."
  },
  {
    "objectID": "TCL.html",
    "href": "TCL.html",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Este é uma página do TCL."
  },
  {
    "objectID": "Probabilidade/TCL.html",
    "href": "Probabilidade/TCL.html",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "O Teorema Central do Limite (TCL) é um dos resultados mais famosos e mais bonitos da Teoria da Probabilidade.\nSejam \\(X_1, X_2, ...\\) variáveis aleatórias independentes e identicamente distribuídas (\\(iid\\)) com média e variância existentes e finitas. Isto é:\n\\(-\\infty &lt; \\mu &lt; +\\infty\\)\n\\(0 &lt; \\sigma^2 &lt; +\\infty\\)\nAlém disso, defina \\(\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n}\\).\nPela Lei dos Grandes Números, sabemos que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\), mas a qual taxa? Como que é a distribuição dessa quantidade?\n\n\nDefinição.  Sejam \\(X, X_1, X_2, ...\\) variáveis aleatórias com, respectivamente, funções de distribuições \\(F, F_1, F_2, ...\\). Dizemos que \\(X_n\\) converge em distribuição para \\(X\\), quando \\(n \\rightarrow +\\infty\\), se \\(F_n(x) \\rightarrow F(x)\\) para todo \\(x\\) ponto de continuidade de \\(F\\).\nNotação:  \\(X_n \\xrightarrow{D} X\\) ou \\(X_n \\xrightarrow{D} F\\).\n\n\n\nPodemos reescrever a quantidade da introdução como sendo \\(\\bar{X} - \\mu \\xrightarrow{n \\rightarrow + \\infty} 0\\). Assim, uma das maneiras de buscar entender a distribuição de uma quantidade quando ela vai zero é multiplicá-la por uma quantidade que vai para infinito de uma maneira que seu valor de convergência não “exploda” para o infinito e nem para zero. De fato se multiplicarmos a quantidade acima por \\(n\\) elevando-o à um exponencial buscando controlar as taxas de convergência. Neste caso, o exponencial é o valor de \\(\\frac{1}{2}\\), ou seja, \\(\\sqrt{n}\\).1\nLogo, o TCL pode ser descrito por:\n\\(\\sqrt{n} \\left(\\bar{X} - \\mu \\right) \\xrightarrow{n \\rightarrow + \\infty} N(0, \\sigma^{2})\\) em distribuição\nOu, alternativamente, na sua forma mais popular:\n\\[\n\\sqrt{n} \\left(\\frac{\\bar{X} - \\mu}{\\sigma} \\right) \\xrightarrow{D} N(0, 1)\n\\tag{1}\\]\nO TCL retrata um dos teoremas mais lindos de toda a probabilidade tendo em vista que com poucas condições iniciais (neste caso, média e variância finitas) conseguimos provar que a média amostral \\(\\bar{X}\\) padronizada converge em distribuição para a distribuição normal padrão. Observe que o teorema não faz nenhuma alusão ao tipo de variável aleatória, podendo ser discreto ou contínuo, e nem sobre o suporte da distribuição, podendo ser positivo, negativo ou ambos.\nEste teorema é um dos principais motivos pela ampla difusão da distribuição Normal em diversas áreas do conhecimento científico. Tendo em vista que, dadas às devidas condições, a média amostral pode ser aproximada pela distribuição normal padrão, as aplicações desses resultados são diversos.\nProva. Faça \\(S_n = X_1 + X_2 + ... +X_n\\) e multiplicando e dividindo por \\(n\\) o termo do Equação 1 chegamos em\n\\[\n\\frac{1}{\\sqrt{n}}\\left(\\frac{S_n - n\\mu}{\\sigma} \\right) = \\frac{1}{\\sqrt{n}}\\left(\\frac{\\sum_{i=1}^nX_i- n\\mu}{\\sigma} \\right) \\overset{\\text{Expandindo o somatório}}{=} \\frac{1}{\\sqrt{n}}\\sum_{i=1}^n\\left(\\frac{X_i- \\mu}{\\sigma} \\right) \\xrightarrow{n \\rightarrow + \\infty} N(0, 1)\\ \\text{em distribuição}\n\\] Podemos assumir, sem perda de generalidade que \\(\\mu = 0\\) e \\(\\sigma = 1\\), pois poderíamos fazer a prova definindo uma variável aleatória sendo \\(Z_i = \\frac{X_i- \\mu}{\\sigma}\\), onde esta variável teria média 0 e desvio padrão 1. Sendo assim, basta fazer a prova para a quantidade:\n\\(\\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}} \\xrightarrow{D} N(0, 1)\\)\nPara continuar a prova, faremos o uso da função geradora de momentos (M) da quantidade acima e avaliando o seu valor quanto \\(n \\rightarrow +\\infty\\) a fim de identificar qual a distribuição limite obtida.2 Ou seja:\n\\(M_{X_n}(x) \\xrightarrow{n \\rightarrow + \\infty} M_{X}(x) \\implies X_n \\xrightarrow{D} X\\)\nPor notação, vamos estabelecer \\(Q_n = \\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}}\\) e aplicar o teorema acima:\n\\(M_{Q_n}(t) = E\\left( e^{t\\left(\\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}}\\right)} \\right) \\overset{\\text{Prop. de Exponencial}}{=} E\\left( e^{\\frac{tX_1}{\\sqrt{n}}} \\times e^{\\frac{tX_2}{\\sqrt{n}}} \\times \\dots \\times e^{\\frac{tX_n}{\\sqrt{n}}} \\right) \\overset{\\text{Independência}}{=} \\prod_{i=1}^{n}E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right)\\)\nComo todos os \\(X_i\\) são identicamente distribuídos, podemos simplificar a expressão para:\n\\(M_{Q_n}(t) = \\prod_{i=1}^{n}E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right) = E\\left( e^{\\frac{tX_1}{\\sqrt{n}}} \\right) \\times E\\left( e^{\\frac{tX_2}{\\sqrt{n}}} \\right) \\times \\dots \\times E\\left( e^{\\frac{tX_n}{\\sqrt{n}}} \\right) = \\left( E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right) \\right)^n, \\ \\forall i\\)\nNo entanto, note acima que \\(E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right)\\) é exatamente a função geradora de momentos aplicada no ponto \\(\\frac{t}{\\sqrt(n)}\\). Logo:\n\\(M_{Q_n}(t) = \\left( M_{X_i}\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n, \\ \\forall i\\)\nAssim, por simplificação de notação vamos suprimir o \\(X_i\\) nos passos seguintes e vamos avaliar esta expressão o que acontece quando \\(n \\rightarrow +\\infty\\).\nObserve que da maneira como está estruturado \\(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n\\) converge para uma indefinição do tipo \\(1^\\infty\\), pois \\(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n = \\left( E\\left( e^{\\frac{tX}{\\sqrt{n}}} \\right) \\right)^n \\rightarrow \\left( E\\left( e^0 \\right) \\right)^\\infty\\).\nPara tratar essa indefinição, aplicaremos o logaritmo natural (\\(log\\)) na expressão e, depois de avaliado o limite, podemos reverter o valor aplicando o exponencial. Logo,\n\\(log\\left(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n \\right) = n\\times log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)\\)\nQuando aplicamos \\(n \\rightarrow +\\infty\\) caímos numa indefinição do tipo \\(+\\infty \\times 0\\), logo vamos reescrever a equação para caírmos numa indefinição do tipo \\(\\frac{0}{0}\\) para aplicarmos l’Hôpital:\n\\(\\frac{log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)}{\\frac{1}{n}}\\)\nComo o \\(n\\) é um número inteiro natural e para melhor tratativa algébrica, vamos fazer uma transforção de variável para facilitar o desenvolvimento e poder aplicar a derivada em um número real. Assim,\n\\[\n\\lim_{n\\to\\infty}\\frac{log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)}{\\frac{1}{n}} \\overset{y=\\frac{1}{\\sqrt(n)}, \\ y \\in \\mathbb{R} }{=} \\lim_{y\\to 0}\\frac{log\\left( M\\left( yt \\right) \\right)}{y^2}\n\\]\nQue é uma indefinição do tipo \\(\\frac{0}{0}\\). Logo, aplicando l’Hôpital:3\n\\[\n\\lim_{y\\to 0}\\frac{tM'(yt)}{M(yt)}\\frac{1}{2y}\n\\]\nEste limite continua sendo uma indefinição do tipo \\(\\frac{0}{0}\\), porque pela definição da função geradora do momento, quando aplicamos as suposições no ponto \\(t=0\\) temos:\n\\[\n\\begin{aligned}\nM(t) = E\\left(e^{tX}\\right) \\implies M(0) = 1 \\\\\n\\mu = 0 \\implies M'(0) = 0 \\\\\n\\sigma^2 = 1 \\implies M''(0) = 1\n\\end{aligned}\n\\] Logo, simplificando os limites e aplicando l’Hôpital novamente temos:\n\\[\n\\lim_{y\\to 0}\\frac{tM'(yt)}{M(yt)}\\frac{1}{2y} \\overset{\\lim_{y\\to 0}{M(yt)}=1}{=} \\lim_{y\\to 0}\\frac{tM'(yt)}{2y} = \\lim_{y\\to 0}\\frac{t^2M''(yt)}{2} = \\frac{t^2}{2} \\lim_{y\\to 0}M''(yt) = \\frac{t^2}{2}\n\\]\nAplicando o exponencial para reverter o logaritmo aplicado originalmente temos:\n\\(M_{Q_n}(t) \\xrightarrow{n \\rightarrow +\\infty} e^{\\frac{t^2}{2}}\\)\nQue coindide com a função geradora de momentos da Normal Padrão.\n\\(\\square\\)\nUma discussão mais aprofundada do Teorema Central do Limite pode ser encontrada no Capítulo 7 de James (2023).\n\n\n\nFazendo \\(S_n = X_1 + X_2 + ... +X_n\\), James (2023) trata o problema central do limite atavés do estudo da convergência em distribuição das somas parciais normalizadas e formula o TCL como sendo:\n\\(\\frac{{S}_n - E({S}_n)}{\\sqrt{Var(S_n)}} \\xrightarrow{D} N(0, 1)\\).\nOutras maneiras de representar o TCL são:\n\\(\\frac{\\bar{X} - E(\\bar{X})}{\\sqrt{Var(\\bar{X})}} \\xrightarrow{D} N(0, 1)\\)\n\\(\\sqrt{n} \\bar{X} \\xrightarrow{D} N(\\mu, \\sigma^2)\\)\n\n\n\n\n\n\nEsta aproximação, também conhecida como Teorema de De Moivre–Laplace, …",
    "crumbs": [
      "Probabilidade",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html",
    "href": "Probabilidade/LGN.html",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei dos Grandes Números (LGN) é um dos resultados mais famosos da Teoria da Probabilidade.\nSejam \\(X_1, X_2, ...\\) variáveis aleatórias independentes e identicamente distribuídas (\\(iid\\)) com média e variância existentes e finitas. Isto é:\n\\(-\\infty &lt; \\mu &lt; +\\infty\\)\n\\(0 &lt; \\sigma^2 &lt; +\\infty\\)\nA primeira condição de finitude das médias é necessária para a Lei Forte dos Grandes Números.\nAlém disso, defina \\(\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n}\\).\n\n\nA Lei Forte estabelece que:\n\\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) com probabilidade 1.\nObserve que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) é um evento. Logo, a Lei Forte poderia ser reescrita com uma notação probabilística alternativa:\n\\(P\\left(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu \\right) = 1\\)\nEste resultado estabelece que \\(\\bar{X}\\) converge quase certamente para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, no infinito, \\(\\bar{X}\\) e \\(\\mu\\) serão iguais. Ou seja, é uma convergência pontual do valor de uma variável aleatória para uma constante (a média populacional).\nA prova deste teorema implica em provar que o evento acima é um evento de probabilidade 1, o que exige maior formalidade matemática fazendo uso do Lema de Borel-Cantelli.\nProva. Ver Cáp. 5 de James (2023).\n\\(\\square\\)\n\n\n\nA Lei Fraca estabelece que \\(\\forall \\varepsilon &gt; 0\\), então:\n\\(P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\xrightarrow{n \\rightarrow + \\infty} 0\\)\nEste resultado estabelece que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\).\nProva. A prova faz uso da Desigualdade de Chebyshev:\n\\(P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\overset{\\text{Des. Cheb.}}{\\le} \\frac{Var(\\bar{X})}{\\varepsilon^2} = \\frac{Var\\left(\\frac{\\sum_{i=1}^{n}{X_i}}{n}\\right)}{\\varepsilon^2} \\overset{\\text{Prop. da Constante}}{=} \\frac{Var(\\sum_{i=1}^{n}{X_i})}{n^2\\varepsilon^2} \\overset{\\text{Indep.}}{=} \\frac{\\sum_{i=1}^{n}{Var(X_i)}}{n^2\\varepsilon^2} \\overset{\\text{Ident. Dist.}}{=} \\frac{n\\sigma^2}{n^2\\varepsilon^2} = \\frac{\\sigma^2}{n\\varepsilon^2} \\xrightarrow{n \\rightarrow + \\infty} 0\\)\n\\(\\square\\)\nEste resultado ilustra que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, é extremamente improvável que a diferença entre \\(\\bar{X}\\) e \\(\\mu\\) seja maior que \\(\\varepsilon\\).\n\n\n\nÉ intuitivo pensar afirmarmos que duas quantidades são iguais é mais “forte” do que dizer que essas duas quantidades são “altamente prováveis de estarem próximas”. Ora, se uma coisa é igual à outra, isso implica de que elas também são próximas. Tendo em vista que a Lei Forte dos Grandes Números representa a convergência quase certa e a Lei Fraca representa a convergência em probabilidade, temos que se\n\\(Convergência\\ Quase\\ Certa \\implies Convergência\\ em\\ Probabilidade\\)\nentão,\n\\(Lei\\ Forte \\implies Lei\\ Fraca\\)\nO contrário não vale para nenhuma das duas afirmativas acima.\n\n\n\nSuponha uma sequência independente de lançamentos de uma moeda honesta e a variável aleatória \\(X_i = 1\\) se cara e \\(X_i = 0\\) se coroa. Ou seja,\n\\(X_i \\overset{iid}{\\sim}Bernoulli(p)\\)\nonde \\(p = 0,5\\). Então, pela Lei Forte dos Grandes Números:\n\\(\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n} \\xrightarrow{n \\rightarrow + \\infty} p\\)\nEste resultado mostra que a medida que o jogarmos a moeda infinitas vezes, iremos convergir para um cenário em que metade dos lançamentos serão caras e metade serão coroas. Note que o resultado vale quando \\(n \\rightarrow + \\infty\\), ou seja, ele não estabelece nada em uma quantidade finita de lançamentos onde pode existir variabilidade. Por exemplo, mesmo que seja altamente improvável que nos primeiros 100 lançamentos todos os resultados sejam a face “Cara”, não existe nada matematicamente que estabeleça que isso seja impossível. No entanto, no limite do infinito as quantidades iniciais serão “engolidas” pela LGN. Para uma discussão mais aprofundada sobre esse tema recomenda-se uma leitura sobre o Gambler’s Fallacy.\n\n\n\nNo R:\n\n\nMostrar Código\n# Carregar as bibliotecas necessárias\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Definir o número de lançamentos\nn &lt;- 500\n\n# Simular lançamentos de uma moeda honesta\nset.seed(123) # Para reprodutibilidade\nlancamentos &lt;- sample(c(\"Cara\", \"Coroa\"), n, replace = TRUE)\n\n# Calcular a proporção acumulada de caras\ndados &lt;- data.frame(lancamentos) %&gt;%\n  mutate(\n    n = row_number(),\n    proporcao_cara = cumsum(lancamentos == \"Cara\") / n\n  )\n\n# Criar o gráfico\nggplot(dados, aes(x = n, y = proporcao_cara)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Lei dos Grandes Números: Lançamento de uma Moeda Honesta\",\n       x = \"Número de Lançamentos\",\n       y = \"Proporção de Caras\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definindo o número de lançamentos\nn = 500\n\n# Simulando lançamentos de uma moeda honesta\nnp.random.seed(123)  # Para reprodutibilidade\nlancamentos = np.random.choice(['Cara', 'Coroa'], size=n)\n\n# Calculando a proporção acumulada de caras\nproporcao_cara = np.cumsum(lancamentos == 'Cara') / np.arange(1, n + 1)\n\n# Criando o gráfico\nplt.figure(figsize=(10, 5))\nplt.plot(proporcao_cara, color='blue', label='Proporção de Caras')\nplt.axhline(y=0.5, color='red', linestyle='--', label='Proporção Esperada (0.5)')\nplt.title('Lei dos Grandes Números: Lançamento de uma Moeda Honesta')\nplt.xlabel('Número de Lançamentos')\nplt.ylabel('Proporção de Caras')\nplt.legend()\nplt.grid()\nplt.show()",
    "crumbs": [
      "Probabilidade",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#seção-2",
    "href": "Probabilidade/LGN.html#seção-2",
    "title": "Lei dos Grandes Números",
    "section": "Seção 2",
    "text": "Seção 2\nColocando uma equação:\n\\(E = mc^2\\)\nE colocando um ggplot2:\n\n\nMostrar Código\n# Definindo parâmetros\nset.seed(123)           # Para reprodutibilidade\nn_lancamentos &lt;- 1000  # Número total de lançamentos\n\n# Simulando lançamentos: 1 representa \"cara\", 0 representa \"coroa\"\nresultados &lt;- sample(c(0, 1), size = n_lancamentos, replace = TRUE)\n\n# Calculando a média acumulada (frequência relativa de \"cara\")\nmedia_acumulada &lt;- cumsum(resultados) / (1:n_lancamentos)\n\n# Valor esperado teórico\nvalor_esperado &lt;- 0.5\n\n# Plotando o gráfico\nplot(media_acumulada, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Número de lançamentos\", ylab = \"Frequência relativa de 'cara'\",\n     main = \"Lei dos Grandes Números - Moeda Honesta\")\nabline(h = valor_esperado, col = \"red\", lty = 2, lwd = 2)\nlegend(\"bottomright\", legend = c(\"Frequência relativa\", \"Valor esperado (0.5)\"),\n       col = c(\"blue\", \"red\"), lty = c(1, 2), lwd = 2)\n\n\n\n\n\n\n\n\n\n\nSeção 3\nEla tem também um table of contents.\nEste é uma página da LGN citando .\n\n\nLinks Úteis:\nAula de Harvard de Lei dos Grandes Números e Teorema Central do Limite\nPágina da Wikipedia da Lei dos Grandes Números",
    "crumbs": [
      "Probabilidade",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#lei-forte-dos-grande-números",
    "href": "Probabilidade/LGN.html#lei-forte-dos-grande-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Forte estabelece que:\n\\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) com probabilidade 1.\nObserve que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) é um evento. Logo, a Lei Forte poderia ser reescrita com uma notação probabilística alternativa:\n\\(P\\left(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu \\right) = 1\\)\nEste resultado estabelece que \\(\\bar{X}\\) converge quase certamente para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, no infinito, \\(\\bar{X}\\) e \\(\\mu\\) serão iguais. Ou seja, é uma convergência pontual do valor de uma variável aleatória para uma constante (a média populacional).\nA prova deste teorema implica em provar que o evento acima é um evento de probabilidade 1, o que exige maior formalidade matemática fazendo uso do Lema de Borel-Cantelli.\nProva. Ver Cáp. 5 de James (2023).",
    "crumbs": [
      "Probabilidade",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#lei-fraca-dos-grande-números",
    "href": "Probabilidade/LGN.html#lei-fraca-dos-grande-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Fraca estabelece que \\(\\forall \\varepsilon &gt; 0\\), então:\n\\(P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\xrightarrow{n \\rightarrow + \\infty} 0\\)\nEste resultado estabelece que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\).\nProva. A prova faz uso da Desigualdade de Chebyshev:\n\\(P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\overset{\\text{Des. Cheb.}}{\\le} \\frac{Var(\\bar{X})}{\\varepsilon^2} = \\frac{Var\\left(\\frac{\\sum_{i=1}^{n}{X_i}}{n}\\right)}{\\varepsilon^2} \\overset{\\text{Prop. da Constante}}{=} \\frac{Var(\\sum_{i=1}^{n}{X_i})}{n^2\\varepsilon^2} \\overset{\\text{Indep.}}{=} \\frac{\\sum_{i=1}^{n}{Var(X_i)}}{n^2\\varepsilon^2} \\overset{\\text{Ident. Dist.}}{=} \\frac{n\\sigma^2}{n^2\\varepsilon^2} = \\frac{\\sigma^2}{n\\varepsilon^2} \\xrightarrow{n \\rightarrow + \\infty} 0\\)\nEste resultado ilustra que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, é extremamente improvável que a diferença entre \\(\\bar{X}\\) e \\(\\mu\\) seja maior que \\(\\varepsilon\\).",
    "crumbs": [
      "Probabilidade",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#exemplo-computacional",
    "href": "Probabilidade/LGN.html#exemplo-computacional",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "No R:\n\n\nMostrar Código\n# Carregar as bibliotecas necessárias\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Definir o número de lançamentos\nn &lt;- 500\n\n# Simular lançamentos de uma moeda honesta\nset.seed(123) # Para reprodutibilidade\nlancamentos &lt;- sample(c(\"Cara\", \"Coroa\"), n, replace = TRUE)\n\n# Calcular a proporção acumulada de caras\ndados &lt;- data.frame(lancamentos) %&gt;%\n  mutate(\n    n = row_number(),\n    proporcao_cara = cumsum(lancamentos == \"Cara\") / n\n  )\n\n# Criar o gráfico\nggplot(dados, aes(x = n, y = proporcao_cara)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Lei dos Grandes Números: Lançamento de uma Moeda Honesta\",\n       x = \"Número de Lançamentos\",\n       y = \"Proporção de Caras\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definindo o número de lançamentos\nn = 500\n\n# Simulando lançamentos de uma moeda honesta\nnp.random.seed(123)  # Para reprodutibilidade\nlancamentos = np.random.choice(['Cara', 'Coroa'], size=n)\n\n# Calculando a proporção acumulada de caras\nproporcao_cara = np.cumsum(lancamentos == 'Cara') / np.arange(1, n + 1)\n\n# Criando o gráfico\nplt.figure(figsize=(10, 5))\nplt.plot(proporcao_cara, color='blue', label='Proporção de Caras')\nplt.axhline(y=0.5, color='red', linestyle='--', label='Proporção Esperada (0.5)')\nplt.title('Lei dos Grandes Números: Lançamento de uma Moeda Honesta')\nplt.xlabel('Número de Lançamentos')\nplt.ylabel('Proporção de Caras')\nplt.legend()\nplt.grid()\nplt.show()",
    "crumbs": [
      "Probabilidade",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#relação-entre-leis-e-convergências",
    "href": "Probabilidade/LGN.html#relação-entre-leis-e-convergências",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "É intuitivo pensar afirmarmos que duas quantidades são iguais é mais “forte” do que dizer que essas duas quantidades são “altamente prováveis de estarem próximas”. Ora, se uma coisa é igual à outra, isso implica de que elas também são próximas. Tendo em vista que a Lei Forte dos Grandes Números representa a convergência quase certa e a Lei Fraca representa a convergência em probabilidade, temos que se\n\\(Convergência\\ Quase\\ Certa \\implies Convergência\\ em\\ Probabilidade\\)\nentão,\n\\(Lei\\ Forte \\implies Lei\\ Fraca\\)\nO contrário não vale para nenhuma das duas afirmativas acima.",
    "crumbs": [
      "Probabilidade",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#exemplo-o-caso-bernoulli",
    "href": "Probabilidade/LGN.html#exemplo-o-caso-bernoulli",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "Suponha uma sequência independente de lançamentos de uma moeda honesta e a variável aleatória \\(X_i = 1\\) se cara e \\(X_i = 0\\) se coroa. Ou seja,\n\\(X_i \\overset{iid}{\\sim}Bernoulli(p)\\)\nonde \\(p = 0,5\\). Então, pela Lei Forte dos Grandes Números:\n\\(\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n} \\xrightarrow{n \\rightarrow + \\infty} p\\)\nEste resultado mostra que a medida que o jogarmos a moeda infinitas vezes, iremos convergir para um cenário em que metade dos lançamentos serão caras e metade serão coroas. Note que o resultado vale quando \\(n \\rightarrow + \\infty\\), ou seja, ele não estabelece nada em uma quantidade finita de lançamentos onde pode existir variabilidade. Por exemplo, mesmo que seja altamente improvável que nos primeiros 100 lançamentos todos os resultados sejam a face “Cara”, não existe nada matematicamente que estabeleça que isso seja impossível. No entanto, no limite do infinito as quantidades iniciais serão “engolidas” pela LGN. Para uma discussão mais aprofundada sobre esse tema recomenda-se uma leitura sobre o Gambler’s Fallacy.",
    "crumbs": [
      "Probabilidade",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#lei-forte-dos-grandes-números",
    "href": "Probabilidade/LGN.html#lei-forte-dos-grandes-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Forte estabelece que:\n\\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) com probabilidade 1.\nObserve que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) é um evento. Logo, a Lei Forte poderia ser reescrita com uma notação probabilística alternativa:\n\\(P\\left(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu \\right) = 1\\)\nEste resultado estabelece que \\(\\bar{X}\\) converge quase certamente para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, no infinito, \\(\\bar{X}\\) e \\(\\mu\\) serão iguais. Ou seja, é uma convergência pontual do valor de uma variável aleatória para uma constante (a média populacional).\nA prova deste teorema implica em provar que o evento acima é um evento de probabilidade 1, o que exige maior formalidade matemática fazendo uso do Lema de Borel-Cantelli.\nProva. Ver Cáp. 5 de James (2023).\n\\(\\square\\)",
    "crumbs": [
      "Probabilidade",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#lei-fraca-dos-grandes-números",
    "href": "Probabilidade/LGN.html#lei-fraca-dos-grandes-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Fraca estabelece que \\(\\forall \\varepsilon &gt; 0\\), então:\n\\(P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\xrightarrow{n \\rightarrow + \\infty} 0\\)\nEste resultado estabelece que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\).\nProva. A prova faz uso da Desigualdade de Chebyshev:\n\\(P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\overset{\\text{Des. Cheb.}}{\\le} \\frac{Var(\\bar{X})}{\\varepsilon^2} = \\frac{Var\\left(\\frac{\\sum_{i=1}^{n}{X_i}}{n}\\right)}{\\varepsilon^2} \\overset{\\text{Prop. da Constante}}{=} \\frac{Var(\\sum_{i=1}^{n}{X_i})}{n^2\\varepsilon^2} \\overset{\\text{Indep.}}{=} \\frac{\\sum_{i=1}^{n}{Var(X_i)}}{n^2\\varepsilon^2} \\overset{\\text{Ident. Dist.}}{=} \\frac{n\\sigma^2}{n^2\\varepsilon^2} = \\frac{\\sigma^2}{n\\varepsilon^2} \\xrightarrow{n \\rightarrow + \\infty} 0\\)\n\\(\\square\\)\nEste resultado ilustra que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, é extremamente improvável que a diferença entre \\(\\bar{X}\\) e \\(\\mu\\) seja maior que \\(\\varepsilon\\).",
    "crumbs": [
      "Probabilidade",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/TCL.html#teorema-central-do-limite",
    "href": "Probabilidade/TCL.html#teorema-central-do-limite",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Podemos reescrever a quantidade da introdução como sendo \\(\\bar{X} - \\mu \\xrightarrow{n \\rightarrow + \\infty} 0\\). Assim, uma das maneiras de buscar entender a distribuição de uma quantidade quando ela vai zero é multiplicá-la por uma quantidade que vai para infinito de uma maneira que seu valor de convergência não “exploda” para o infinito e nem para zero. De fato se multiplicarmos a quantidade acima por \\(n\\) elevando-o à um exponencial buscando controlar as taxas de convergência. Neste caso, o exponencial é o valor de \\(\\frac{1}{2}\\), ou seja, \\(\\sqrt{n}\\).1\nLogo, o TCL pode ser descrito por:\n\\(\\sqrt{n} \\left(\\bar{X} - \\mu \\right) \\xrightarrow{n \\rightarrow + \\infty} N(0, \\sigma^{2})\\) em distribuição\nOu, alternativamente, na sua forma mais popular:\n\\[\n\\sqrt{n} \\left(\\frac{\\bar{X} - \\mu}{\\sigma} \\right) \\xrightarrow{D} N(0, 1)\n\\tag{1}\\]\nO TCL retrata um dos teoremas mais lindos de toda a probabilidade tendo em vista que com poucas condições iniciais (neste caso, média e variância finitas) conseguimos provar que a média amostral \\(\\bar{X}\\) padronizada converge em distribuição para a distribuição normal padrão. Observe que o teorema não faz nenhuma alusão ao tipo de variável aleatória, podendo ser discreto ou contínuo, e nem sobre o suporte da distribuição, podendo ser positivo, negativo ou ambos.\nEste teorema é um dos principais motivos pela ampla difusão da distribuição Normal em diversas áreas do conhecimento científico. Tendo em vista que, dadas às devidas condições, a média amostral pode ser aproximada pela distribuição normal padrão, as aplicações desses resultados são diversos.\nProva. Faça \\(S_n = X_1 + X_2 + ... +X_n\\) e multiplicando e dividindo por \\(n\\) o termo do Equação 1 chegamos em\n\\[\n\\frac{1}{\\sqrt{n}}\\left(\\frac{S_n - n\\mu}{\\sigma} \\right) = \\frac{1}{\\sqrt{n}}\\left(\\frac{\\sum_{i=1}^nX_i- n\\mu}{\\sigma} \\right) \\overset{\\text{Expandindo o somatório}}{=} \\frac{1}{\\sqrt{n}}\\sum_{i=1}^n\\left(\\frac{X_i- \\mu}{\\sigma} \\right) \\xrightarrow{n \\rightarrow + \\infty} N(0, 1)\\ \\text{em distribuição}\n\\] Podemos assumir, sem perda de generalidade que \\(\\mu = 0\\) e \\(\\sigma = 1\\), pois poderíamos fazer a prova definindo uma variável aleatória sendo \\(Z_i = \\frac{X_i- \\mu}{\\sigma}\\), onde esta variável teria média 0 e desvio padrão 1. Sendo assim, basta fazer a prova para a quantidade:\n\\(\\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}} \\xrightarrow{D} N(0, 1)\\)\nPara continuar a prova, faremos o uso da função geradora de momentos (M) da quantidade acima e avaliando o seu valor quanto \\(n \\rightarrow +\\infty\\) a fim de identificar qual a distribuição limite obtida.2 Ou seja:\n\\(M_{X_n}(x) \\xrightarrow{n \\rightarrow + \\infty} M_{X}(x) \\implies X_n \\xrightarrow{D} X\\)\nPor notação, vamos estabelecer \\(Q_n = \\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}}\\) e aplicar o teorema acima:\n\\(M_{Q_n}(t) = E\\left( e^{t\\left(\\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}}\\right)} \\right) \\overset{\\text{Prop. de Exponencial}}{=} E\\left( e^{\\frac{tX_1}{\\sqrt{n}}} \\times e^{\\frac{tX_2}{\\sqrt{n}}} \\times \\dots \\times e^{\\frac{tX_n}{\\sqrt{n}}} \\right) \\overset{\\text{Independência}}{=} \\prod_{i=1}^{n}E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right)\\)\nComo todos os \\(X_i\\) são identicamente distribuídos, podemos simplificar a expressão para:\n\\(M_{Q_n}(t) = \\prod_{i=1}^{n}E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right) = E\\left( e^{\\frac{tX_1}{\\sqrt{n}}} \\right) \\times E\\left( e^{\\frac{tX_2}{\\sqrt{n}}} \\right) \\times \\dots \\times E\\left( e^{\\frac{tX_n}{\\sqrt{n}}} \\right) = \\left( E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right) \\right)^n, \\ \\forall i\\)\nNo entanto, note acima que \\(E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right)\\) é exatamente a função geradora de momentos aplicada no ponto \\(\\frac{t}{\\sqrt(n)}\\). Logo:\n\\(M_{Q_n}(t) = \\left( M_{X_i}\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n, \\ \\forall i\\)\nAssim, por simplificação de notação vamos suprimir o \\(X_i\\) nos passos seguintes e vamos avaliar esta expressão o que acontece quando \\(n \\rightarrow +\\infty\\).\nObserve que da maneira como está estruturado \\(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n\\) converge para uma indefinição do tipo \\(1^\\infty\\), pois \\(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n = \\left( E\\left( e^{\\frac{tX}{\\sqrt{n}}} \\right) \\right)^n \\rightarrow \\left( E\\left( e^0 \\right) \\right)^\\infty\\).\nPara tratar essa indefinição, aplicaremos o logaritmo natural (\\(log\\)) na expressão e, depois de avaliado o limite, podemos reverter o valor aplicando o exponencial. Logo,\n\\(log\\left(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n \\right) = n\\times log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)\\)\nQuando aplicamos \\(n \\rightarrow +\\infty\\) caímos numa indefinição do tipo \\(+\\infty \\times 0\\), logo vamos reescrever a equação para caírmos numa indefinição do tipo \\(\\frac{0}{0}\\) para aplicarmos l’Hôpital:\n\\(\\frac{log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)}{\\frac{1}{n}}\\)\nComo o \\(n\\) é um número inteiro natural e para melhor tratativa algébrica, vamos fazer uma transforção de variável para facilitar o desenvolvimento e poder aplicar a derivada em um número real. Assim,\n\\[\n\\lim_{n\\to\\infty}\\frac{log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)}{\\frac{1}{n}} \\overset{y=\\frac{1}{\\sqrt(n)}, \\ y \\in \\mathbb{R} }{=} \\lim_{y\\to 0}\\frac{log\\left( M\\left( yt \\right) \\right)}{y^2}\n\\]\nQue é uma indefinição do tipo \\(\\frac{0}{0}\\). Logo, aplicando l’Hôpital:3\n\\[\n\\lim_{y\\to 0}\\frac{tM'(yt)}{M(yt)}\\frac{1}{2y}\n\\]\nEste limite continua sendo uma indefinição do tipo \\(\\frac{0}{0}\\), porque pela definição da função geradora do momento, quando aplicamos as suposições no ponto \\(t=0\\) temos:\n\\[\n\\begin{aligned}\nM(t) = E\\left(e^{tX}\\right) \\implies M(0) = 1 \\\\\n\\mu = 0 \\implies M'(0) = 0 \\\\\n\\sigma^2 = 1 \\implies M''(0) = 1\n\\end{aligned}\n\\] Logo, simplificando os limites e aplicando l’Hôpital novamente temos:\n\\[\n\\lim_{y\\to 0}\\frac{tM'(yt)}{M(yt)}\\frac{1}{2y} \\overset{\\lim_{y\\to 0}{M(yt)}=1}{=} \\lim_{y\\to 0}\\frac{tM'(yt)}{2y} = \\lim_{y\\to 0}\\frac{t^2M''(yt)}{2} = \\frac{t^2}{2} \\lim_{y\\to 0}M''(yt) = \\frac{t^2}{2}\n\\]\nAplicando o exponencial para reverter o logaritmo aplicado originalmente temos:\n\\(M_{Q_n}(t) \\xrightarrow{n \\rightarrow +\\infty} e^{\\frac{t^2}{2}}\\)\nQue coindide com a função geradora de momentos da Normal Padrão.\n\\(\\square\\)\nUma discussão mais aprofundada do Teorema Central do Limite pode ser encontrada no Capítulo 7 de James (2023).",
    "crumbs": [
      "Probabilidade",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade/TCL.html#footnotes",
    "href": "Probabilidade/TCL.html#footnotes",
    "title": "Teorema Central do Limite",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nUma discussão sobre este fator de convergência é discutido aqui↩︎\nEste teorema é um caso particular do Teorema de Continuidade de Levy para funções características. E pode ser melhor detalhado em Curtiss (1942) ou aqui.↩︎\nLembrando que \\(\\frac{d\\log(f(x))}{dx} = \\frac{1}{f(x)} \\cdot \\frac{df(x)}{dx} = \\frac{f'(x)}{f(x)}\\)↩︎",
    "crumbs": [
      "Probabilidade",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade/TCL.html#convergência-em-distribuição",
    "href": "Probabilidade/TCL.html#convergência-em-distribuição",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Definição.  Sejam \\(X, X_1, X_2, ...\\) variáveis aleatórias com, respectivamente, funções de distribuições \\(F, F_1, F_2, ...\\). Dizemos que \\(X_n\\) converge em distribuição para \\(X\\), quando \\(n \\rightarrow +\\infty\\), se \\(F_n(x) \\rightarrow F(x)\\) para todo \\(x\\) ponto de continuidade de \\(F\\).\nNotação:  \\(X_n \\xrightarrow{D} X\\) ou \\(X_n \\xrightarrow{D} F\\).",
    "crumbs": [
      "Probabilidade",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade/TCL.html#usando-o-tcl-pra-aproximar-a-binomial-pela-normal",
    "href": "Probabilidade/TCL.html#usando-o-tcl-pra-aproximar-a-binomial-pela-normal",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Esta aproximação, também conhecida como Teorema de De Moivre–Laplace, …",
    "crumbs": [
      "Probabilidade",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade/TCL.html#diferentes-formulações-e-versões-do-tcl",
    "href": "Probabilidade/TCL.html#diferentes-formulações-e-versões-do-tcl",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Fazendo \\(S_n = X_1 + X_2 + ... +X_n\\), James (2023) trata o problema central do limite atavés do estudo da convergência em distribuição das somas parciais normalizadas e formula o TCL como sendo:\n\\(\\frac{{S}_n - E({S}_n)}{\\sqrt{Var(S_n)}} \\xrightarrow{D} N(0, 1)\\).\nOutras maneiras de representar o TCL são:\n\\(\\frac{\\bar{X} - E(\\bar{X})}{\\sqrt{Var(\\bar{X})}} \\xrightarrow{D} N(0, 1)\\)\n\\(\\sqrt{n} \\bar{X} \\xrightarrow{D} N(\\mu, \\sigma^2)\\)",
    "crumbs": [
      "Probabilidade",
      "Teorema Central do Limite"
    ]
  }
]