[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sobre",
    "section": "",
    "text": "Este é um Website do\nConstante Evolução\nConceitos Gerais de Probabilidade, Estatística e afins\nSinta-se livre para sugerir melhorias abrindo uma issue neste link\nEste é um website baseado no Quarto da empresa Posit.\n\nLinks Úteis\nSite oficial do Quarto\nTemplates de Quarto úteis do prof. Gang He\nLivro de Estatística Básica do Prof. Filipe Zabala"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre",
    "section": "",
    "text": "This is a Quarto website.\nTemplates úteis: https://drganghe.github.io/quarto-academic-site-examples.html\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\nTo add a plotly graph:\nytd library(plotly) fig &lt;- plot_ly(data = iris, x = ~Sepal.Length, y = ~Petal.Length) fig}"
  },
  {
    "objectID": "LGN.html",
    "href": "LGN.html",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "Este é uma página da LGN."
  },
  {
    "objectID": "TCL.html",
    "href": "TCL.html",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Este é uma página do TCL."
  },
  {
    "objectID": "Probabilidade/TCL.html",
    "href": "Probabilidade/TCL.html",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "O Teorema Central do Limite (TCL) é um dos resultados mais famosos e mais bonitos da Teoria da Probabilidade.\nSejam \\(X_1, X_2, ...\\) variáveis aleatórias independentes e identicamente distribuídas (\\(iid\\)) com média e variância existentes e finitas, isto é \\(-\\infty &lt; \\mu &lt; +\\infty\\) e \\(0 &lt; \\sigma^2 &lt; +\\infty\\).\nAlém disso, defina \\(\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n}\\) como sendo a média amostral.\nPela Lei dos Grandes Números, sabemos que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\), mas a qual taxa? Como que é a distribuição dessa quantidade?\n\n\nDefinição. Sejam \\(X, X_1, X_2, ...\\) variáveis aleatórias com, respectivamente, funções de distribuições \\(F, F_1, F_2, ...\\). Dizemos que \\(X_n\\) converge em distribuição para \\(X\\), quando \\(n \\rightarrow +\\infty\\), se \\(F_n(x) \\rightarrow F(x)\\) para todo \\(x\\) ponto de continuidade de \\(F\\).\nNotação: \\(X_n \\xrightarrow{D} X\\) ou \\(X_n \\xrightarrow{D} F\\).\n\n\n\nPodemos reescrever a quantidade da introdução como sendo \\(\\bar{X} - \\mu \\xrightarrow{n \\rightarrow + \\infty} 0\\). Assim, uma das maneiras de buscar entender a distribuição de uma quantidade quando ela vai zero é multiplicá-la por uma quantidade que vai para infinito de uma maneira que seu valor de convergência não “exploda” (ou “degenere”) para o infinito e nem para zero. De fato se multiplicarmos a quantidade acima por \\(n\\) elevando-o à um exponencial buscando controlar as taxas de convergência. Neste caso, o exponencial é o valor de \\(\\frac{1}{2}\\), ou seja, \\(\\sqrt{n}\\).1\nLogo, o TCL pode ser definido por:\n\\(\\sqrt{n} \\left(\\bar{X} - \\mu \\right) \\xrightarrow{n \\rightarrow + \\infty} N(0, \\sigma^{2})\\) em distribuição\nOu, alternativamente, na sua forma mais popular:\n\\[\n\\sqrt{n} \\left(\\frac{\\bar{X} - \\mu}{\\sigma} \\right) \\xrightarrow{D} N(0, 1)\n\\tag{1}\\]\nO TCL retrata um dos teoremas mais lindos de toda a probabilidade tendo em vista que com poucas condições iniciais (neste caso, média e variância finitas) conseguimos provar que a média amostral \\(\\bar{X}\\) padronizada converge em distribuição para a distribuição normal padrão. Observe que o teorema não faz nenhuma alusão ao tipo de variável aleatória, podendo ser discreto ou contínuo, e nem sobre o suporte da distribuição, podendo ser positivo, negativo ou ambos.\nEste teorema é um dos principais motivos pela ampla difusão da distribuição Normal em diversas áreas do conhecimento científico. Tendo em vista que, dadas às devidas condições, a média amostral pode ser aproximada pela distribuição normal padrão, as aplicações desses resultados são diversos.\nProva. Faça \\(S_n = X_1 + X_2 + ... +X_n\\) e multiplicando e dividindo por \\(n\\) o termo da Equação 1 chegamos em\n\\[\n\\frac{1}{\\sqrt{n}}\\left(\\frac{S_n - n\\mu}{\\sigma} \\right) = \\frac{1}{\\sqrt{n}}\\left(\\frac{\\sum_{i=1}^nX_i- n\\mu}{\\sigma} \\right) \\overset{\\text{Expandindo o somatório}}{=} \\frac{1}{\\sqrt{n}}\\sum_{i=1}^n\\left(\\frac{X_i- \\mu}{\\sigma} \\right) \\xrightarrow{D} N(0, 1)\n\\]\nPodemos assumir, sem perda de generalidade que \\(\\mu = 0\\) e \\(\\sigma = 1\\), pois poderíamos fazer a prova definindo uma variável aleatória sendo \\(Z_i = \\frac{X_i- \\mu}{\\sigma}\\), onde esta variável teria média 0 e desvio padrão 1. Sendo assim, basta provar\n\\[\\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}} \\xrightarrow{D} N(0, 1)\\]\nPara isso, faremos o uso da função geradora de momentos (\\(M\\)) da quantidade acima e avaliando o seu valor quanto \\(n \\rightarrow +\\infty\\) a fim de identificar qual a distribuição limite obtida.2 Ou seja:\n\\(M_{X_n}(x) \\xrightarrow{n \\rightarrow + \\infty} M_{X}(x) \\implies X_n \\xrightarrow{D} X\\)\nPor notação, vamos estabelecer \\(Q_n = \\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}}\\) e aplicar o teorema acima:\n\\(M_{Q_n}(t) = E\\left( e^{t\\left(\\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}}\\right)} \\right) \\overset{\\text{Prop. de Exponencial}}{=} E\\left( e^{\\frac{tX_1}{\\sqrt{n}}} \\times e^{\\frac{tX_2}{\\sqrt{n}}} \\times \\dots \\times e^{\\frac{tX_n}{\\sqrt{n}}} \\right) \\overset{\\text{Independência}}{=} \\prod_{i=1}^{n}E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right)\\)\nComo todos os \\(X_i\\) são identicamente distribuídos, podemos simplificar a expressão para:\n\\(M_{Q_n}(t) = \\prod_{i=1}^{n}E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right) = E\\left( e^{\\frac{tX_1}{\\sqrt{n}}} \\right) \\times E\\left( e^{\\frac{tX_2}{\\sqrt{n}}} \\right) \\times \\dots \\times E\\left( e^{\\frac{tX_n}{\\sqrt{n}}} \\right) = \\left( E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right) \\right)^n, \\ \\forall i\\)\nNo entanto, note acima que \\(E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right)\\) é exatamente a função geradora de momentos aplicada no ponto \\(\\frac{t}{\\sqrt{n}}\\). Logo:\n\\(M_{Q_n}(t) = \\left( M_{X_i}\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n, \\ \\forall i\\)\nPor simplificação de notação, vamos suprimir o \\(X_i\\)3 nos passos seguintes e vamos avaliar esta expressão quando \\(n \\rightarrow +\\infty\\).\nObserve que da maneira como está estruturado \\(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n\\) converge para uma indefinição do tipo \\(1^\\infty\\), pois \\(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n = \\left( E\\left( e^{\\frac{tX}{\\sqrt{n}}} \\right) \\right)^n \\rightarrow \\left( E\\left( e^0 \\right) \\right)^\\infty\\).\nPara tratar essa indefinição, aplicaremos o logaritmo natural (\\(log\\)) na expressão e, depois de avaliado o limite, podemos reverter o valor aplicando o exponencial. Logo,\n\\[log\\left(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n \\right) = n\\times log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)\\]\nQuando aplicamos \\(n \\rightarrow +\\infty\\) caímos numa indefinição do tipo \\(+\\infty \\times 0\\), logo vamos reescrever a equação para \\(n\\times log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right) = \\frac{log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)}{\\frac{1}{n}}\\) e, desta maneira, caírmos numa indefinição do tipo \\(\\frac{0}{0}\\) para aplicarmos l’Hôpital. Como o \\(n\\) é um número inteiro natural e para melhor tratativa algébrica, vamos fazer uma transforção de variável para facilitar o desenvolvimento e poder aplicar a derivada em um número real. Assim,\n\\[\n\\lim_{n\\to\\infty}\\frac{log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)}{\\frac{1}{n}} \\overset{y=\\frac{1}{\\sqrt{n}}, \\ y \\in \\mathbb{R} }{=} \\lim_{y\\to 0}\\frac{log\\left( M\\left( yt \\right) \\right)}{y^2}\n\\]\nQue é uma indefinição do tipo \\(\\frac{0}{0}\\). Logo, aplicando l’Hôpital:4\n\\[\n\\lim_{y\\to 0}\\frac{tM'(yt)}{M(yt)}\\frac{1}{2y}\n\\]\nEste limite continua sendo uma indefinição do tipo \\(\\frac{0}{0}\\), porque pela definição da função geradora do momento, quando aplicamos as suposições no ponto \\(t=0\\) temos:\n\\[\n\\begin{aligned}\nM(t) = E\\left(e^{tX}\\right) \\implies M(0) = 1 \\\\\n\\mu = 0 \\implies M'(0) = 0 \\\\\n\\sigma^2 = 1 \\implies M''(0) = 1\n\\end{aligned}\n\\] Logo, simplificando os limites e aplicando l’Hôpital novamente temos:\n\\[\n\\lim_{y\\to 0}\\frac{tM'(yt)}{M(yt)}\\frac{1}{2y} \\overset{\\lim_{y\\to 0}{M(yt)}=1}{=} \\lim_{y\\to 0}\\frac{tM'(yt)}{2y} = \\lim_{y\\to 0}\\frac{t^2M''(yt)}{2} = \\frac{t^2}{2} \\lim_{y\\to 0}M''(yt) = \\frac{t^2}{2}\n\\]\nAplicando o exponencial para reverter o logaritmo aplicado originalmente temos:\n\\[M_{Q_n}(t) \\xrightarrow{n \\rightarrow +\\infty} e^{\\frac{t^2}{2}}\\]\nQue coindide com a função geradora de momentos da Normal Padrão.\n\\(\\square\\)\nUma discussão mais aprofundada do Teorema Central do Limite pode ser encontrada no Capítulo 7 de James (2023) ou no Capítulo 6 de DeGroot e Schervish (2012).\n\n\n\nFazendo \\(S_n = X_1 + X_2 + ... +X_n\\), James (2023) trata o problema central do limite atavés do estudo da convergência em distribuição das somas parciais normalizadas e formula o TCL como sendo:\n\\[\\frac{{S}_n - E({S}_n)}{\\sqrt{Var(S_n)}} \\xrightarrow{D} N(0, 1)\\]\nOutras maneiras de representar o TCL são:\n\\[\\frac{\\bar{X} - E(\\bar{X})}{\\sqrt{Var(\\bar{X})}} \\xrightarrow{D} N(0, 1)\\]\n\\[\\sqrt{n} \\bar{X} \\xrightarrow{D} N(\\mu, \\sigma^2)\\]\n\n\n\nSe \\(X_i \\overset{iid}{\\sim} Bernoulli(p)\\), então \\(S_n = \\sum_{i=1}^{n}X_i \\sim Binomial(n, p)\\).\nA seguir, apresentamos a distribuição da quantidade:\n\\[\\frac{{S}_n - E({S}_n)}{\\sqrt{Var(S_n)}} = \\frac{{S}_n - np}{\\sqrt{np(1-p)}} \\xrightarrow{D} N(0, 1)\\]\nNo R:\n\n\nMostrar Código\nlibrary(ggplot2)\n\nset.seed(123)\n\n# Parâmetros\nn_simulacoes &lt;- 1000 # Número de Simulações\nn_lancamentos &lt;- 300 # Número de lançamentos por simulação\nprob_cara &lt;- 0.5     # Probabilidade de sair cara\n\n# Simulação\nSn &lt;- replicate(n_simulacoes, {\n  \n  lancamentos &lt;- sample(c(0,1), n_lancamentos, replace = TRUE, prob = c(1 - prob_cara, prob_cara))\n  sum(lancamentos)\n  \n})\n\n# Valores Normal Padrão\nvalores_pad &lt;- (Sn - n_lancamentos * prob_cara) / sqrt(n_lancamentos * prob_cara * (1 - prob_cara))\n\n# Dados para o gráfico\ndados &lt;- data.frame(valores = valores_pad)\n\n# Valores Teóricos\nmedia_pop &lt;- 0\ndesvio_padrao_pop &lt;- 1\n\n# Gráfico\nggplot(dados, aes(x = valores)) +\n  geom_histogram(aes(y = ..density..), fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = media_pop, sd = desvio_padrao_pop), col = \"red\", size = 1) +\n  labs(title = \"Teorema Central do Limite - Moeda Honesta\",\n       x = \"Média Padronizada dos Lançamentos\",\n       y = \"Densidade\")\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Parâmetros\nn_simulacoes = 1000  # Número de Simulações\nn_lancamentos = 300  # Número de lançamentos por simulação\nprob_cara = 0.5      # Probabilidade de sair cara\n\n# Simulação\nSn = np.array([\n    np.sum(np.random.choice([0, 1], size=n_lancamentos, p=[1 - prob_cara, prob_cara]))\n    for _ in range(n_simulacoes)\n])\n\n# Valores Normal Padrão\nvalores_pad = (Sn - n_lancamentos * prob_cara) / np.sqrt(n_lancamentos * prob_cara * (1 - prob_cara))\n\n# Valores Teóricos\nmedia_pop = 0\ndesvio_padrao_pop = 1\n\n# Gráfico\nplt.figure(figsize=(6, 4))\ncount, bins, ignored = plt.hist(valores_pad, bins=30, density=True, color='skyblue', edgecolor='black', alpha=0.7)\n\n# Curva Teórica Normal Padrão\nx = np.linspace(min(bins), max(bins), 1000)\nplt.plot(x, norm.pdf(x, media_pop, desvio_padrao_pop), color='red', lw=2, label='Normal(0,1)')\n\n# Labels e Título\nplt.title(\"Teorema Central do Limite - Moeda Honesta\")\nplt.xlabel(\"Média Padronizada dos Lançamentos\")\nplt.ylabel(\"Densidade\")\nplt.legend()\n\n# Exibir\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nEsta aproximação, também conhecida como Teorema de De Moivre–Laplace, ilustra como o TCL pode ser usado para aproximar a distribuição discreta Binomial pela distribuição contínua Normal.\nLembrando que se \\(X_i \\overset{iid}{\\sim} Bernoulli(p)\\), então \\(X = \\sum_{i=1}^{n}X_i \\sim Binomial(n, p)\\) (pela notação, \\(X = S_n\\) do exemplo computacional anterior) e queremos calcular a probabilidade da variável aleatória \\(X\\) estar entre dois valores inteiros \\(a\\) e \\(b\\) o que pode ser computacionalmente intenso. Nesse sentido, vamos aproximar essa probabilidade através do TCL usando o fato de que \\(\\frac{\\bar{X} - E(\\bar{X})}{\\sqrt{Var(\\bar{X})}} \\xrightarrow{D} N(0, 1)\\):\n\\[\nP(a \\le X \\le b) = P\\left( \\frac{a-np}{\\sqrt{np(1-p)}} \\le \\frac{X-np}{\\sqrt{np(1-p)}} \\le \\frac{b-np}{\\sqrt{np(1-p)}} \\right) \\approx \\Phi\\left( \\frac{b-np}{\\sqrt{np(1-p)}} \\right) - \\Phi\\left( \\frac{a-np}{\\sqrt{np(1-p)}} \\right)\n\\]\nEsta expressão representa a diferença das distribuição acumuladas da Normal Padrão entre os pontos \\(\\frac{b-np}{\\sqrt{np(1-p)}}\\) e \\(\\frac{a-np}{\\sqrt{np(1-p)}}\\).\n\n\nObserve que esta aproximação se mostra útil, no entanto temos que ter um cuidado adicional ao aproximarmos distribuições contínuas de discretas para evitarmos conclusões equivocadas. Por exemplo, suponha que gostaríamos de usar esta aproximação para calcular a probabilidade de a variável assumir um ponto específico \\(a\\), então podemos concluir, erroneamente, que:\n\\[P(X = a) = P(a \\le X \\le a) \\neq \\Phi(a) - \\Phi(a) = 0, \\ \\forall a\\]\nLogo, podemos melhorar essa aproximação de probabilidade através de uma especificação de um intervalo contínuo no entorno do valor \\(a\\):\n\\[P(X=a) \\overset{a \\ é \\ inteiro!}{=} P\\left( a - \\frac{1}{2} \\lt X \\lt a + \\frac{1}{2} \\right) \\approx \\Phi\\left( a - \\frac{1}{2} \\right) - \\Phi\\left( a + \\frac{1}{2} \\right)\\]",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html",
    "href": "Probabilidade/LGN.html",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei dos Grandes Números (LGN) é um dos resultados mais famosos da Teoria da Probabilidade.\nSejam \\(X_1, X_2, ...\\) variáveis aleatórias independentes e identicamente distribuídas (\\(iid\\)) com média e variância existentes e finitas, isto é \\(-\\infty &lt; \\mu &lt; +\\infty\\) e \\(0 &lt; \\sigma^2 &lt; +\\infty\\).\nAlém disso, defina \\(\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n}\\) como sendo a média amostral.\n\n\nA Lei Forte estabelece que a média amostral converge para \\(\\mu\\) no infinito com probabilidade 1. Ou seja, \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) com probabilidade 1.\nObserve que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) é um evento. Logo, a Lei Forte poderia ser reescrita com uma notação probabilística alternativa:\n\\[P\\left(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu \\right) = 1\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge quase certamente para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, no infinito, \\(\\bar{X}\\) e \\(\\mu\\) serão iguais. Ou seja, é uma convergência pontual do valor de uma variável aleatória (a média amostral) para uma constante (a média populacional).\nA prova deste teorema implica em provar que o evento acima é um evento de probabilidade 1, o que exige maior formalidade matemática fazendo uso do Lema de Borel-Cantelli.\nProva. Ver Cáp. 5 de James (2023).\n\\(\\square\\)\n\n\n\nA Lei Fraca estabelece que \\(\\forall \\varepsilon &gt; 0\\), então:\n\\[P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\xrightarrow{n \\rightarrow + \\infty} 0\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\).\nProva. A prova faz uso da Desigualdade de Chebyshev:\n\\[\n\\begin{align*}\nP\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\overset{\\text{Des. Cheb.}}{\\le} \\frac{Var(\\bar{X})}{\\varepsilon^2} = \\frac{Var\\left(\\frac{\\sum_{i=1}^{n}{X_i}}{n}\\right)}{\\varepsilon^2} \\overset{\\text{Prop. da Constante}}{=} \\frac{Var(\\sum_{i=1}^{n}{X_i})}{n^2\\varepsilon^2} \\overset{\\text{Indep.}}{=} \\\\\n= \\frac{\\sum_{i=1}^{n}{Var(X_i)}}{n^2\\varepsilon^2} \\overset{\\text{Ident. Dist.}}{=} \\frac{n\\sigma^2}{n^2\\varepsilon^2} = \\frac{\\sigma^2}{n\\varepsilon^2} \\xrightarrow{n \\rightarrow + \\infty} 0\n\\end{align*}\n\\]\n\\(\\square\\)\nEste resultado ilustra que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, é extremamente improvável que a diferença entre \\(\\bar{X}\\) e \\(\\mu\\) seja maior que \\(\\varepsilon\\).\n\n\n\nÉ intuitivo pensar afirmarmos que duas quantidades são iguais é mais “forte” do que dizer que essas duas quantidades são “altamente prováveis de estarem próximas”. Ora, se uma coisa é igual à outra, isso implica de que elas também são próximas. Tendo em vista que a Lei Forte dos Grandes Números representa a convergência quase certa e a Lei Fraca representa a convergência em probabilidade, temos que se\n\\[Convergência\\ Quase\\ Certa \\implies Convergência\\ em\\ Probabilidade\\]\nentão,\n\\[Lei\\ Forte \\implies Lei\\ Fraca\\]\nO contrário não vale para nenhuma das duas afirmativas acima.\n\n\n\nSuponha uma sequência independente de lançamentos de uma moeda honesta e a variável aleatória \\(X_i = 1\\) se cara e \\(X_i = 0\\) se coroa. Ou seja,\n\\[X_i \\overset{iid}{\\sim}Bernoulli(p)\\]\nonde \\(p = 0,5\\). Então, pela Lei Forte dos Grandes Números:\n\\[\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n} \\xrightarrow{n \\rightarrow + \\infty} p\\]\nEste resultado mostra que a medida que ao jogarmos a moeda infinitas vezes, iremos convergir para um cenário em que metade dos lançamentos serão caras e metade serão coroas. Note que o resultado vale quando \\(n \\rightarrow + \\infty\\), ou seja, ele não estabelece nada em uma quantidade finita de lançamentos onde pode existir variabilidade. Por exemplo, mesmo que seja altamente improvável que nos primeiros 100 lançamentos todos os resultados sejam a face “Cara”, não existe nada matematicamente que estabeleça que isso seja impossível. No entanto, no limite do infinito as quantidades iniciais serão “engolidas” pela LGN. Para uma discussão mais aprofundada sobre esse tema recomenda-se uma leitura sobre o Gambler’s Fallacy.\n\n\n\nNo R:\n\n\nMostrar Código\n# Carregar as bibliotecas necessárias\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Definir o número de lançamentos\nn &lt;- 500\n\n# Simular lançamentos de uma moeda honesta\nset.seed(123) # Para reprodutibilidade\nlancamentos &lt;- sample(c(\"Cara\", \"Coroa\"), n, replace = TRUE)\n\n# Calcular a proporção acumulada de caras\ndados &lt;- data.frame(lancamentos) %&gt;%\n  mutate(\n    n = row_number(),\n    proporcao_cara = cumsum(lancamentos == \"Cara\") / n\n  )\n\n# Criar o gráfico\nggplot(dados, aes(x = n, y = proporcao_cara)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Lei dos Grandes Números: Lançamento de uma Moeda Honesta\",\n       x = \"Número de Lançamentos\",\n       y = \"Proporção de Caras\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definindo o número de lançamentos\nn = 500\n\n# Simulando lançamentos de uma moeda honesta\nnp.random.seed(123)  # Para reprodutibilidade\nlancamentos = np.random.choice(['Cara', 'Coroa'], size=n)\n\n# Calculando a proporção acumulada de caras\nproporcao_cara = np.cumsum(lancamentos == 'Cara') / np.arange(1, n + 1)\n\n# Criando o gráfico\nplt.figure(figsize=(6, 4))\nplt.plot(proporcao_cara, color='blue', label='Proporção de Caras')\nplt.axhline(y=0.5, color='red', linestyle='--', label='Proporção Esperada (0.5)')\nplt.title('Lei dos Grandes Números: Lançamento de uma Moeda Honesta')\nplt.xlabel('Número de Lançamentos')\nplt.ylabel('Proporção de Caras')\nplt.legend()\nplt.grid()\nplt.show()",
    "crumbs": [
      "Grandes Amostras",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#seção-2",
    "href": "Probabilidade/LGN.html#seção-2",
    "title": "Lei dos Grandes Números",
    "section": "Seção 2",
    "text": "Seção 2\nColocando uma equação:\n\\(E = mc^2\\)\nE colocando um ggplot2:\n\n\nMostrar Código\n# Definindo parâmetros\nset.seed(123)           # Para reprodutibilidade\nn_lancamentos &lt;- 1000  # Número total de lançamentos\n\n# Simulando lançamentos: 1 representa \"cara\", 0 representa \"coroa\"\nresultados &lt;- sample(c(0, 1), size = n_lancamentos, replace = TRUE)\n\n# Calculando a média acumulada (frequência relativa de \"cara\")\nmedia_acumulada &lt;- cumsum(resultados) / (1:n_lancamentos)\n\n# Valor esperado teórico\nvalor_esperado &lt;- 0.5\n\n# Plotando o gráfico\nplot(media_acumulada, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Número de lançamentos\", ylab = \"Frequência relativa de 'cara'\",\n     main = \"Lei dos Grandes Números - Moeda Honesta\")\nabline(h = valor_esperado, col = \"red\", lty = 2, lwd = 2)\nlegend(\"bottomright\", legend = c(\"Frequência relativa\", \"Valor esperado (0.5)\"),\n       col = c(\"blue\", \"red\"), lty = c(1, 2), lwd = 2)\n\n\n\n\n\n\n\n\n\n\nSeção 3\nEla tem também um table of contents.\nEste é uma página da LGN citando .\n\n\nLinks Úteis:\nAula de Harvard de Lei dos Grandes Números e Teorema Central do Limite\nPágina da Wikipedia da Lei dos Grandes Números",
    "crumbs": [
      "Probabilidade",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#lei-forte-dos-grande-números",
    "href": "Probabilidade/LGN.html#lei-forte-dos-grande-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Forte estabelece que:\n\\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) com probabilidade 1.\nObserve que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) é um evento. Logo, a Lei Forte poderia ser reescrita com uma notação probabilística alternativa:\n\\(P\\left(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu \\right) = 1\\)\nEste resultado estabelece que \\(\\bar{X}\\) converge quase certamente para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, no infinito, \\(\\bar{X}\\) e \\(\\mu\\) serão iguais. Ou seja, é uma convergência pontual do valor de uma variável aleatória para uma constante (a média populacional).\nA prova deste teorema implica em provar que o evento acima é um evento de probabilidade 1, o que exige maior formalidade matemática fazendo uso do Lema de Borel-Cantelli.\nProva. Ver Cáp. 5 de James (2023).",
    "crumbs": [
      "Probabilidade",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#lei-fraca-dos-grande-números",
    "href": "Probabilidade/LGN.html#lei-fraca-dos-grande-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Fraca estabelece que \\(\\forall \\varepsilon &gt; 0\\), então:\n\\(P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\xrightarrow{n \\rightarrow + \\infty} 0\\)\nEste resultado estabelece que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\).\nProva. A prova faz uso da Desigualdade de Chebyshev:\n\\(P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\overset{\\text{Des. Cheb.}}{\\le} \\frac{Var(\\bar{X})}{\\varepsilon^2} = \\frac{Var\\left(\\frac{\\sum_{i=1}^{n}{X_i}}{n}\\right)}{\\varepsilon^2} \\overset{\\text{Prop. da Constante}}{=} \\frac{Var(\\sum_{i=1}^{n}{X_i})}{n^2\\varepsilon^2} \\overset{\\text{Indep.}}{=} \\frac{\\sum_{i=1}^{n}{Var(X_i)}}{n^2\\varepsilon^2} \\overset{\\text{Ident. Dist.}}{=} \\frac{n\\sigma^2}{n^2\\varepsilon^2} = \\frac{\\sigma^2}{n\\varepsilon^2} \\xrightarrow{n \\rightarrow + \\infty} 0\\)\nEste resultado ilustra que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, é extremamente improvável que a diferença entre \\(\\bar{X}\\) e \\(\\mu\\) seja maior que \\(\\varepsilon\\).",
    "crumbs": [
      "Probabilidade",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#exemplo-computacional",
    "href": "Probabilidade/LGN.html#exemplo-computacional",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "No R:\n\n\nMostrar Código\n# Carregar as bibliotecas necessárias\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Definir o número de lançamentos\nn &lt;- 500\n\n# Simular lançamentos de uma moeda honesta\nset.seed(123) # Para reprodutibilidade\nlancamentos &lt;- sample(c(\"Cara\", \"Coroa\"), n, replace = TRUE)\n\n# Calcular a proporção acumulada de caras\ndados &lt;- data.frame(lancamentos) %&gt;%\n  mutate(\n    n = row_number(),\n    proporcao_cara = cumsum(lancamentos == \"Cara\") / n\n  )\n\n# Criar o gráfico\nggplot(dados, aes(x = n, y = proporcao_cara)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Lei dos Grandes Números: Lançamento de uma Moeda Honesta\",\n       x = \"Número de Lançamentos\",\n       y = \"Proporção de Caras\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definindo o número de lançamentos\nn = 500\n\n# Simulando lançamentos de uma moeda honesta\nnp.random.seed(123)  # Para reprodutibilidade\nlancamentos = np.random.choice(['Cara', 'Coroa'], size=n)\n\n# Calculando a proporção acumulada de caras\nproporcao_cara = np.cumsum(lancamentos == 'Cara') / np.arange(1, n + 1)\n\n# Criando o gráfico\nplt.figure(figsize=(6, 4))\nplt.plot(proporcao_cara, color='blue', label='Proporção de Caras')\nplt.axhline(y=0.5, color='red', linestyle='--', label='Proporção Esperada (0.5)')\nplt.title('Lei dos Grandes Números: Lançamento de uma Moeda Honesta')\nplt.xlabel('Número de Lançamentos')\nplt.ylabel('Proporção de Caras')\nplt.legend()\nplt.grid()\nplt.show()",
    "crumbs": [
      "Grandes Amostras",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#relação-entre-leis-e-convergências",
    "href": "Probabilidade/LGN.html#relação-entre-leis-e-convergências",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "É intuitivo pensar afirmarmos que duas quantidades são iguais é mais “forte” do que dizer que essas duas quantidades são “altamente prováveis de estarem próximas”. Ora, se uma coisa é igual à outra, isso implica de que elas também são próximas. Tendo em vista que a Lei Forte dos Grandes Números representa a convergência quase certa e a Lei Fraca representa a convergência em probabilidade, temos que se\n\\[Convergência\\ Quase\\ Certa \\implies Convergência\\ em\\ Probabilidade\\]\nentão,\n\\[Lei\\ Forte \\implies Lei\\ Fraca\\]\nO contrário não vale para nenhuma das duas afirmativas acima.",
    "crumbs": [
      "Grandes Amostras",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#exemplo-o-caso-bernoulli",
    "href": "Probabilidade/LGN.html#exemplo-o-caso-bernoulli",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "Suponha uma sequência independente de lançamentos de uma moeda honesta e a variável aleatória \\(X_i = 1\\) se cara e \\(X_i = 0\\) se coroa. Ou seja,\n\\[X_i \\overset{iid}{\\sim}Bernoulli(p)\\]\nonde \\(p = 0,5\\). Então, pela Lei Forte dos Grandes Números:\n\\[\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n} \\xrightarrow{n \\rightarrow + \\infty} p\\]\nEste resultado mostra que a medida que ao jogarmos a moeda infinitas vezes, iremos convergir para um cenário em que metade dos lançamentos serão caras e metade serão coroas. Note que o resultado vale quando \\(n \\rightarrow + \\infty\\), ou seja, ele não estabelece nada em uma quantidade finita de lançamentos onde pode existir variabilidade. Por exemplo, mesmo que seja altamente improvável que nos primeiros 100 lançamentos todos os resultados sejam a face “Cara”, não existe nada matematicamente que estabeleça que isso seja impossível. No entanto, no limite do infinito as quantidades iniciais serão “engolidas” pela LGN. Para uma discussão mais aprofundada sobre esse tema recomenda-se uma leitura sobre o Gambler’s Fallacy.",
    "crumbs": [
      "Grandes Amostras",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#lei-forte-dos-grandes-números",
    "href": "Probabilidade/LGN.html#lei-forte-dos-grandes-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Forte estabelece que a média amostral converge para \\(\\mu\\) no infinito com probabilidade 1. Ou seja, \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) com probabilidade 1.\nObserve que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) é um evento. Logo, a Lei Forte poderia ser reescrita com uma notação probabilística alternativa:\n\\[P\\left(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu \\right) = 1\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge quase certamente para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, no infinito, \\(\\bar{X}\\) e \\(\\mu\\) serão iguais. Ou seja, é uma convergência pontual do valor de uma variável aleatória (a média amostral) para uma constante (a média populacional).\nA prova deste teorema implica em provar que o evento acima é um evento de probabilidade 1, o que exige maior formalidade matemática fazendo uso do Lema de Borel-Cantelli.\nProva. Ver Cáp. 5 de James (2023).\n\\(\\square\\)",
    "crumbs": [
      "Grandes Amostras",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/LGN.html#lei-fraca-dos-grandes-números",
    "href": "Probabilidade/LGN.html#lei-fraca-dos-grandes-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Fraca estabelece que \\(\\forall \\varepsilon &gt; 0\\), então:\n\\[P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\xrightarrow{n \\rightarrow + \\infty} 0\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\).\nProva. A prova faz uso da Desigualdade de Chebyshev:\n\\[\n\\begin{align*}\nP\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\overset{\\text{Des. Cheb.}}{\\le} \\frac{Var(\\bar{X})}{\\varepsilon^2} = \\frac{Var\\left(\\frac{\\sum_{i=1}^{n}{X_i}}{n}\\right)}{\\varepsilon^2} \\overset{\\text{Prop. da Constante}}{=} \\frac{Var(\\sum_{i=1}^{n}{X_i})}{n^2\\varepsilon^2} \\overset{\\text{Indep.}}{=} \\\\\n= \\frac{\\sum_{i=1}^{n}{Var(X_i)}}{n^2\\varepsilon^2} \\overset{\\text{Ident. Dist.}}{=} \\frac{n\\sigma^2}{n^2\\varepsilon^2} = \\frac{\\sigma^2}{n\\varepsilon^2} \\xrightarrow{n \\rightarrow + \\infty} 0\n\\end{align*}\n\\]\n\\(\\square\\)\nEste resultado ilustra que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, é extremamente improvável que a diferença entre \\(\\bar{X}\\) e \\(\\mu\\) seja maior que \\(\\varepsilon\\).",
    "crumbs": [
      "Grandes Amostras",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade/TCL.html#teorema-central-do-limite",
    "href": "Probabilidade/TCL.html#teorema-central-do-limite",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Podemos reescrever a quantidade da introdução como sendo \\(\\bar{X} - \\mu \\xrightarrow{n \\rightarrow + \\infty} 0\\). Assim, uma das maneiras de buscar entender a distribuição de uma quantidade quando ela vai zero é multiplicá-la por uma quantidade que vai para infinito de uma maneira que seu valor de convergência não “exploda” (ou “degenere”) para o infinito e nem para zero. De fato se multiplicarmos a quantidade acima por \\(n\\) elevando-o à um exponencial buscando controlar as taxas de convergência. Neste caso, o exponencial é o valor de \\(\\frac{1}{2}\\), ou seja, \\(\\sqrt{n}\\).1\nLogo, o TCL pode ser definido por:\n\\(\\sqrt{n} \\left(\\bar{X} - \\mu \\right) \\xrightarrow{n \\rightarrow + \\infty} N(0, \\sigma^{2})\\) em distribuição\nOu, alternativamente, na sua forma mais popular:\n\\[\n\\sqrt{n} \\left(\\frac{\\bar{X} - \\mu}{\\sigma} \\right) \\xrightarrow{D} N(0, 1)\n\\tag{1}\\]\nO TCL retrata um dos teoremas mais lindos de toda a probabilidade tendo em vista que com poucas condições iniciais (neste caso, média e variância finitas) conseguimos provar que a média amostral \\(\\bar{X}\\) padronizada converge em distribuição para a distribuição normal padrão. Observe que o teorema não faz nenhuma alusão ao tipo de variável aleatória, podendo ser discreto ou contínuo, e nem sobre o suporte da distribuição, podendo ser positivo, negativo ou ambos.\nEste teorema é um dos principais motivos pela ampla difusão da distribuição Normal em diversas áreas do conhecimento científico. Tendo em vista que, dadas às devidas condições, a média amostral pode ser aproximada pela distribuição normal padrão, as aplicações desses resultados são diversos.\nProva. Faça \\(S_n = X_1 + X_2 + ... +X_n\\) e multiplicando e dividindo por \\(n\\) o termo da Equação 1 chegamos em\n\\[\n\\frac{1}{\\sqrt{n}}\\left(\\frac{S_n - n\\mu}{\\sigma} \\right) = \\frac{1}{\\sqrt{n}}\\left(\\frac{\\sum_{i=1}^nX_i- n\\mu}{\\sigma} \\right) \\overset{\\text{Expandindo o somatório}}{=} \\frac{1}{\\sqrt{n}}\\sum_{i=1}^n\\left(\\frac{X_i- \\mu}{\\sigma} \\right) \\xrightarrow{D} N(0, 1)\n\\]\nPodemos assumir, sem perda de generalidade que \\(\\mu = 0\\) e \\(\\sigma = 1\\), pois poderíamos fazer a prova definindo uma variável aleatória sendo \\(Z_i = \\frac{X_i- \\mu}{\\sigma}\\), onde esta variável teria média 0 e desvio padrão 1. Sendo assim, basta provar\n\\[\\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}} \\xrightarrow{D} N(0, 1)\\]\nPara isso, faremos o uso da função geradora de momentos (\\(M\\)) da quantidade acima e avaliando o seu valor quanto \\(n \\rightarrow +\\infty\\) a fim de identificar qual a distribuição limite obtida.2 Ou seja:\n\\(M_{X_n}(x) \\xrightarrow{n \\rightarrow + \\infty} M_{X}(x) \\implies X_n \\xrightarrow{D} X\\)\nPor notação, vamos estabelecer \\(Q_n = \\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}}\\) e aplicar o teorema acima:\n\\(M_{Q_n}(t) = E\\left( e^{t\\left(\\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}}\\right)} \\right) \\overset{\\text{Prop. de Exponencial}}{=} E\\left( e^{\\frac{tX_1}{\\sqrt{n}}} \\times e^{\\frac{tX_2}{\\sqrt{n}}} \\times \\dots \\times e^{\\frac{tX_n}{\\sqrt{n}}} \\right) \\overset{\\text{Independência}}{=} \\prod_{i=1}^{n}E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right)\\)\nComo todos os \\(X_i\\) são identicamente distribuídos, podemos simplificar a expressão para:\n\\(M_{Q_n}(t) = \\prod_{i=1}^{n}E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right) = E\\left( e^{\\frac{tX_1}{\\sqrt{n}}} \\right) \\times E\\left( e^{\\frac{tX_2}{\\sqrt{n}}} \\right) \\times \\dots \\times E\\left( e^{\\frac{tX_n}{\\sqrt{n}}} \\right) = \\left( E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right) \\right)^n, \\ \\forall i\\)\nNo entanto, note acima que \\(E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right)\\) é exatamente a função geradora de momentos aplicada no ponto \\(\\frac{t}{\\sqrt{n}}\\). Logo:\n\\(M_{Q_n}(t) = \\left( M_{X_i}\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n, \\ \\forall i\\)\nPor simplificação de notação, vamos suprimir o \\(X_i\\)3 nos passos seguintes e vamos avaliar esta expressão quando \\(n \\rightarrow +\\infty\\).\nObserve que da maneira como está estruturado \\(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n\\) converge para uma indefinição do tipo \\(1^\\infty\\), pois \\(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n = \\left( E\\left( e^{\\frac{tX}{\\sqrt{n}}} \\right) \\right)^n \\rightarrow \\left( E\\left( e^0 \\right) \\right)^\\infty\\).\nPara tratar essa indefinição, aplicaremos o logaritmo natural (\\(log\\)) na expressão e, depois de avaliado o limite, podemos reverter o valor aplicando o exponencial. Logo,\n\\[log\\left(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n \\right) = n\\times log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)\\]\nQuando aplicamos \\(n \\rightarrow +\\infty\\) caímos numa indefinição do tipo \\(+\\infty \\times 0\\), logo vamos reescrever a equação para \\(n\\times log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right) = \\frac{log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)}{\\frac{1}{n}}\\) e, desta maneira, caírmos numa indefinição do tipo \\(\\frac{0}{0}\\) para aplicarmos l’Hôpital. Como o \\(n\\) é um número inteiro natural e para melhor tratativa algébrica, vamos fazer uma transforção de variável para facilitar o desenvolvimento e poder aplicar a derivada em um número real. Assim,\n\\[\n\\lim_{n\\to\\infty}\\frac{log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)}{\\frac{1}{n}} \\overset{y=\\frac{1}{\\sqrt{n}}, \\ y \\in \\mathbb{R} }{=} \\lim_{y\\to 0}\\frac{log\\left( M\\left( yt \\right) \\right)}{y^2}\n\\]\nQue é uma indefinição do tipo \\(\\frac{0}{0}\\). Logo, aplicando l’Hôpital:4\n\\[\n\\lim_{y\\to 0}\\frac{tM'(yt)}{M(yt)}\\frac{1}{2y}\n\\]\nEste limite continua sendo uma indefinição do tipo \\(\\frac{0}{0}\\), porque pela definição da função geradora do momento, quando aplicamos as suposições no ponto \\(t=0\\) temos:\n\\[\n\\begin{aligned}\nM(t) = E\\left(e^{tX}\\right) \\implies M(0) = 1 \\\\\n\\mu = 0 \\implies M'(0) = 0 \\\\\n\\sigma^2 = 1 \\implies M''(0) = 1\n\\end{aligned}\n\\] Logo, simplificando os limites e aplicando l’Hôpital novamente temos:\n\\[\n\\lim_{y\\to 0}\\frac{tM'(yt)}{M(yt)}\\frac{1}{2y} \\overset{\\lim_{y\\to 0}{M(yt)}=1}{=} \\lim_{y\\to 0}\\frac{tM'(yt)}{2y} = \\lim_{y\\to 0}\\frac{t^2M''(yt)}{2} = \\frac{t^2}{2} \\lim_{y\\to 0}M''(yt) = \\frac{t^2}{2}\n\\]\nAplicando o exponencial para reverter o logaritmo aplicado originalmente temos:\n\\[M_{Q_n}(t) \\xrightarrow{n \\rightarrow +\\infty} e^{\\frac{t^2}{2}}\\]\nQue coindide com a função geradora de momentos da Normal Padrão.\n\\(\\square\\)\nUma discussão mais aprofundada do Teorema Central do Limite pode ser encontrada no Capítulo 7 de James (2023) ou no Capítulo 6 de DeGroot e Schervish (2012).",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade/TCL.html#footnotes",
    "href": "Probabilidade/TCL.html#footnotes",
    "title": "Teorema Central do Limite",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nUma discussão sobre este fator de convergência é discutido aqui↩︎\nEste teorema é um caso particular do Teorema de Continuidade de Levy para funções características. Ele pode ser melhor detalhado em Curtiss (1942) ou aqui.↩︎\nMagalhães (2006) também realiza esta simplificação de notação em uma prova similar do TCL fazendo uso da Função Característica.↩︎\nLembrando que \\(\\frac{d\\log(f(x))}{dx} = \\frac{1}{f(x)} \\cdot \\frac{df(x)}{dx} = \\frac{f'(x)}{f(x)}\\)↩︎",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade/TCL.html#convergência-em-distribuição",
    "href": "Probabilidade/TCL.html#convergência-em-distribuição",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Definição. Sejam \\(X, X_1, X_2, ...\\) variáveis aleatórias com, respectivamente, funções de distribuições \\(F, F_1, F_2, ...\\). Dizemos que \\(X_n\\) converge em distribuição para \\(X\\), quando \\(n \\rightarrow +\\infty\\), se \\(F_n(x) \\rightarrow F(x)\\) para todo \\(x\\) ponto de continuidade de \\(F\\).\nNotação: \\(X_n \\xrightarrow{D} X\\) ou \\(X_n \\xrightarrow{D} F\\).",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade/TCL.html#usando-o-tcl-pra-aproximar-a-binomial-pela-normal",
    "href": "Probabilidade/TCL.html#usando-o-tcl-pra-aproximar-a-binomial-pela-normal",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Esta aproximação, também conhecida como Teorema de De Moivre–Laplace, ilustra como o TCL pode ser usado para aproximar a distribuição discreta Binomial pela distribuição contínua Normal.\nSeja \\(X_i \\overset{iid}{\\sim} Bernoulli(p)\\), então \\(X = \\sum_{i=1}^{n}X_i \\sim Binomial(n, p)\\) e queremos calcular a probabilidade da variável aleatória \\(X\\) estar entre dois valores inteiros \\(a\\) e \\(b\\) o que pode ser computacionalmente intenso. Nesse sentido, vamos aproximar essa probabilidade através do TCL usando o fato de que \\(\\frac{\\bar{X} - E(\\bar{X})}{\\sqrt{Var(\\bar{X})}} \\xrightarrow{D} N(0, 1)\\):\n\\[\nP(a \\le X \\le b) = P\\left( \\frac{a-np}{\\sqrt{np(1-p)}} \\le \\frac{X-np}{\\sqrt{np(1-p)}} \\le \\frac{b-np}{\\sqrt{np(1-p)}} \\right) \\approx \\Phi\\left( \\frac{b-np}{\\sqrt{np(1-p)}} \\right) - \\Phi\\left( \\frac{a-np}{\\sqrt{np(1-p)}} \\right)\n\\]\nEsta expressão representa a diferença das distribuição acumuladas da Normal Padrão entre os pontos \\(\\frac{b-np}{\\sqrt{np(1-p)}}\\) e \\(\\frac{a-np}{\\sqrt{np(1-p)}}\\).\n\n\nObserve que esta aproximação se mostra útil, no entanto temos que ter um cuidado adicional ao aproximarmos distribuições contínuas de discretas para evitarmos conclusões equivocadas. Por exemplo, suponha que gostaríamos de usar esta aproximação para calcular a probabilidade de a variável assumir um ponto específico \\(a\\), então podemos concluir, erroneamente, que:\n\\(P(X = a) = P(a \\le X \\le a) \\neq \\Phi(a) - \\Phi(a) = 0, \\ \\forall a\\)\nLogo, podemos melhorar essa aproximação de probabilidade através de uma especificação de um intervalo contínuo no entorno do valor \\(a\\):\n\\(P(X=a) \\overset{a \\ é \\ inteiro!}{=} P\\left( a - \\frac{1}{2} \\lt X \\lt a + \\frac{1}{2} \\right) \\approx \\Phi\\left( a - \\frac{1}{2} \\right) - \\Phi\\left( a + \\frac{1}{2} \\right)\\)",
    "crumbs": [
      "Probabilidade",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade/TCL.html#diferentes-formulações-e-versões-do-tcl",
    "href": "Probabilidade/TCL.html#diferentes-formulações-e-versões-do-tcl",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Fazendo \\(S_n = X_1 + X_2 + ... +X_n\\), James (2023) trata o problema central do limite atavés do estudo da convergência em distribuição das somas parciais normalizadas e formula o TCL como sendo:\n\\[\\frac{{S}_n - E({S}_n)}{\\sqrt{Var(S_n)}} \\xrightarrow{D} N(0, 1)\\]\nOutras maneiras de representar o TCL são:\n\\[\\frac{\\bar{X} - E(\\bar{X})}{\\sqrt{Var(\\bar{X})}} \\xrightarrow{D} N(0, 1)\\]\n\\[\\sqrt{n} \\bar{X} \\xrightarrow{D} N(\\mu, \\sigma^2)\\]",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade/TCL.html#exemplo-computacional",
    "href": "Probabilidade/TCL.html#exemplo-computacional",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Lembrando que se \\(X_i \\overset{iid}{\\sim} Bernoulli(p)\\), então \\(S_n = \\sum_{i=1}^{n}X_i \\sim Binomial(n, p)\\)\nA seguir, apresentamos a distribuição da quantidade:\n\\(\\frac{{S}_n - E({S}_n)}{\\sqrt{Var(S_n)}} = \\frac{{S}_n - np}{\\sqrt{np(1-p)}} \\xrightarrow{D} N(0, 1)\\).\nNo R:\n\n\nMostrar Código\nlibrary(ggplot2)\n\nset.seed(123)\n\n# Parâmetros\nn_simulacoes &lt;- 1000 # Número de Simulações\nn_lancamentos &lt;- 300 # Número de lançamentos por simulação\nprob_cara &lt;- 0.5     # Probabilidade de sair cara\n\n# Simulação\nSn &lt;- replicate(n_simulacoes, {\n  \n  lancamentos &lt;- sample(c(0,1), n_lancamentos, replace = TRUE, prob = c(1 - prob_cara, prob_cara))\n  sum(lancamentos)\n  \n})\n\n# Valores Normal Padrão\nvalores_pad &lt;- (Sn - n_lancamentos * prob_cara) / sqrt(n_lancamentos * prob_cara * (1 - prob_cara))\n\n# Dados para o gráfico\ndados &lt;- data.frame(valores = valores_pad)\n\n# Valores Teóricos\nmedia_pop &lt;- 0\ndesvio_padrao_pop &lt;- 1\n\n# Gráfico\nggplot(dados, aes(x = valores)) +\n  geom_histogram(aes(y = ..density..), fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = media_pop, sd = desvio_padrao_pop), col = \"red\", size = 1) +\n  labs(title = \"Teorema Central do Limite - Moeda Honesta\",\n       x = \"Média Padronizada dos Lançamentos\",\n       y = \"Densidade\")\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Parâmetros\nn_simulacoes = 1000  # Número de Simulações\nn_lancamentos = 300  # Número de lançamentos por simulação\nprob_cara = 0.5      # Probabilidade de sair cara\n\n# Simulação\nSn = np.array([\n    np.sum(np.random.choice([0, 1], size=n_lancamentos, p=[1 - prob_cara, prob_cara]))\n    for _ in range(n_simulacoes)\n])\n\n# Valores Normal Padrão\nvalores_pad = (Sn - n_lancamentos * prob_cara) / np.sqrt(n_lancamentos * prob_cara * (1 - prob_cara))\n\n# Valores Teóricos\nmedia_pop = 0\ndesvio_padrao_pop = 1\n\n# Gráfico\nplt.figure(figsize=(10, 6))\ncount, bins, ignored = plt.hist(valores_pad, bins=30, density=True, color='skyblue', edgecolor='black', alpha=0.7)\n\n# Curva Teórica Normal Padrão\nx = np.linspace(min(bins), max(bins), 1000)\nplt.plot(x, norm.pdf(x, media_pop, desvio_padrao_pop), color='red', lw=2, label='Normal(0,1)')\n\n# Labels e Título\nplt.title(\"Teorema Central do Limite - Moeda Honesta\")\nplt.xlabel(\"Média Padronizada dos Lançamentos\")\nplt.ylabel(\"Densidade\")\nplt.legend()\n\n# Exibir\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Probabilidade",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade/TCL.html#exemplo-computacional-o-caso-binomial",
    "href": "Probabilidade/TCL.html#exemplo-computacional-o-caso-binomial",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Se \\(X_i \\overset{iid}{\\sim} Bernoulli(p)\\), então \\(S_n = \\sum_{i=1}^{n}X_i \\sim Binomial(n, p)\\).\nA seguir, apresentamos a distribuição da quantidade:\n\\[\\frac{{S}_n - E({S}_n)}{\\sqrt{Var(S_n)}} = \\frac{{S}_n - np}{\\sqrt{np(1-p)}} \\xrightarrow{D} N(0, 1)\\]\nNo R:\n\n\nMostrar Código\nlibrary(ggplot2)\n\nset.seed(123)\n\n# Parâmetros\nn_simulacoes &lt;- 1000 # Número de Simulações\nn_lancamentos &lt;- 300 # Número de lançamentos por simulação\nprob_cara &lt;- 0.5     # Probabilidade de sair cara\n\n# Simulação\nSn &lt;- replicate(n_simulacoes, {\n  \n  lancamentos &lt;- sample(c(0,1), n_lancamentos, replace = TRUE, prob = c(1 - prob_cara, prob_cara))\n  sum(lancamentos)\n  \n})\n\n# Valores Normal Padrão\nvalores_pad &lt;- (Sn - n_lancamentos * prob_cara) / sqrt(n_lancamentos * prob_cara * (1 - prob_cara))\n\n# Dados para o gráfico\ndados &lt;- data.frame(valores = valores_pad)\n\n# Valores Teóricos\nmedia_pop &lt;- 0\ndesvio_padrao_pop &lt;- 1\n\n# Gráfico\nggplot(dados, aes(x = valores)) +\n  geom_histogram(aes(y = ..density..), fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = media_pop, sd = desvio_padrao_pop), col = \"red\", size = 1) +\n  labs(title = \"Teorema Central do Limite - Moeda Honesta\",\n       x = \"Média Padronizada dos Lançamentos\",\n       y = \"Densidade\")\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Parâmetros\nn_simulacoes = 1000  # Número de Simulações\nn_lancamentos = 300  # Número de lançamentos por simulação\nprob_cara = 0.5      # Probabilidade de sair cara\n\n# Simulação\nSn = np.array([\n    np.sum(np.random.choice([0, 1], size=n_lancamentos, p=[1 - prob_cara, prob_cara]))\n    for _ in range(n_simulacoes)\n])\n\n# Valores Normal Padrão\nvalores_pad = (Sn - n_lancamentos * prob_cara) / np.sqrt(n_lancamentos * prob_cara * (1 - prob_cara))\n\n# Valores Teóricos\nmedia_pop = 0\ndesvio_padrao_pop = 1\n\n# Gráfico\nplt.figure(figsize=(6, 4))\ncount, bins, ignored = plt.hist(valores_pad, bins=30, density=True, color='skyblue', edgecolor='black', alpha=0.7)\n\n# Curva Teórica Normal Padrão\nx = np.linspace(min(bins), max(bins), 1000)\nplt.plot(x, norm.pdf(x, media_pop, desvio_padrao_pop), color='red', lw=2, label='Normal(0,1)')\n\n# Labels e Título\nplt.title(\"Teorema Central do Limite - Moeda Honesta\")\nplt.xlabel(\"Média Padronizada dos Lançamentos\")\nplt.ylabel(\"Densidade\")\nplt.legend()\n\n# Exibir\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade/TCL.html#usando-o-tcl-pra-aproximar-probabilidades-da-binomial-pela-normal",
    "href": "Probabilidade/TCL.html#usando-o-tcl-pra-aproximar-probabilidades-da-binomial-pela-normal",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Esta aproximação, também conhecida como Teorema de De Moivre–Laplace, ilustra como o TCL pode ser usado para aproximar a distribuição discreta Binomial pela distribuição contínua Normal.\nLembrando que se \\(X_i \\overset{iid}{\\sim} Bernoulli(p)\\), então \\(X = \\sum_{i=1}^{n}X_i \\sim Binomial(n, p)\\) (pela notação, \\(X = S_n\\) do exemplo computacional anterior) e queremos calcular a probabilidade da variável aleatória \\(X\\) estar entre dois valores inteiros \\(a\\) e \\(b\\) o que pode ser computacionalmente intenso. Nesse sentido, vamos aproximar essa probabilidade através do TCL usando o fato de que \\(\\frac{\\bar{X} - E(\\bar{X})}{\\sqrt{Var(\\bar{X})}} \\xrightarrow{D} N(0, 1)\\):\n\\[\nP(a \\le X \\le b) = P\\left( \\frac{a-np}{\\sqrt{np(1-p)}} \\le \\frac{X-np}{\\sqrt{np(1-p)}} \\le \\frac{b-np}{\\sqrt{np(1-p)}} \\right) \\approx \\Phi\\left( \\frac{b-np}{\\sqrt{np(1-p)}} \\right) - \\Phi\\left( \\frac{a-np}{\\sqrt{np(1-p)}} \\right)\n\\]\nEsta expressão representa a diferença das distribuição acumuladas da Normal Padrão entre os pontos \\(\\frac{b-np}{\\sqrt{np(1-p)}}\\) e \\(\\frac{a-np}{\\sqrt{np(1-p)}}\\).\n\n\nObserve que esta aproximação se mostra útil, no entanto temos que ter um cuidado adicional ao aproximarmos distribuições contínuas de discretas para evitarmos conclusões equivocadas. Por exemplo, suponha que gostaríamos de usar esta aproximação para calcular a probabilidade de a variável assumir um ponto específico \\(a\\), então podemos concluir, erroneamente, que:\n\\[P(X = a) = P(a \\le X \\le a) \\neq \\Phi(a) - \\Phi(a) = 0, \\ \\forall a\\]\nLogo, podemos melhorar essa aproximação de probabilidade através de uma especificação de um intervalo contínuo no entorno do valor \\(a\\):\n\\[P(X=a) \\overset{a \\ é \\ inteiro!}{=} P\\left( a - \\frac{1}{2} \\lt X \\lt a + \\frac{1}{2} \\right) \\approx \\Phi\\left( a - \\frac{1}{2} \\right) - \\Phi\\left( a + \\frac{1}{2} \\right)\\]",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Multivariada/Componentes_Principais.html",
    "href": "Multivariada/Componentes_Principais.html",
    "title": "Componentes Principais",
    "section": "",
    "text": "\\[\n\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n\\]",
    "crumbs": [
      "Principais Tópicos",
      "Componentes Principais"
    ]
  },
  {
    "objectID": "Multivariada/Cluster.html",
    "href": "Multivariada/Cluster.html",
    "title": "Análise de Cluster",
    "section": "",
    "text": "Introdução\nAnálise de Cluster",
    "crumbs": [
      "Principais Tópicos",
      "Análise de Cluster"
    ]
  },
  {
    "objectID": "section2/topic2.html",
    "href": "section2/topic2.html",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei dos Grandes Números (LGN) é um dos resultados mais famosos da Teoria da Probabilidade.\nSejam \\(X_1, X_2, ...\\) variáveis aleatórias independentes e identicamente distribuídas (\\(iid\\)) com média e variância existentes e finitas, isto é \\(-\\infty &lt; \\mu &lt; +\\infty\\) e \\(0 &lt; \\sigma^2 &lt; +\\infty\\).\nAlém disso, defina \\(\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n}\\) como sendo a média amostral.\n\n\nA Lei Forte estabelece que a média amostral converge para \\(\\mu\\) no infinito com probabilidade 1. Ou seja, \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) com probabilidade 1.\nObserve que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) é um evento. Logo, a Lei Forte poderia ser reescrita com uma notação probabilística alternativa:\n\\[P\\left(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu \\right) = 1\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge quase certamente para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, no infinito, \\(\\bar{X}\\) e \\(\\mu\\) serão iguais. Ou seja, é uma convergência pontual do valor de uma variável aleatória (a média amostral) para uma constante (a média populacional).\nA prova deste teorema implica em provar que o evento acima é um evento de probabilidade 1, o que exige maior formalidade matemática fazendo uso do Lema de Borel-Cantelli.\nProva. Ver Cáp. 5 de James (2023).\n\\(\\square\\)\n\n\n\nA Lei Fraca estabelece que \\(\\forall \\varepsilon &gt; 0\\), então:\n\\[P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\xrightarrow{n \\rightarrow + \\infty} 0\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\).\nProva. A prova faz uso da Desigualdade de Chebyshev:\n\\[\n\\begin{align*}\nP\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\overset{\\text{Des. Cheb.}}{\\le} \\frac{Var(\\bar{X})}{\\varepsilon^2} = \\frac{Var\\left(\\frac{\\sum_{i=1}^{n}{X_i}}{n}\\right)}{\\varepsilon^2} \\overset{\\text{Prop. da Constante}}{=} \\frac{Var(\\sum_{i=1}^{n}{X_i})}{n^2\\varepsilon^2} \\overset{\\text{Indep.}}{=} \\\\\n= \\frac{\\sum_{i=1}^{n}{Var(X_i)}}{n^2\\varepsilon^2} \\overset{\\text{Ident. Dist.}}{=} \\frac{n\\sigma^2}{n^2\\varepsilon^2} = \\frac{\\sigma^2}{n\\varepsilon^2} \\xrightarrow{n \\rightarrow + \\infty} 0\n\\end{align*}\n\\]\n\\(\\square\\)\nEste resultado ilustra que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, é extremamente improvável que a diferença entre \\(\\bar{X}\\) e \\(\\mu\\) seja maior que \\(\\varepsilon\\).\n\n\n\nÉ intuitivo pensar afirmarmos que duas quantidades são iguais é mais “forte” do que dizer que essas duas quantidades são “altamente prováveis de estarem próximas”. Ora, se uma coisa é igual à outra, isso implica de que elas também são próximas. Tendo em vista que a Lei Forte dos Grandes Números representa a convergência quase certa e a Lei Fraca representa a convergência em probabilidade, temos que se\n\\[Convergência\\ Quase\\ Certa \\implies Convergência\\ em\\ Probabilidade\\]\nentão,\n\\[Lei\\ Forte \\implies Lei\\ Fraca\\]\nO contrário não vale para nenhuma das duas afirmativas acima.\n\n\n\nSuponha uma sequência independente de lançamentos de uma moeda honesta e a variável aleatória \\(X_i = 1\\) se cara e \\(X_i = 0\\) se coroa. Ou seja,\n\\[X_i \\overset{iid}{\\sim}Bernoulli(p)\\]\nonde \\(p = 0,5\\). Então, pela Lei Forte dos Grandes Números:\n\\[\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n} \\xrightarrow{n \\rightarrow + \\infty} p\\]\nEste resultado mostra que a medida que ao jogarmos a moeda infinitas vezes, iremos convergir para um cenário em que metade dos lançamentos serão caras e metade serão coroas. Note que o resultado vale quando \\(n \\rightarrow + \\infty\\), ou seja, ele não estabelece nada em uma quantidade finita de lançamentos onde pode existir variabilidade. Por exemplo, mesmo que seja altamente improvável que nos primeiros 100 lançamentos todos os resultados sejam a face “Cara”, não existe nada matematicamente que estabeleça que isso seja impossível. No entanto, no limite do infinito as quantidades iniciais serão “engolidas” pela LGN. Para uma discussão mais aprofundada sobre esse tema recomenda-se uma leitura sobre o Gambler’s Fallacy.\n\n\n\nNo R:\n\n\nMostrar Código\n# Carregar as bibliotecas necessárias\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Definir o número de lançamentos\nn &lt;- 500\n\n# Simular lançamentos de uma moeda honesta\nset.seed(123) # Para reprodutibilidade\nlancamentos &lt;- sample(c(\"Cara\", \"Coroa\"), n, replace = TRUE)\n\n# Calcular a proporção acumulada de caras\ndados &lt;- data.frame(lancamentos) %&gt;%\n  mutate(\n    n = row_number(),\n    proporcao_cara = cumsum(lancamentos == \"Cara\") / n\n  )\n\n# Criar o gráfico\nggplot(dados, aes(x = n, y = proporcao_cara)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Lei dos Grandes Números: Lançamento de uma Moeda Honesta\",\n       x = \"Número de Lançamentos\",\n       y = \"Proporção de Caras\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definindo o número de lançamentos\nn = 500\n\n# Simulando lançamentos de uma moeda honesta\nnp.random.seed(123)  # Para reprodutibilidade\nlancamentos = np.random.choice(['Cara', 'Coroa'], size=n)\n\n# Calculando a proporção acumulada de caras\nproporcao_cara = np.cumsum(lancamentos == 'Cara') / np.arange(1, n + 1)\n\n# Criando o gráfico\nplt.figure(figsize=(6, 4))\nplt.plot(proporcao_cara, color='blue', label='Proporção de Caras')\nplt.axhline(y=0.5, color='red', linestyle='--', label='Proporção Esperada (0.5)')\nplt.title('Lei dos Grandes Números: Lançamento de uma Moeda Honesta')\nplt.xlabel('Número de Lançamentos')\nplt.ylabel('Proporção de Caras')\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "section2/topic2.html#lei-forte-dos-grandes-números",
    "href": "section2/topic2.html#lei-forte-dos-grandes-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Forte estabelece que a média amostral converge para \\(\\mu\\) no infinito com probabilidade 1. Ou seja, \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) com probabilidade 1.\nObserve que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) é um evento. Logo, a Lei Forte poderia ser reescrita com uma notação probabilística alternativa:\n\\[P\\left(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu \\right) = 1\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge quase certamente para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, no infinito, \\(\\bar{X}\\) e \\(\\mu\\) serão iguais. Ou seja, é uma convergência pontual do valor de uma variável aleatória (a média amostral) para uma constante (a média populacional).\nA prova deste teorema implica em provar que o evento acima é um evento de probabilidade 1, o que exige maior formalidade matemática fazendo uso do Lema de Borel-Cantelli.\nProva. Ver Cáp. 5 de James (2023).\n\\(\\square\\)"
  },
  {
    "objectID": "section2/topic2.html#lei-fraca-dos-grandes-números",
    "href": "section2/topic2.html#lei-fraca-dos-grandes-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Fraca estabelece que \\(\\forall \\varepsilon &gt; 0\\), então:\n\\[P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\xrightarrow{n \\rightarrow + \\infty} 0\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\).\nProva. A prova faz uso da Desigualdade de Chebyshev:\n\\[\n\\begin{align*}\nP\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\overset{\\text{Des. Cheb.}}{\\le} \\frac{Var(\\bar{X})}{\\varepsilon^2} = \\frac{Var\\left(\\frac{\\sum_{i=1}^{n}{X_i}}{n}\\right)}{\\varepsilon^2} \\overset{\\text{Prop. da Constante}}{=} \\frac{Var(\\sum_{i=1}^{n}{X_i})}{n^2\\varepsilon^2} \\overset{\\text{Indep.}}{=} \\\\\n= \\frac{\\sum_{i=1}^{n}{Var(X_i)}}{n^2\\varepsilon^2} \\overset{\\text{Ident. Dist.}}{=} \\frac{n\\sigma^2}{n^2\\varepsilon^2} = \\frac{\\sigma^2}{n\\varepsilon^2} \\xrightarrow{n \\rightarrow + \\infty} 0\n\\end{align*}\n\\]\n\\(\\square\\)\nEste resultado ilustra que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, é extremamente improvável que a diferença entre \\(\\bar{X}\\) e \\(\\mu\\) seja maior que \\(\\varepsilon\\)."
  },
  {
    "objectID": "section2/topic2.html#relação-entre-leis-e-convergências",
    "href": "section2/topic2.html#relação-entre-leis-e-convergências",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "É intuitivo pensar afirmarmos que duas quantidades são iguais é mais “forte” do que dizer que essas duas quantidades são “altamente prováveis de estarem próximas”. Ora, se uma coisa é igual à outra, isso implica de que elas também são próximas. Tendo em vista que a Lei Forte dos Grandes Números representa a convergência quase certa e a Lei Fraca representa a convergência em probabilidade, temos que se\n\\[Convergência\\ Quase\\ Certa \\implies Convergência\\ em\\ Probabilidade\\]\nentão,\n\\[Lei\\ Forte \\implies Lei\\ Fraca\\]\nO contrário não vale para nenhuma das duas afirmativas acima."
  },
  {
    "objectID": "section2/topic2.html#exemplo-o-caso-bernoulli",
    "href": "section2/topic2.html#exemplo-o-caso-bernoulli",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "Suponha uma sequência independente de lançamentos de uma moeda honesta e a variável aleatória \\(X_i = 1\\) se cara e \\(X_i = 0\\) se coroa. Ou seja,\n\\[X_i \\overset{iid}{\\sim}Bernoulli(p)\\]\nonde \\(p = 0,5\\). Então, pela Lei Forte dos Grandes Números:\n\\[\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n} \\xrightarrow{n \\rightarrow + \\infty} p\\]\nEste resultado mostra que a medida que ao jogarmos a moeda infinitas vezes, iremos convergir para um cenário em que metade dos lançamentos serão caras e metade serão coroas. Note que o resultado vale quando \\(n \\rightarrow + \\infty\\), ou seja, ele não estabelece nada em uma quantidade finita de lançamentos onde pode existir variabilidade. Por exemplo, mesmo que seja altamente improvável que nos primeiros 100 lançamentos todos os resultados sejam a face “Cara”, não existe nada matematicamente que estabeleça que isso seja impossível. No entanto, no limite do infinito as quantidades iniciais serão “engolidas” pela LGN. Para uma discussão mais aprofundada sobre esse tema recomenda-se uma leitura sobre o Gambler’s Fallacy."
  },
  {
    "objectID": "section2/topic2.html#exemplo-computacional",
    "href": "section2/topic2.html#exemplo-computacional",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "No R:\n\n\nMostrar Código\n# Carregar as bibliotecas necessárias\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Definir o número de lançamentos\nn &lt;- 500\n\n# Simular lançamentos de uma moeda honesta\nset.seed(123) # Para reprodutibilidade\nlancamentos &lt;- sample(c(\"Cara\", \"Coroa\"), n, replace = TRUE)\n\n# Calcular a proporção acumulada de caras\ndados &lt;- data.frame(lancamentos) %&gt;%\n  mutate(\n    n = row_number(),\n    proporcao_cara = cumsum(lancamentos == \"Cara\") / n\n  )\n\n# Criar o gráfico\nggplot(dados, aes(x = n, y = proporcao_cara)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Lei dos Grandes Números: Lançamento de uma Moeda Honesta\",\n       x = \"Número de Lançamentos\",\n       y = \"Proporção de Caras\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definindo o número de lançamentos\nn = 500\n\n# Simulando lançamentos de uma moeda honesta\nnp.random.seed(123)  # Para reprodutibilidade\nlancamentos = np.random.choice(['Cara', 'Coroa'], size=n)\n\n# Calculando a proporção acumulada de caras\nproporcao_cara = np.cumsum(lancamentos == 'Cara') / np.arange(1, n + 1)\n\n# Criando o gráfico\nplt.figure(figsize=(6, 4))\nplt.plot(proporcao_cara, color='blue', label='Proporção de Caras')\nplt.axhline(y=0.5, color='red', linestyle='--', label='Proporção Esperada (0.5)')\nplt.title('Lei dos Grandes Números: Lançamento de uma Moeda Honesta')\nplt.xlabel('Número de Lançamentos')\nplt.ylabel('Proporção de Caras')\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "section1/topic1.html",
    "href": "section1/topic1.html",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei dos Grandes Números (LGN) é um dos resultados mais famosos da Teoria da Probabilidade.\nSejam \\(X_1, X_2, ...\\) variáveis aleatórias independentes e identicamente distribuídas (\\(iid\\)) com média e variância existentes e finitas, isto é \\(-\\infty &lt; \\mu &lt; +\\infty\\) e \\(0 &lt; \\sigma^2 &lt; +\\infty\\).\nAlém disso, defina \\(\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n}\\) como sendo a média amostral.\n\n\nA Lei Forte estabelece que a média amostral converge para \\(\\mu\\) no infinito com probabilidade 1. Ou seja, \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) com probabilidade 1.\nObserve que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) é um evento. Logo, a Lei Forte poderia ser reescrita com uma notação probabilística alternativa:\n\\[P\\left(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu \\right) = 1\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge quase certamente para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, no infinito, \\(\\bar{X}\\) e \\(\\mu\\) serão iguais. Ou seja, é uma convergência pontual do valor de uma variável aleatória (a média amostral) para uma constante (a média populacional).\nA prova deste teorema implica em provar que o evento acima é um evento de probabilidade 1, o que exige maior formalidade matemática fazendo uso do Lema de Borel-Cantelli.\nProva. Ver Cáp. 5 de James (2023).\n\\(\\square\\)\n\n\n\nA Lei Fraca estabelece que \\(\\forall \\varepsilon &gt; 0\\), então:\n\\[P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\xrightarrow{n \\rightarrow + \\infty} 0\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\).\nProva. A prova faz uso da Desigualdade de Chebyshev:\n\\[\n\\begin{align*}\nP\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\overset{\\text{Des. Cheb.}}{\\le} \\frac{Var(\\bar{X})}{\\varepsilon^2} = \\frac{Var\\left(\\frac{\\sum_{i=1}^{n}{X_i}}{n}\\right)}{\\varepsilon^2} \\overset{\\text{Prop. da Constante}}{=} \\frac{Var(\\sum_{i=1}^{n}{X_i})}{n^2\\varepsilon^2} \\overset{\\text{Indep.}}{=} \\\\\n= \\frac{\\sum_{i=1}^{n}{Var(X_i)}}{n^2\\varepsilon^2} \\overset{\\text{Ident. Dist.}}{=} \\frac{n\\sigma^2}{n^2\\varepsilon^2} = \\frac{\\sigma^2}{n\\varepsilon^2} \\xrightarrow{n \\rightarrow + \\infty} 0\n\\end{align*}\n\\]\n\\(\\square\\)\nEste resultado ilustra que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, é extremamente improvável que a diferença entre \\(\\bar{X}\\) e \\(\\mu\\) seja maior que \\(\\varepsilon\\).\n\n\n\nÉ intuitivo pensar afirmarmos que duas quantidades são iguais é mais “forte” do que dizer que essas duas quantidades são “altamente prováveis de estarem próximas”. Ora, se uma coisa é igual à outra, isso implica de que elas também são próximas. Tendo em vista que a Lei Forte dos Grandes Números representa a convergência quase certa e a Lei Fraca representa a convergência em probabilidade, temos que se\n\\[Convergência\\ Quase\\ Certa \\implies Convergência\\ em\\ Probabilidade\\]\nentão,\n\\[Lei\\ Forte \\implies Lei\\ Fraca\\]\nO contrário não vale para nenhuma das duas afirmativas acima.\n\n\n\nSuponha uma sequência independente de lançamentos de uma moeda honesta e a variável aleatória \\(X_i = 1\\) se cara e \\(X_i = 0\\) se coroa. Ou seja,\n\\[X_i \\overset{iid}{\\sim}Bernoulli(p)\\]\nonde \\(p = 0,5\\). Então, pela Lei Forte dos Grandes Números:\n\\[\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n} \\xrightarrow{n \\rightarrow + \\infty} p\\]\nEste resultado mostra que a medida que ao jogarmos a moeda infinitas vezes, iremos convergir para um cenário em que metade dos lançamentos serão caras e metade serão coroas. Note que o resultado vale quando \\(n \\rightarrow + \\infty\\), ou seja, ele não estabelece nada em uma quantidade finita de lançamentos onde pode existir variabilidade. Por exemplo, mesmo que seja altamente improvável que nos primeiros 100 lançamentos todos os resultados sejam a face “Cara”, não existe nada matematicamente que estabeleça que isso seja impossível. No entanto, no limite do infinito as quantidades iniciais serão “engolidas” pela LGN. Para uma discussão mais aprofundada sobre esse tema recomenda-se uma leitura sobre o Gambler’s Fallacy.\n\n\n\nNo R:\n\n\nMostrar Código\n# Carregar as bibliotecas necessárias\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Definir o número de lançamentos\nn &lt;- 500\n\n# Simular lançamentos de uma moeda honesta\nset.seed(123) # Para reprodutibilidade\nlancamentos &lt;- sample(c(\"Cara\", \"Coroa\"), n, replace = TRUE)\n\n# Calcular a proporção acumulada de caras\ndados &lt;- data.frame(lancamentos) %&gt;%\n  mutate(\n    n = row_number(),\n    proporcao_cara = cumsum(lancamentos == \"Cara\") / n\n  )\n\n# Criar o gráfico\nggplot(dados, aes(x = n, y = proporcao_cara)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Lei dos Grandes Números: Lançamento de uma Moeda Honesta\",\n       x = \"Número de Lançamentos\",\n       y = \"Proporção de Caras\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definindo o número de lançamentos\nn = 500\n\n# Simulando lançamentos de uma moeda honesta\nnp.random.seed(123)  # Para reprodutibilidade\nlancamentos = np.random.choice(['Cara', 'Coroa'], size=n)\n\n# Calculando a proporção acumulada de caras\nproporcao_cara = np.cumsum(lancamentos == 'Cara') / np.arange(1, n + 1)\n\n# Criando o gráfico\nplt.figure(figsize=(6, 4))\nplt.plot(proporcao_cara, color='blue', label='Proporção de Caras')\nplt.axhline(y=0.5, color='red', linestyle='--', label='Proporção Esperada (0.5)')\nplt.title('Lei dos Grandes Números: Lançamento de uma Moeda Honesta')\nplt.xlabel('Número de Lançamentos')\nplt.ylabel('Proporção de Caras')\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "section1/topic1.html#lei-forte-dos-grandes-números",
    "href": "section1/topic1.html#lei-forte-dos-grandes-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Forte estabelece que a média amostral converge para \\(\\mu\\) no infinito com probabilidade 1. Ou seja, \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) com probabilidade 1.\nObserve que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) é um evento. Logo, a Lei Forte poderia ser reescrita com uma notação probabilística alternativa:\n\\[P\\left(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu \\right) = 1\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge quase certamente para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, no infinito, \\(\\bar{X}\\) e \\(\\mu\\) serão iguais. Ou seja, é uma convergência pontual do valor de uma variável aleatória (a média amostral) para uma constante (a média populacional).\nA prova deste teorema implica em provar que o evento acima é um evento de probabilidade 1, o que exige maior formalidade matemática fazendo uso do Lema de Borel-Cantelli.\nProva. Ver Cáp. 5 de James (2023).\n\\(\\square\\)"
  },
  {
    "objectID": "section1/topic1.html#lei-fraca-dos-grandes-números",
    "href": "section1/topic1.html#lei-fraca-dos-grandes-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Fraca estabelece que \\(\\forall \\varepsilon &gt; 0\\), então:\n\\[P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\xrightarrow{n \\rightarrow + \\infty} 0\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\).\nProva. A prova faz uso da Desigualdade de Chebyshev:\n\\[\n\\begin{align*}\nP\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\overset{\\text{Des. Cheb.}}{\\le} \\frac{Var(\\bar{X})}{\\varepsilon^2} = \\frac{Var\\left(\\frac{\\sum_{i=1}^{n}{X_i}}{n}\\right)}{\\varepsilon^2} \\overset{\\text{Prop. da Constante}}{=} \\frac{Var(\\sum_{i=1}^{n}{X_i})}{n^2\\varepsilon^2} \\overset{\\text{Indep.}}{=} \\\\\n= \\frac{\\sum_{i=1}^{n}{Var(X_i)}}{n^2\\varepsilon^2} \\overset{\\text{Ident. Dist.}}{=} \\frac{n\\sigma^2}{n^2\\varepsilon^2} = \\frac{\\sigma^2}{n\\varepsilon^2} \\xrightarrow{n \\rightarrow + \\infty} 0\n\\end{align*}\n\\]\n\\(\\square\\)\nEste resultado ilustra que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, é extremamente improvável que a diferença entre \\(\\bar{X}\\) e \\(\\mu\\) seja maior que \\(\\varepsilon\\)."
  },
  {
    "objectID": "section1/topic1.html#relação-entre-leis-e-convergências",
    "href": "section1/topic1.html#relação-entre-leis-e-convergências",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "É intuitivo pensar afirmarmos que duas quantidades são iguais é mais “forte” do que dizer que essas duas quantidades são “altamente prováveis de estarem próximas”. Ora, se uma coisa é igual à outra, isso implica de que elas também são próximas. Tendo em vista que a Lei Forte dos Grandes Números representa a convergência quase certa e a Lei Fraca representa a convergência em probabilidade, temos que se\n\\[Convergência\\ Quase\\ Certa \\implies Convergência\\ em\\ Probabilidade\\]\nentão,\n\\[Lei\\ Forte \\implies Lei\\ Fraca\\]\nO contrário não vale para nenhuma das duas afirmativas acima."
  },
  {
    "objectID": "section1/topic1.html#exemplo-o-caso-bernoulli",
    "href": "section1/topic1.html#exemplo-o-caso-bernoulli",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "Suponha uma sequência independente de lançamentos de uma moeda honesta e a variável aleatória \\(X_i = 1\\) se cara e \\(X_i = 0\\) se coroa. Ou seja,\n\\[X_i \\overset{iid}{\\sim}Bernoulli(p)\\]\nonde \\(p = 0,5\\). Então, pela Lei Forte dos Grandes Números:\n\\[\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n} \\xrightarrow{n \\rightarrow + \\infty} p\\]\nEste resultado mostra que a medida que ao jogarmos a moeda infinitas vezes, iremos convergir para um cenário em que metade dos lançamentos serão caras e metade serão coroas. Note que o resultado vale quando \\(n \\rightarrow + \\infty\\), ou seja, ele não estabelece nada em uma quantidade finita de lançamentos onde pode existir variabilidade. Por exemplo, mesmo que seja altamente improvável que nos primeiros 100 lançamentos todos os resultados sejam a face “Cara”, não existe nada matematicamente que estabeleça que isso seja impossível. No entanto, no limite do infinito as quantidades iniciais serão “engolidas” pela LGN. Para uma discussão mais aprofundada sobre esse tema recomenda-se uma leitura sobre o Gambler’s Fallacy."
  },
  {
    "objectID": "section1/topic1.html#exemplo-computacional",
    "href": "section1/topic1.html#exemplo-computacional",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "No R:\n\n\nMostrar Código\n# Carregar as bibliotecas necessárias\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Definir o número de lançamentos\nn &lt;- 500\n\n# Simular lançamentos de uma moeda honesta\nset.seed(123) # Para reprodutibilidade\nlancamentos &lt;- sample(c(\"Cara\", \"Coroa\"), n, replace = TRUE)\n\n# Calcular a proporção acumulada de caras\ndados &lt;- data.frame(lancamentos) %&gt;%\n  mutate(\n    n = row_number(),\n    proporcao_cara = cumsum(lancamentos == \"Cara\") / n\n  )\n\n# Criar o gráfico\nggplot(dados, aes(x = n, y = proporcao_cara)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Lei dos Grandes Números: Lançamento de uma Moeda Honesta\",\n       x = \"Número de Lançamentos\",\n       y = \"Proporção de Caras\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definindo o número de lançamentos\nn = 500\n\n# Simulando lançamentos de uma moeda honesta\nnp.random.seed(123)  # Para reprodutibilidade\nlancamentos = np.random.choice(['Cara', 'Coroa'], size=n)\n\n# Calculando a proporção acumulada de caras\nproporcao_cara = np.cumsum(lancamentos == 'Cara') / np.arange(1, n + 1)\n\n# Criando o gráfico\nplt.figure(figsize=(6, 4))\nplt.plot(proporcao_cara, color='blue', label='Proporção de Caras')\nplt.axhline(y=0.5, color='red', linestyle='--', label='Proporção Esperada (0.5)')\nplt.title('Lei dos Grandes Números: Lançamento de uma Moeda Honesta')\nplt.xlabel('Número de Lançamentos')\nplt.ylabel('Proporção de Caras')\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "section1/index.html",
    "href": "section1/index.html",
    "title": "Sobre",
    "section": "",
    "text": "Este é um Website do\nConstante Evolução\nConceitos Gerais de Probabilidade, Estatística e afins\nSinta-se livre para sugerir melhorias abrindo uma issue neste link\nEste é um website baseado no Quarto da empresa Posit.\n\nLinks Úteis\nSite oficial do Quarto\nTemplates úteis do prof. Gang He\nSite do prof. André Zibetti\n\n\nConsiderações de Desenvolvimento do Site\nO código para geração deste site por ser acessado neste link.\nPara rodar códigos em Python, é necessário instalar as bibliotecas usando o reticulate. Especificamente, o Quarto cria um ambiente virtual temporário chamado r-reticulate. Ou seja, as bibliotecas usadas durante a geração do site serão as bibliotecas instaladas neste ambiente.\nExemplo de instalação de biblioteca do Python usando o reticulate:\nreticulate::py_install(\"matplotlib\")\nreticulate::py_install(\"scipy\")\nAlém disso, cuidado com o gerenciamento das engines no momento de compilar o website. Segundo a documentação do Quarto, é importante sabermos as diferentes distinções de engine e suas coexistências conforme a seguir:\nEngine Conflicts:\n\nKnitr vs. Jupyter: If you explicitly set the engine to jupyter in your YAML, R code chunks will not be executed, and only Python (or the kernel’s language) will run. For mixed R and Python, engine: knitr is typically used, relying on reticulate for Python execution."
  },
  {
    "objectID": "section2/index.html",
    "href": "section2/index.html",
    "title": "Sobre",
    "section": "",
    "text": "Este é um Website do\nConstante Evolução\nConceitos Gerais de Probabilidade, Estatística e afins\nSinta-se livre para sugerir melhorias abrindo uma issue neste link\nEste é um website baseado no Quarto da empresa Posit.\n\nLinks Úteis\nSite oficial do Quarto\nTemplates úteis do prof. Gang He\nSite do prof. André Zibetti\n\n\nConsiderações de Desenvolvimento do Site\nO código para geração deste site por ser acessado neste link.\nPara rodar códigos em Python, é necessário instalar as bibliotecas usando o reticulate. Especificamente, o Quarto cria um ambiente virtual temporário chamado r-reticulate. Ou seja, as bibliotecas usadas durante a geração do site serão as bibliotecas instaladas neste ambiente.\nExemplo de instalação de biblioteca do Python usando o reticulate:\nreticulate::py_install(\"matplotlib\")\nreticulate::py_install(\"scipy\")\nAlém disso, cuidado com o gerenciamento das engines no momento de compilar o website. Segundo a documentação do Quarto, é importante sabermos as diferentes distinções de engine e suas coexistências conforme a seguir:\nEngine Conflicts:\n\nKnitr vs. Jupyter: If you explicitly set the engine to jupyter in your YAML, R code chunks will not be executed, and only Python (or the kernel’s language) will run. For mixed R and Python, engine: knitr is typically used, relying on reticulate for Python execution."
  },
  {
    "objectID": "Multivariada/Componentes_Principais.html#footnotes",
    "href": "Multivariada/Componentes_Principais.html#footnotes",
    "title": "Componentes Principais",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nJohnson e Wichern (2007) chama essa quantidade de “vetor aleatório”. No entanto, imagino que pensar como sendo uma base de dados com “p” colunas mais intuitivo.↩︎",
    "crumbs": [
      "Principais Tópicos",
      "Componentes Principais"
    ]
  },
  {
    "objectID": "Multivariada/Componentes_Principais.html#formulação",
    "href": "Multivariada/Componentes_Principais.html#formulação",
    "title": "Componentes Principais",
    "section": "Formulação",
    "text": "Formulação\nSeja \\(X_1, X_2, ..., X_p\\) variáveis aleatórias e assuma \\(\\bs{X'} = [X_1, X_2, ..., X_p]\\) ser toda a sua base de dados1 tendo uma matriz de covariância \\(\\bs{\\Sigma}\\) com os pares de autovalores e autovetores associados \\((\\lambda_1, \\bs{e_1}), (\\lambda_2, \\bs{e_2}), ..., (\\lambda_p, \\bs{e_p})\\) onde \\(\\lambda_1 \\ge \\lambda_2 \\ge ... \\ge \\lambda_p \\ge 0\\) e \\(||\\bs{e_i}|| = 1, \\forall i\\). Então, o i-ésimo componente principal será dado por:\n\\[\nY_i = \\bs{e_i'X} = e_{i1}X_1 + e_{i2}X_2 + ... +  e_{ip}X_p, \\ \\ \\ \\ i = 1, ..., p\n\\]\nEspecificamente com essas escolhas:\n\\[\n\\begin{align*}\nVar(Y_i) = \\bs{e_i'\\Sigma e_i} &= \\lambda_i  \\ \\ \\ \\ i = 1, ..., p \\\\\nCov(Y_i,Y_k) = \\bs{e_i'\\Sigma e_k} &= 0,  \\ \\ \\ \\ i \\ne k\n\\end{align*}\n\\]\nAs quantidades \\(Y_i\\)’s são considerados os componentes principais da base de dados \\(\\bs{X'}\\). Observe que \\(Y_i\\)’s são novas variáveis criadas a partir das variáveis \\(X_i\\)’s originais. Além disso, os \\(Y_i\\)’s não são correlacionados entre si.\nA análise de componentes principais nada mais é do que uma aplicação prática da Decomposição Espectral da matriz de covariância/correlação de uma base de dados.\nExpandindo a notação, os \\(p\\) componentes principais podem ser escritos como:\n\\[\n\\begin{align*}\nY_1 = \\bs{e_1'X} &= e_{11}X_1 + e_{12}X_2 + ... +  e_{1p}X_p \\\\\nY_2 = \\bs{e_2'X} &= e_{21}X_1 + e_{22}X_2 + ... +  e_{2p}X_p \\\\\n&\\;\\;\\vdots \\notag \\\\\nY_p = \\bs{e_p'X} &= e_{p1}X_1 + e_{p2}X_2 + ... +  e_{pp}X_p\n\\end{align*}\n\\]\nAs quantidades \\(e_{ij}\\) são denominadas loadings associado do componente \\(i\\) à variável \\(j\\).",
    "crumbs": [
      "Principais Tópicos",
      "Componentes Principais"
    ]
  },
  {
    "objectID": "Multivariada/Componentes_Principais.html#componentes-principais-na-matriz-de-correlação-bsrho",
    "href": "Multivariada/Componentes_Principais.html#componentes-principais-na-matriz-de-correlação-bsrho",
    "title": "Componentes Principais",
    "section": "Componentes Principais na Matriz de Correlação \\(\\bs{\\rho}\\)",
    "text": "Componentes Principais na Matriz de Correlação \\(\\bs{\\rho}\\)\nIncluir aqui",
    "crumbs": [
      "Principais Tópicos",
      "Componentes Principais"
    ]
  },
  {
    "objectID": "Multivariada/Componentes_Principais.html#o-cuidado-com-a-arbitrariedade-dos-autovetores",
    "href": "Multivariada/Componentes_Principais.html#o-cuidado-com-a-arbitrariedade-dos-autovetores",
    "title": "Componentes Principais",
    "section": "O cuidado com a arbitrariedade dos autovetores",
    "text": "O cuidado com a arbitrariedade dos autovetores\nIncluir aqui…",
    "crumbs": [
      "Principais Tópicos",
      "Componentes Principais"
    ]
  },
  {
    "objectID": "Multivariada/Componentes_Principais.html#o-cuidado-com-a-arbitrariedade-do-sentido-dos-autovetores",
    "href": "Multivariada/Componentes_Principais.html#o-cuidado-com-a-arbitrariedade-do-sentido-dos-autovetores",
    "title": "Componentes Principais",
    "section": "O cuidado com a arbitrariedade do sentido dos autovetores",
    "text": "O cuidado com a arbitrariedade do sentido dos autovetores\nIncluir aqui…",
    "crumbs": [
      "Principais Tópicos",
      "Componentes Principais"
    ]
  },
  {
    "objectID": "Probabilidade_B/TCL.html",
    "href": "Probabilidade_B/TCL.html",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "O Teorema Central do Limite (TCL) é um dos resultados mais famosos e mais bonitos da Teoria da Probabilidade.\nSejam \\(X_1, X_2, ...\\) variáveis aleatórias independentes e identicamente distribuídas (\\(iid\\)) com média e variância existentes e finitas, isto é \\(-\\infty &lt; \\mu &lt; +\\infty\\) e \\(0 &lt; \\sigma^2 &lt; +\\infty\\).\nAlém disso, defina \\(\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n}\\) como sendo a média amostral.\nPela Lei dos Grandes Números, sabemos que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\), mas a qual taxa? Como que é a distribuição dessa quantidade?\n\n\nDefinição. Sejam \\(X, X_1, X_2, ...\\) variáveis aleatórias com, respectivamente, funções de distribuições \\(F, F_1, F_2, ...\\). Dizemos que \\(X_n\\) converge em distribuição para \\(X\\), quando \\(n \\rightarrow +\\infty\\), se \\(F_n(x) \\rightarrow F(x)\\) para todo \\(x\\) ponto de continuidade de \\(F\\).\nNotação: \\(X_n \\xrightarrow{D} X\\) ou \\(X_n \\xrightarrow{D} F\\).\n\n\n\nPodemos reescrever a quantidade da introdução como sendo \\(\\bar{X} - \\mu \\xrightarrow{n \\rightarrow + \\infty} 0\\). Assim, uma das maneiras de buscar entender a distribuição de uma quantidade quando ela vai zero é multiplicá-la por uma quantidade que vai para infinito de uma maneira que seu valor de convergência não “exploda” (ou “degenere”) para o infinito e nem para zero. De fato se multiplicarmos a quantidade acima por \\(n\\) elevando-o à um exponencial buscando controlar as taxas de convergência. Neste caso, o exponencial é o valor de \\(\\frac{1}{2}\\), ou seja, \\(\\sqrt{n}\\).1\nLogo, o TCL pode ser definido por:\n\\(\\sqrt{n} \\left(\\bar{X} - \\mu \\right) \\xrightarrow{n \\rightarrow + \\infty} N(0, \\sigma^{2})\\) em distribuição\nOu, alternativamente, na sua forma mais popular:\n\\[\n\\sqrt{n} \\left(\\frac{\\bar{X} - \\mu}{\\sigma} \\right) \\xrightarrow{D} N(0, 1)\n\\tag{1}\\]\nO TCL retrata um dos teoremas mais lindos de toda a probabilidade tendo em vista que com poucas condições iniciais (neste caso, média e variância finitas) conseguimos provar que a média amostral \\(\\bar{X}\\) padronizada converge em distribuição para a distribuição normal padrão. Observe que o teorema não faz nenhuma alusão ao tipo de variável aleatória, podendo ser discreto ou contínuo, e nem sobre o suporte da distribuição, podendo ser positivo, negativo ou ambos.\nEste teorema é um dos principais motivos pela ampla difusão da distribuição Normal em diversas áreas do conhecimento científico. Tendo em vista que, dadas às devidas condições, a média amostral pode ser aproximada pela distribuição normal padrão, as aplicações desses resultados são diversos.\nProva. Faça \\(S_n = X_1 + X_2 + ... +X_n\\) e multiplicando e dividindo por \\(n\\) o termo da Equação 1 chegamos em\n\\[\n\\frac{1}{\\sqrt{n}}\\left(\\frac{S_n - n\\mu}{\\sigma} \\right) = \\frac{1}{\\sqrt{n}}\\left(\\frac{\\sum_{i=1}^nX_i- n\\mu}{\\sigma} \\right) \\overset{\\text{Expandindo o somatório}}{=} \\frac{1}{\\sqrt{n}}\\sum_{i=1}^n\\left(\\frac{X_i- \\mu}{\\sigma} \\right) \\xrightarrow{D} N(0, 1)\n\\]\nPodemos assumir, sem perda de generalidade que \\(\\mu = 0\\) e \\(\\sigma = 1\\), pois poderíamos fazer a prova definindo uma variável aleatória sendo \\(Z_i = \\frac{X_i- \\mu}{\\sigma}\\), onde esta variável teria média 0 e desvio padrão 1. Sendo assim, basta provar\n\\[\\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}} \\xrightarrow{D} N(0, 1)\\]\nPara isso, faremos o uso da função geradora de momentos (\\(M\\)) da quantidade acima e avaliando o seu valor quanto \\(n \\rightarrow +\\infty\\) a fim de identificar qual a distribuição limite obtida.2 Ou seja:\n\\(M_{X_n}(x) \\xrightarrow{n \\rightarrow + \\infty} M_{X}(x) \\implies X_n \\xrightarrow{D} X\\)\nPor notação, vamos estabelecer \\(Q_n = \\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}}\\) e aplicar o teorema acima:\n\\(M_{Q_n}(t) = E\\left( e^{t\\left(\\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}}\\right)} \\right) \\overset{\\text{Prop. de Exponencial}}{=} E\\left( e^{\\frac{tX_1}{\\sqrt{n}}} \\times e^{\\frac{tX_2}{\\sqrt{n}}} \\times \\dots \\times e^{\\frac{tX_n}{\\sqrt{n}}} \\right) \\overset{\\text{Independência}}{=} \\prod_{i=1}^{n}E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right)\\)\nComo todos os \\(X_i\\) são identicamente distribuídos, podemos simplificar a expressão para:\n\\(M_{Q_n}(t) = \\prod_{i=1}^{n}E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right) = E\\left( e^{\\frac{tX_1}{\\sqrt{n}}} \\right) \\times E\\left( e^{\\frac{tX_2}{\\sqrt{n}}} \\right) \\times \\dots \\times E\\left( e^{\\frac{tX_n}{\\sqrt{n}}} \\right) = \\left( E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right) \\right)^n, \\ \\forall i\\)\nNo entanto, note acima que \\(E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right)\\) é exatamente a função geradora de momentos aplicada no ponto \\(\\frac{t}{\\sqrt{n}}\\). Logo:\n\\(M_{Q_n}(t) = \\left( M_{X_i}\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n, \\ \\forall i\\)\nPor simplificação de notação, vamos suprimir o \\(X_i\\)3 nos passos seguintes e vamos avaliar esta expressão quando \\(n \\rightarrow +\\infty\\).\nObserve que da maneira como está estruturado \\(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n\\) converge para uma indefinição do tipo \\(1^\\infty\\), pois \\(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n = \\left( E\\left( e^{\\frac{tX}{\\sqrt{n}}} \\right) \\right)^n \\rightarrow \\left( E\\left( e^0 \\right) \\right)^\\infty\\).\nPara tratar essa indefinição, aplicaremos o logaritmo natural (\\(log\\)) na expressão e, depois de avaliado o limite, podemos reverter o valor aplicando o exponencial. Logo,\n\\[log\\left(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n \\right) = n\\times log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)\\]\nQuando aplicamos \\(n \\rightarrow +\\infty\\) caímos numa indefinição do tipo \\(+\\infty \\times 0\\), logo vamos reescrever a equação para \\(n\\times log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right) = \\frac{log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)}{\\frac{1}{n}}\\) e, desta maneira, caírmos numa indefinição do tipo \\(\\frac{0}{0}\\) para aplicarmos l’Hôpital. Como o \\(n\\) é um número inteiro natural e para melhor tratativa algébrica, vamos fazer uma transforção de variável para facilitar o desenvolvimento e poder aplicar a derivada em um número real. Assim,\n\\[\n\\lim_{n\\to\\infty}\\frac{log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)}{\\frac{1}{n}} \\overset{y=\\frac{1}{\\sqrt{n}}, \\ y \\in \\mathbb{R} }{=} \\lim_{y\\to 0}\\frac{log\\left( M\\left( yt \\right) \\right)}{y^2}\n\\]\nQue é uma indefinição do tipo \\(\\frac{0}{0}\\). Logo, aplicando l’Hôpital:4\n\\[\n\\lim_{y\\to 0}\\frac{tM'(yt)}{M(yt)}\\frac{1}{2y}\n\\]\nEste limite continua sendo uma indefinição do tipo \\(\\frac{0}{0}\\), porque pela definição da função geradora do momento, quando aplicamos as suposições no ponto \\(t=0\\) temos:\n\\[\n\\begin{aligned}\nM(t) = E\\left(e^{tX}\\right) \\implies M(0) = 1 \\\\\n\\mu = 0 \\implies M'(0) = 0 \\\\\n\\sigma^2 = 1 \\implies M''(0) = 1\n\\end{aligned}\n\\] Logo, simplificando os limites e aplicando l’Hôpital novamente temos:\n\\[\n\\lim_{y\\to 0}\\frac{tM'(yt)}{M(yt)}\\frac{1}{2y} \\overset{\\lim_{y\\to 0}{M(yt)}=1}{=} \\lim_{y\\to 0}\\frac{tM'(yt)}{2y} = \\lim_{y\\to 0}\\frac{t^2M''(yt)}{2} = \\frac{t^2}{2} \\lim_{y\\to 0}M''(yt) = \\frac{t^2}{2}\n\\]\nAplicando o exponencial para reverter o logaritmo aplicado originalmente temos:\n\\[M_{Q_n}(t) \\xrightarrow{n \\rightarrow +\\infty} e^{\\frac{t^2}{2}}\\]\nQue coindide com a função geradora de momentos da Normal Padrão.\n\\(\\square\\)\nUma discussão mais aprofundada do Teorema Central do Limite pode ser encontrada no Capítulo 7 de James (2023) ou no Capítulo 6 de DeGroot e Schervish (2012).\n\n\n\nFazendo \\(S_n = X_1 + X_2 + ... +X_n\\), James (2023) trata o problema central do limite atavés do estudo da convergência em distribuição das somas parciais normalizadas e formula o TCL como sendo:\n\\[\\frac{{S}_n - E({S}_n)}{\\sqrt{Var(S_n)}} \\xrightarrow{D} N(0, 1)\\]\nOutras maneiras de representar o TCL são:\n\\[\\frac{\\bar{X} - E(\\bar{X})}{\\sqrt{Var(\\bar{X})}} \\xrightarrow{D} N(0, 1)\\]\n\\[\\sqrt{n} \\bar{X} \\xrightarrow{D} N(\\mu, \\sigma^2)\\]\n\n\n\nSe \\(X_i \\overset{iid}{\\sim} Bernoulli(p)\\), então \\(S_n = \\sum_{i=1}^{n}X_i \\sim Binomial(n, p)\\).\nA seguir, apresentamos a distribuição da quantidade:\n\\[\\frac{{S}_n - E({S}_n)}{\\sqrt{Var(S_n)}} = \\frac{{S}_n - np}{\\sqrt{np(1-p)}} \\xrightarrow{D} N(0, 1)\\]\nNo R:\n\n\nMostrar Código\nlibrary(ggplot2)\n\nset.seed(123)\n\n# Parâmetros\nn_simulacoes &lt;- 1000 # Número de Simulações\nn_lancamentos &lt;- 300 # Número de lançamentos por simulação\nprob_cara &lt;- 0.5     # Probabilidade de sair cara\n\n# Simulação\nSn &lt;- replicate(n_simulacoes, {\n  \n  lancamentos &lt;- sample(c(0,1), n_lancamentos, replace = TRUE, prob = c(1 - prob_cara, prob_cara))\n  sum(lancamentos)\n  \n})\n\n# Valores Normal Padrão\nvalores_pad &lt;- (Sn - n_lancamentos * prob_cara) / sqrt(n_lancamentos * prob_cara * (1 - prob_cara))\n\n# Dados para o gráfico\ndados &lt;- data.frame(valores = valores_pad)\n\n# Valores Teóricos\nmedia_pop &lt;- 0\ndesvio_padrao_pop &lt;- 1\n\n# Gráfico\nggplot(dados, aes(x = valores)) +\n  geom_histogram(aes(y = ..density..), fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = media_pop, sd = desvio_padrao_pop), col = \"red\", size = 1) +\n  labs(title = \"Teorema Central do Limite - Moeda Honesta\",\n       x = \"Média Padronizada dos Lançamentos\",\n       y = \"Densidade\")\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Parâmetros\nn_simulacoes = 1000  # Número de Simulações\nn_lancamentos = 300  # Número de lançamentos por simulação\nprob_cara = 0.5      # Probabilidade de sair cara\n\n# Simulação\nSn = np.array([\n    np.sum(np.random.choice([0, 1], size=n_lancamentos, p=[1 - prob_cara, prob_cara]))\n    for _ in range(n_simulacoes)\n])\n\n# Valores Normal Padrão\nvalores_pad = (Sn - n_lancamentos * prob_cara) / np.sqrt(n_lancamentos * prob_cara * (1 - prob_cara))\n\n# Valores Teóricos\nmedia_pop = 0\ndesvio_padrao_pop = 1\n\n# Gráfico\nplt.figure(figsize=(6, 4))\ncount, bins, ignored = plt.hist(valores_pad, bins=30, density=True, color='skyblue', edgecolor='black', alpha=0.7)\n\n# Curva Teórica Normal Padrão\nx = np.linspace(min(bins), max(bins), 1000)\nplt.plot(x, norm.pdf(x, media_pop, desvio_padrao_pop), color='red', lw=2, label='Normal(0,1)')\n\n# Labels e Título\nplt.title(\"Teorema Central do Limite - Moeda Honesta\")\nplt.xlabel(\"Média Padronizada dos Lançamentos\")\nplt.ylabel(\"Densidade\")\nplt.legend()\n\n# Exibir\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nEsta aproximação, também conhecida como Teorema de De Moivre–Laplace, ilustra como o TCL pode ser usado para aproximar a distribuição discreta Binomial pela distribuição contínua Normal.\nLembrando que se \\(X_i \\overset{iid}{\\sim} Bernoulli(p)\\), então \\(X = \\sum_{i=1}^{n}X_i \\sim Binomial(n, p)\\) (pela notação, \\(X = S_n\\) do exemplo computacional anterior) e queremos calcular a probabilidade da variável aleatória \\(X\\) estar entre dois valores inteiros \\(a\\) e \\(b\\) o que pode ser computacionalmente intenso. Nesse sentido, vamos aproximar essa probabilidade através do TCL usando o fato de que \\(\\frac{\\bar{X} - E(\\bar{X})}{\\sqrt{Var(\\bar{X})}} \\xrightarrow{D} N(0, 1)\\):\n\\[\nP(a \\le X \\le b) = P\\left( \\frac{a-np}{\\sqrt{np(1-p)}} \\le \\frac{X-np}{\\sqrt{np(1-p)}} \\le \\frac{b-np}{\\sqrt{np(1-p)}} \\right) \\approx \\Phi\\left( \\frac{b-np}{\\sqrt{np(1-p)}} \\right) - \\Phi\\left( \\frac{a-np}{\\sqrt{np(1-p)}} \\right)\n\\]\nEsta expressão representa a diferença das distribuição acumuladas da Normal Padrão entre os pontos \\(\\frac{b-np}{\\sqrt{np(1-p)}}\\) e \\(\\frac{a-np}{\\sqrt{np(1-p)}}\\).\n\n\nObserve que esta aproximação se mostra útil, no entanto temos que ter um cuidado adicional ao aproximarmos distribuições contínuas de discretas para evitarmos conclusões equivocadas. Por exemplo, suponha que gostaríamos de usar esta aproximação para calcular a probabilidade de a variável assumir um ponto específico \\(a\\), então podemos concluir, erroneamente, que:\n\\[P(X = a) = P(a \\le X \\le a) \\neq \\Phi(a) - \\Phi(a) = 0, \\ \\forall a\\]\nLogo, podemos melhorar essa aproximação de probabilidade através de uma especificação de um intervalo contínuo no entorno do valor \\(a\\):\n\\[P(X=a) \\overset{a \\ é \\ inteiro!}{=} P\\left( a - \\frac{1}{2} \\lt X \\lt a + \\frac{1}{2} \\right) \\approx \\Phi\\left( a - \\frac{1}{2} \\right) - \\Phi\\left( a + \\frac{1}{2} \\right)\\]",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade_B/TCL.html#convergência-em-distribuição",
    "href": "Probabilidade_B/TCL.html#convergência-em-distribuição",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Definição. Sejam \\(X, X_1, X_2, ...\\) variáveis aleatórias com, respectivamente, funções de distribuições \\(F, F_1, F_2, ...\\). Dizemos que \\(X_n\\) converge em distribuição para \\(X\\), quando \\(n \\rightarrow +\\infty\\), se \\(F_n(x) \\rightarrow F(x)\\) para todo \\(x\\) ponto de continuidade de \\(F\\).\nNotação: \\(X_n \\xrightarrow{D} X\\) ou \\(X_n \\xrightarrow{D} F\\).",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade_B/TCL.html#teorema-central-do-limite",
    "href": "Probabilidade_B/TCL.html#teorema-central-do-limite",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Podemos reescrever a quantidade da introdução como sendo \\(\\bar{X} - \\mu \\xrightarrow{n \\rightarrow + \\infty} 0\\). Assim, uma das maneiras de buscar entender a distribuição de uma quantidade quando ela vai zero é multiplicá-la por uma quantidade que vai para infinito de uma maneira que seu valor de convergência não “exploda” (ou “degenere”) para o infinito e nem para zero. De fato se multiplicarmos a quantidade acima por \\(n\\) elevando-o à um exponencial buscando controlar as taxas de convergência. Neste caso, o exponencial é o valor de \\(\\frac{1}{2}\\), ou seja, \\(\\sqrt{n}\\).1\nLogo, o TCL pode ser definido por:\n\\(\\sqrt{n} \\left(\\bar{X} - \\mu \\right) \\xrightarrow{n \\rightarrow + \\infty} N(0, \\sigma^{2})\\) em distribuição\nOu, alternativamente, na sua forma mais popular:\n\\[\n\\sqrt{n} \\left(\\frac{\\bar{X} - \\mu}{\\sigma} \\right) \\xrightarrow{D} N(0, 1)\n\\tag{1}\\]\nO TCL retrata um dos teoremas mais lindos de toda a probabilidade tendo em vista que com poucas condições iniciais (neste caso, média e variância finitas) conseguimos provar que a média amostral \\(\\bar{X}\\) padronizada converge em distribuição para a distribuição normal padrão. Observe que o teorema não faz nenhuma alusão ao tipo de variável aleatória, podendo ser discreto ou contínuo, e nem sobre o suporte da distribuição, podendo ser positivo, negativo ou ambos.\nEste teorema é um dos principais motivos pela ampla difusão da distribuição Normal em diversas áreas do conhecimento científico. Tendo em vista que, dadas às devidas condições, a média amostral pode ser aproximada pela distribuição normal padrão, as aplicações desses resultados são diversos.\nProva. Faça \\(S_n = X_1 + X_2 + ... +X_n\\) e multiplicando e dividindo por \\(n\\) o termo da Equação 1 chegamos em\n\\[\n\\frac{1}{\\sqrt{n}}\\left(\\frac{S_n - n\\mu}{\\sigma} \\right) = \\frac{1}{\\sqrt{n}}\\left(\\frac{\\sum_{i=1}^nX_i- n\\mu}{\\sigma} \\right) \\overset{\\text{Expandindo o somatório}}{=} \\frac{1}{\\sqrt{n}}\\sum_{i=1}^n\\left(\\frac{X_i- \\mu}{\\sigma} \\right) \\xrightarrow{D} N(0, 1)\n\\]\nPodemos assumir, sem perda de generalidade que \\(\\mu = 0\\) e \\(\\sigma = 1\\), pois poderíamos fazer a prova definindo uma variável aleatória sendo \\(Z_i = \\frac{X_i- \\mu}{\\sigma}\\), onde esta variável teria média 0 e desvio padrão 1. Sendo assim, basta provar\n\\[\\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}} \\xrightarrow{D} N(0, 1)\\]\nPara isso, faremos o uso da função geradora de momentos (\\(M\\)) da quantidade acima e avaliando o seu valor quanto \\(n \\rightarrow +\\infty\\) a fim de identificar qual a distribuição limite obtida.2 Ou seja:\n\\(M_{X_n}(x) \\xrightarrow{n \\rightarrow + \\infty} M_{X}(x) \\implies X_n \\xrightarrow{D} X\\)\nPor notação, vamos estabelecer \\(Q_n = \\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}}\\) e aplicar o teorema acima:\n\\(M_{Q_n}(t) = E\\left( e^{t\\left(\\frac{\\sum_{i=1}^n X_i}{\\sqrt{n}}\\right)} \\right) \\overset{\\text{Prop. de Exponencial}}{=} E\\left( e^{\\frac{tX_1}{\\sqrt{n}}} \\times e^{\\frac{tX_2}{\\sqrt{n}}} \\times \\dots \\times e^{\\frac{tX_n}{\\sqrt{n}}} \\right) \\overset{\\text{Independência}}{=} \\prod_{i=1}^{n}E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right)\\)\nComo todos os \\(X_i\\) são identicamente distribuídos, podemos simplificar a expressão para:\n\\(M_{Q_n}(t) = \\prod_{i=1}^{n}E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right) = E\\left( e^{\\frac{tX_1}{\\sqrt{n}}} \\right) \\times E\\left( e^{\\frac{tX_2}{\\sqrt{n}}} \\right) \\times \\dots \\times E\\left( e^{\\frac{tX_n}{\\sqrt{n}}} \\right) = \\left( E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right) \\right)^n, \\ \\forall i\\)\nNo entanto, note acima que \\(E\\left( e^{\\frac{tX_i}{\\sqrt{n}}} \\right)\\) é exatamente a função geradora de momentos aplicada no ponto \\(\\frac{t}{\\sqrt{n}}\\). Logo:\n\\(M_{Q_n}(t) = \\left( M_{X_i}\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n, \\ \\forall i\\)\nPor simplificação de notação, vamos suprimir o \\(X_i\\)3 nos passos seguintes e vamos avaliar esta expressão quando \\(n \\rightarrow +\\infty\\).\nObserve que da maneira como está estruturado \\(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n\\) converge para uma indefinição do tipo \\(1^\\infty\\), pois \\(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n = \\left( E\\left( e^{\\frac{tX}{\\sqrt{n}}} \\right) \\right)^n \\rightarrow \\left( E\\left( e^0 \\right) \\right)^\\infty\\).\nPara tratar essa indefinição, aplicaremos o logaritmo natural (\\(log\\)) na expressão e, depois de avaliado o limite, podemos reverter o valor aplicando o exponencial. Logo,\n\\[log\\left(\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n \\right) = n\\times log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)\\]\nQuando aplicamos \\(n \\rightarrow +\\infty\\) caímos numa indefinição do tipo \\(+\\infty \\times 0\\), logo vamos reescrever a equação para \\(n\\times log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right) = \\frac{log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)}{\\frac{1}{n}}\\) e, desta maneira, caírmos numa indefinição do tipo \\(\\frac{0}{0}\\) para aplicarmos l’Hôpital. Como o \\(n\\) é um número inteiro natural e para melhor tratativa algébrica, vamos fazer uma transforção de variável para facilitar o desenvolvimento e poder aplicar a derivada em um número real. Assim,\n\\[\n\\lim_{n\\to\\infty}\\frac{log\\left( M\\left( \\frac{t}{\\sqrt{n}} \\right) \\right)}{\\frac{1}{n}} \\overset{y=\\frac{1}{\\sqrt{n}}, \\ y \\in \\mathbb{R} }{=} \\lim_{y\\to 0}\\frac{log\\left( M\\left( yt \\right) \\right)}{y^2}\n\\]\nQue é uma indefinição do tipo \\(\\frac{0}{0}\\). Logo, aplicando l’Hôpital:4\n\\[\n\\lim_{y\\to 0}\\frac{tM'(yt)}{M(yt)}\\frac{1}{2y}\n\\]\nEste limite continua sendo uma indefinição do tipo \\(\\frac{0}{0}\\), porque pela definição da função geradora do momento, quando aplicamos as suposições no ponto \\(t=0\\) temos:\n\\[\n\\begin{aligned}\nM(t) = E\\left(e^{tX}\\right) \\implies M(0) = 1 \\\\\n\\mu = 0 \\implies M'(0) = 0 \\\\\n\\sigma^2 = 1 \\implies M''(0) = 1\n\\end{aligned}\n\\] Logo, simplificando os limites e aplicando l’Hôpital novamente temos:\n\\[\n\\lim_{y\\to 0}\\frac{tM'(yt)}{M(yt)}\\frac{1}{2y} \\overset{\\lim_{y\\to 0}{M(yt)}=1}{=} \\lim_{y\\to 0}\\frac{tM'(yt)}{2y} = \\lim_{y\\to 0}\\frac{t^2M''(yt)}{2} = \\frac{t^2}{2} \\lim_{y\\to 0}M''(yt) = \\frac{t^2}{2}\n\\]\nAplicando o exponencial para reverter o logaritmo aplicado originalmente temos:\n\\[M_{Q_n}(t) \\xrightarrow{n \\rightarrow +\\infty} e^{\\frac{t^2}{2}}\\]\nQue coindide com a função geradora de momentos da Normal Padrão.\n\\(\\square\\)\nUma discussão mais aprofundada do Teorema Central do Limite pode ser encontrada no Capítulo 7 de James (2023) ou no Capítulo 6 de DeGroot e Schervish (2012).",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade_B/TCL.html#diferentes-formulações-e-versões-do-tcl",
    "href": "Probabilidade_B/TCL.html#diferentes-formulações-e-versões-do-tcl",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Fazendo \\(S_n = X_1 + X_2 + ... +X_n\\), James (2023) trata o problema central do limite atavés do estudo da convergência em distribuição das somas parciais normalizadas e formula o TCL como sendo:\n\\[\\frac{{S}_n - E({S}_n)}{\\sqrt{Var(S_n)}} \\xrightarrow{D} N(0, 1)\\]\nOutras maneiras de representar o TCL são:\n\\[\\frac{\\bar{X} - E(\\bar{X})}{\\sqrt{Var(\\bar{X})}} \\xrightarrow{D} N(0, 1)\\]\n\\[\\sqrt{n} \\bar{X} \\xrightarrow{D} N(\\mu, \\sigma^2)\\]",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade_B/TCL.html#exemplo-computacional-o-caso-binomial",
    "href": "Probabilidade_B/TCL.html#exemplo-computacional-o-caso-binomial",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Se \\(X_i \\overset{iid}{\\sim} Bernoulli(p)\\), então \\(S_n = \\sum_{i=1}^{n}X_i \\sim Binomial(n, p)\\).\nA seguir, apresentamos a distribuição da quantidade:\n\\[\\frac{{S}_n - E({S}_n)}{\\sqrt{Var(S_n)}} = \\frac{{S}_n - np}{\\sqrt{np(1-p)}} \\xrightarrow{D} N(0, 1)\\]\nNo R:\n\n\nMostrar Código\nlibrary(ggplot2)\n\nset.seed(123)\n\n# Parâmetros\nn_simulacoes &lt;- 1000 # Número de Simulações\nn_lancamentos &lt;- 300 # Número de lançamentos por simulação\nprob_cara &lt;- 0.5     # Probabilidade de sair cara\n\n# Simulação\nSn &lt;- replicate(n_simulacoes, {\n  \n  lancamentos &lt;- sample(c(0,1), n_lancamentos, replace = TRUE, prob = c(1 - prob_cara, prob_cara))\n  sum(lancamentos)\n  \n})\n\n# Valores Normal Padrão\nvalores_pad &lt;- (Sn - n_lancamentos * prob_cara) / sqrt(n_lancamentos * prob_cara * (1 - prob_cara))\n\n# Dados para o gráfico\ndados &lt;- data.frame(valores = valores_pad)\n\n# Valores Teóricos\nmedia_pop &lt;- 0\ndesvio_padrao_pop &lt;- 1\n\n# Gráfico\nggplot(dados, aes(x = valores)) +\n  geom_histogram(aes(y = ..density..), fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = media_pop, sd = desvio_padrao_pop), col = \"red\", size = 1) +\n  labs(title = \"Teorema Central do Limite - Moeda Honesta\",\n       x = \"Média Padronizada dos Lançamentos\",\n       y = \"Densidade\")\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Parâmetros\nn_simulacoes = 1000  # Número de Simulações\nn_lancamentos = 300  # Número de lançamentos por simulação\nprob_cara = 0.5      # Probabilidade de sair cara\n\n# Simulação\nSn = np.array([\n    np.sum(np.random.choice([0, 1], size=n_lancamentos, p=[1 - prob_cara, prob_cara]))\n    for _ in range(n_simulacoes)\n])\n\n# Valores Normal Padrão\nvalores_pad = (Sn - n_lancamentos * prob_cara) / np.sqrt(n_lancamentos * prob_cara * (1 - prob_cara))\n\n# Valores Teóricos\nmedia_pop = 0\ndesvio_padrao_pop = 1\n\n# Gráfico\nplt.figure(figsize=(6, 4))\ncount, bins, ignored = plt.hist(valores_pad, bins=30, density=True, color='skyblue', edgecolor='black', alpha=0.7)\n\n# Curva Teórica Normal Padrão\nx = np.linspace(min(bins), max(bins), 1000)\nplt.plot(x, norm.pdf(x, media_pop, desvio_padrao_pop), color='red', lw=2, label='Normal(0,1)')\n\n# Labels e Título\nplt.title(\"Teorema Central do Limite - Moeda Honesta\")\nplt.xlabel(\"Média Padronizada dos Lançamentos\")\nplt.ylabel(\"Densidade\")\nplt.legend()\n\n# Exibir\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade_B/TCL.html#usando-o-tcl-pra-aproximar-probabilidades-da-binomial-pela-normal",
    "href": "Probabilidade_B/TCL.html#usando-o-tcl-pra-aproximar-probabilidades-da-binomial-pela-normal",
    "title": "Teorema Central do Limite",
    "section": "",
    "text": "Esta aproximação, também conhecida como Teorema de De Moivre–Laplace, ilustra como o TCL pode ser usado para aproximar a distribuição discreta Binomial pela distribuição contínua Normal.\nLembrando que se \\(X_i \\overset{iid}{\\sim} Bernoulli(p)\\), então \\(X = \\sum_{i=1}^{n}X_i \\sim Binomial(n, p)\\) (pela notação, \\(X = S_n\\) do exemplo computacional anterior) e queremos calcular a probabilidade da variável aleatória \\(X\\) estar entre dois valores inteiros \\(a\\) e \\(b\\) o que pode ser computacionalmente intenso. Nesse sentido, vamos aproximar essa probabilidade através do TCL usando o fato de que \\(\\frac{\\bar{X} - E(\\bar{X})}{\\sqrt{Var(\\bar{X})}} \\xrightarrow{D} N(0, 1)\\):\n\\[\nP(a \\le X \\le b) = P\\left( \\frac{a-np}{\\sqrt{np(1-p)}} \\le \\frac{X-np}{\\sqrt{np(1-p)}} \\le \\frac{b-np}{\\sqrt{np(1-p)}} \\right) \\approx \\Phi\\left( \\frac{b-np}{\\sqrt{np(1-p)}} \\right) - \\Phi\\left( \\frac{a-np}{\\sqrt{np(1-p)}} \\right)\n\\]\nEsta expressão representa a diferença das distribuição acumuladas da Normal Padrão entre os pontos \\(\\frac{b-np}{\\sqrt{np(1-p)}}\\) e \\(\\frac{a-np}{\\sqrt{np(1-p)}}\\).\n\n\nObserve que esta aproximação se mostra útil, no entanto temos que ter um cuidado adicional ao aproximarmos distribuições contínuas de discretas para evitarmos conclusões equivocadas. Por exemplo, suponha que gostaríamos de usar esta aproximação para calcular a probabilidade de a variável assumir um ponto específico \\(a\\), então podemos concluir, erroneamente, que:\n\\[P(X = a) = P(a \\le X \\le a) \\neq \\Phi(a) - \\Phi(a) = 0, \\ \\forall a\\]\nLogo, podemos melhorar essa aproximação de probabilidade através de uma especificação de um intervalo contínuo no entorno do valor \\(a\\):\n\\[P(X=a) \\overset{a \\ é \\ inteiro!}{=} P\\left( a - \\frac{1}{2} \\lt X \\lt a + \\frac{1}{2} \\right) \\approx \\Phi\\left( a - \\frac{1}{2} \\right) - \\Phi\\left( a + \\frac{1}{2} \\right)\\]",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade_B/TCL.html#footnotes",
    "href": "Probabilidade_B/TCL.html#footnotes",
    "title": "Teorema Central do Limite",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nUma discussão sobre este fator de convergência é discutido aqui↩︎\nEste teorema é um caso particular do Teorema de Continuidade de Levy para funções características. Ele pode ser melhor detalhado em Curtiss (1942) ou aqui.↩︎\nMagalhães (2006) também realiza esta simplificação de notação em uma prova similar do TCL fazendo uso da Função Característica.↩︎\nLembrando que \\(\\frac{d\\log(f(x))}{dx} = \\frac{1}{f(x)} \\cdot \\frac{df(x)}{dx} = \\frac{f'(x)}{f(x)}\\)↩︎",
    "crumbs": [
      "Grandes Amostras",
      "Teorema Central do Limite"
    ]
  },
  {
    "objectID": "Probabilidade_B/LGN.html",
    "href": "Probabilidade_B/LGN.html",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei dos Grandes Números (LGN) é um dos resultados mais famosos da Teoria da Probabilidade.\nSejam \\(X_1, X_2, ...\\) variáveis aleatórias independentes e identicamente distribuídas (\\(iid\\)) com média e variância existentes e finitas, isto é \\(-\\infty &lt; \\mu &lt; +\\infty\\) e \\(0 &lt; \\sigma^2 &lt; +\\infty\\).\nAlém disso, defina \\(\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n}\\) como sendo a média amostral.\n\n\nA Lei Forte estabelece que a média amostral converge para \\(\\mu\\) no infinito com probabilidade 1. Ou seja, \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) com probabilidade 1.\nObserve que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) é um evento. Logo, a Lei Forte poderia ser reescrita com uma notação probabilística alternativa:\n\\[P\\left(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu \\right) = 1\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge quase certamente para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, no infinito, \\(\\bar{X}\\) e \\(\\mu\\) serão iguais. Ou seja, é uma convergência pontual do valor de uma variável aleatória (a média amostral) para uma constante (a média populacional).\nA prova deste teorema implica em provar que o evento acima é um evento de probabilidade 1, o que exige maior formalidade matemática fazendo uso do Lema de Borel-Cantelli.\nProva. Ver Cáp. 5 de James (2023).\n\\(\\square\\)\n\n\n\nA Lei Fraca estabelece que \\(\\forall \\varepsilon &gt; 0\\), então:\n\\[P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\xrightarrow{n \\rightarrow + \\infty} 0\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\).\nProva. A prova faz uso da Desigualdade de Chebyshev:\n\\[\n\\begin{align*}\nP\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\overset{\\text{Des. Cheb.}}{\\le} \\frac{Var(\\bar{X})}{\\varepsilon^2} = \\frac{Var\\left(\\frac{\\sum_{i=1}^{n}{X_i}}{n}\\right)}{\\varepsilon^2} \\overset{\\text{Prop. da Constante}}{=} \\frac{Var(\\sum_{i=1}^{n}{X_i})}{n^2\\varepsilon^2} \\overset{\\text{Indep.}}{=} \\\\\n= \\frac{\\sum_{i=1}^{n}{Var(X_i)}}{n^2\\varepsilon^2} \\overset{\\text{Ident. Dist.}}{=} \\frac{n\\sigma^2}{n^2\\varepsilon^2} = \\frac{\\sigma^2}{n\\varepsilon^2} \\xrightarrow{n \\rightarrow + \\infty} 0\n\\end{align*}\n\\]\n\\(\\square\\)\nEste resultado ilustra que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, é extremamente improvável que a diferença entre \\(\\bar{X}\\) e \\(\\mu\\) seja maior que \\(\\varepsilon\\).\n\n\n\nÉ intuitivo pensar afirmarmos que duas quantidades são iguais é mais “forte” do que dizer que essas duas quantidades são “altamente prováveis de estarem próximas”. Ora, se uma coisa é igual à outra, isso implica de que elas também são próximas. Tendo em vista que a Lei Forte dos Grandes Números representa a convergência quase certa e a Lei Fraca representa a convergência em probabilidade, temos que se\n\\[Convergência\\ Quase\\ Certa \\implies Convergência\\ em\\ Probabilidade\\]\nentão,\n\\[Lei\\ Forte \\implies Lei\\ Fraca\\]\nO contrário não vale para nenhuma das duas afirmativas acima.\n\n\n\nSuponha uma sequência independente de lançamentos de uma moeda honesta e a variável aleatória \\(X_i = 1\\) se cara e \\(X_i = 0\\) se coroa. Ou seja,\n\\[X_i \\overset{iid}{\\sim}Bernoulli(p)\\]\nonde \\(p = 0,5\\). Então, pela Lei Forte dos Grandes Números:\n\\[\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n} \\xrightarrow{n \\rightarrow + \\infty} p\\]\nEste resultado mostra que a medida que ao jogarmos a moeda infinitas vezes, iremos convergir para um cenário em que metade dos lançamentos serão caras e metade serão coroas. Note que o resultado vale quando \\(n \\rightarrow + \\infty\\), ou seja, ele não estabelece nada em uma quantidade finita de lançamentos onde pode existir variabilidade. Por exemplo, mesmo que seja altamente improvável que nos primeiros 100 lançamentos todos os resultados sejam a face “Cara”, não existe nada matematicamente que estabeleça que isso seja impossível. No entanto, no limite do infinito as quantidades iniciais serão “engolidas” pela LGN. Para uma discussão mais aprofundada sobre esse tema recomenda-se uma leitura sobre o Gambler’s Fallacy.\n\n\n\nNo R:\n\n\nMostrar Código\n# Carregar as bibliotecas necessárias\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Definir o número de lançamentos\nn &lt;- 500\n\n# Simular lançamentos de uma moeda honesta\nset.seed(123) # Para reprodutibilidade\nlancamentos &lt;- sample(c(\"Cara\", \"Coroa\"), n, replace = TRUE)\n\n# Calcular a proporção acumulada de caras\ndados &lt;- data.frame(lancamentos) %&gt;%\n  mutate(\n    n = row_number(),\n    proporcao_cara = cumsum(lancamentos == \"Cara\") / n\n  )\n\n# Criar o gráfico\nggplot(dados, aes(x = n, y = proporcao_cara)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Lei dos Grandes Números: Lançamento de uma Moeda Honesta\",\n       x = \"Número de Lançamentos\",\n       y = \"Proporção de Caras\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definindo o número de lançamentos\nn = 500\n\n# Simulando lançamentos de uma moeda honesta\nnp.random.seed(123)  # Para reprodutibilidade\nlancamentos = np.random.choice(['Cara', 'Coroa'], size=n)\n\n# Calculando a proporção acumulada de caras\nproporcao_cara = np.cumsum(lancamentos == 'Cara') / np.arange(1, n + 1)\n\n# Criando o gráfico\nplt.figure(figsize=(6, 4))\nplt.plot(proporcao_cara, color='blue', label='Proporção de Caras')\nplt.axhline(y=0.5, color='red', linestyle='--', label='Proporção Esperada (0.5)')\nplt.title('Lei dos Grandes Números: Lançamento de uma Moeda Honesta')\nplt.xlabel('Número de Lançamentos')\nplt.ylabel('Proporção de Caras')\nplt.legend()\nplt.grid()\nplt.show()",
    "crumbs": [
      "Grandes Amostras",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade_B/LGN.html#lei-forte-dos-grandes-números",
    "href": "Probabilidade_B/LGN.html#lei-forte-dos-grandes-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Forte estabelece que a média amostral converge para \\(\\mu\\) no infinito com probabilidade 1. Ou seja, \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) com probabilidade 1.\nObserve que \\(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu\\) é um evento. Logo, a Lei Forte poderia ser reescrita com uma notação probabilística alternativa:\n\\[P\\left(\\bar{X} \\xrightarrow{n \\rightarrow + \\infty} \\mu \\right) = 1\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge quase certamente para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, no infinito, \\(\\bar{X}\\) e \\(\\mu\\) serão iguais. Ou seja, é uma convergência pontual do valor de uma variável aleatória (a média amostral) para uma constante (a média populacional).\nA prova deste teorema implica em provar que o evento acima é um evento de probabilidade 1, o que exige maior formalidade matemática fazendo uso do Lema de Borel-Cantelli.\nProva. Ver Cáp. 5 de James (2023).\n\\(\\square\\)",
    "crumbs": [
      "Grandes Amostras",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade_B/LGN.html#lei-fraca-dos-grandes-números",
    "href": "Probabilidade_B/LGN.html#lei-fraca-dos-grandes-números",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "A Lei Fraca estabelece que \\(\\forall \\varepsilon &gt; 0\\), então:\n\\[P\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\xrightarrow{n \\rightarrow + \\infty} 0\\]\nEste resultado estabelece que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\).\nProva. A prova faz uso da Desigualdade de Chebyshev:\n\\[\n\\begin{align*}\nP\\left(\\left| \\bar{X} - \\mu \\right| \\ge \\varepsilon \\right) \\overset{\\text{Des. Cheb.}}{\\le} \\frac{Var(\\bar{X})}{\\varepsilon^2} = \\frac{Var\\left(\\frac{\\sum_{i=1}^{n}{X_i}}{n}\\right)}{\\varepsilon^2} \\overset{\\text{Prop. da Constante}}{=} \\frac{Var(\\sum_{i=1}^{n}{X_i})}{n^2\\varepsilon^2} \\overset{\\text{Indep.}}{=} \\\\\n= \\frac{\\sum_{i=1}^{n}{Var(X_i)}}{n^2\\varepsilon^2} \\overset{\\text{Ident. Dist.}}{=} \\frac{n\\sigma^2}{n^2\\varepsilon^2} = \\frac{\\sigma^2}{n\\varepsilon^2} \\xrightarrow{n \\rightarrow + \\infty} 0\n\\end{align*}\n\\]\n\\(\\square\\)\nEste resultado ilustra que \\(\\bar{X}\\) converge em probabilidade para \\(\\mu\\). Podemos interpretar esse resultado como sendo à medida que n cresce, é extremamente improvável que a diferença entre \\(\\bar{X}\\) e \\(\\mu\\) seja maior que \\(\\varepsilon\\).",
    "crumbs": [
      "Grandes Amostras",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade_B/LGN.html#relação-entre-leis-e-convergências",
    "href": "Probabilidade_B/LGN.html#relação-entre-leis-e-convergências",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "É intuitivo pensar afirmarmos que duas quantidades são iguais é mais “forte” do que dizer que essas duas quantidades são “altamente prováveis de estarem próximas”. Ora, se uma coisa é igual à outra, isso implica de que elas também são próximas. Tendo em vista que a Lei Forte dos Grandes Números representa a convergência quase certa e a Lei Fraca representa a convergência em probabilidade, temos que se\n\\[Convergência\\ Quase\\ Certa \\implies Convergência\\ em\\ Probabilidade\\]\nentão,\n\\[Lei\\ Forte \\implies Lei\\ Fraca\\]\nO contrário não vale para nenhuma das duas afirmativas acima.",
    "crumbs": [
      "Grandes Amostras",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade_B/LGN.html#exemplo-o-caso-bernoulli",
    "href": "Probabilidade_B/LGN.html#exemplo-o-caso-bernoulli",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "Suponha uma sequência independente de lançamentos de uma moeda honesta e a variável aleatória \\(X_i = 1\\) se cara e \\(X_i = 0\\) se coroa. Ou seja,\n\\[X_i \\overset{iid}{\\sim}Bernoulli(p)\\]\nonde \\(p = 0,5\\). Então, pela Lei Forte dos Grandes Números:\n\\[\\bar{X} = \\frac{\\sum_{i=1}^{n}{X_i}}{n} \\xrightarrow{n \\rightarrow + \\infty} p\\]\nEste resultado mostra que a medida que ao jogarmos a moeda infinitas vezes, iremos convergir para um cenário em que metade dos lançamentos serão caras e metade serão coroas. Note que o resultado vale quando \\(n \\rightarrow + \\infty\\), ou seja, ele não estabelece nada em uma quantidade finita de lançamentos onde pode existir variabilidade. Por exemplo, mesmo que seja altamente improvável que nos primeiros 100 lançamentos todos os resultados sejam a face “Cara”, não existe nada matematicamente que estabeleça que isso seja impossível. No entanto, no limite do infinito as quantidades iniciais serão “engolidas” pela LGN. Para uma discussão mais aprofundada sobre esse tema recomenda-se uma leitura sobre o Gambler’s Fallacy.",
    "crumbs": [
      "Grandes Amostras",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Probabilidade_B/LGN.html#exemplo-computacional",
    "href": "Probabilidade_B/LGN.html#exemplo-computacional",
    "title": "Lei dos Grandes Números",
    "section": "",
    "text": "No R:\n\n\nMostrar Código\n# Carregar as bibliotecas necessárias\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Definir o número de lançamentos\nn &lt;- 500\n\n# Simular lançamentos de uma moeda honesta\nset.seed(123) # Para reprodutibilidade\nlancamentos &lt;- sample(c(\"Cara\", \"Coroa\"), n, replace = TRUE)\n\n# Calcular a proporção acumulada de caras\ndados &lt;- data.frame(lancamentos) %&gt;%\n  mutate(\n    n = row_number(),\n    proporcao_cara = cumsum(lancamentos == \"Cara\") / n\n  )\n\n# Criar o gráfico\nggplot(dados, aes(x = n, y = proporcao_cara)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Lei dos Grandes Números: Lançamento de uma Moeda Honesta\",\n       x = \"Número de Lançamentos\",\n       y = \"Proporção de Caras\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNo Python:\n\n\nMostrar Código\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definindo o número de lançamentos\nn = 500\n\n# Simulando lançamentos de uma moeda honesta\nnp.random.seed(123)  # Para reprodutibilidade\nlancamentos = np.random.choice(['Cara', 'Coroa'], size=n)\n\n# Calculando a proporção acumulada de caras\nproporcao_cara = np.cumsum(lancamentos == 'Cara') / np.arange(1, n + 1)\n\n# Criando o gráfico\nplt.figure(figsize=(6, 4))\nplt.plot(proporcao_cara, color='blue', label='Proporção de Caras')\nplt.axhline(y=0.5, color='red', linestyle='--', label='Proporção Esperada (0.5)')\nplt.title('Lei dos Grandes Números: Lançamento de uma Moeda Honesta')\nplt.xlabel('Número de Lançamentos')\nplt.ylabel('Proporção de Caras')\nplt.legend()\nplt.grid()\nplt.show()",
    "crumbs": [
      "Grandes Amostras",
      "Lei dos Grandes Números"
    ]
  },
  {
    "objectID": "Int_Prob/experimento_aleatorio.html",
    "href": "Int_Prob/experimento_aleatorio.html",
    "title": "Experimento Aleatório, Espaço Amostral e Eventos",
    "section": "",
    "text": "Experimentos aleatórios são situação na natureza que envolvem incertezas. A busca por avaliar as diversas probabilidades de ocorrência é um dos objetivos no estudo desses fenômenos.\nO espaço amostral é o conjunto de todos os possíveis resultados de um experimento aleatório e é representado por \\(\\Omega\\). Cada resultado possível é denominado ponto ou elemento de \\(\\Omega\\) e denotado por \\(\\omega\\). Assim, escrevemos \\(\\omega \\in \\Omega\\) para indicar que \\(\\omega\\) está em \\(\\Omega\\). O conjunto sem elementos é o conjunto vazio, denotado por \\(\\emptyset\\). Muitos autores (Magalhães (2006), James (2023), DeGroot e Schervish (2012)), denominam eventos (ou conjunto) como sendo letras maiúsculas do alfabeto, tais como A, B, etc. Assim:\nDefinição Seja \\(\\Omega\\) o espaço amostral do experimento. Todo subconjunto \\(A \\in \\Omega\\) será chamado evento. \\(\\Omega\\) é o evento certo, \\(\\emptyset\\) o evento impossível. Se \\(\\omega \\in \\Omega\\), o evento \\(\\{\\omega\\}\\) é dito elementar (ou simples).\n\n\n\n\n\n\\(A^c\\) (ou \\(\\bar{A}\\)) é o complementar de \\(A\\), isto é, todos os elementos de \\(\\Omega\\) exceto os de \\(A\\)\n\\(A_1 \\cup A_2 ... \\cup A_n\\) ou \\(\\bigcup\\limits_{i=1}^{n}A_i\\) é a união de \\(A_1, A_2, ..., A_n\\) e representa os pontos de \\(\\Omega\\) que pertencem à pelo menos um \\(A_i\\)\n\\(A_1 \\cap A_2 ... \\cap A_n\\) ou \\(\\bigcap\\limits_{i=1}^{n}A_i\\) é a intersecção de \\(A_1, A_2, ..., A_n\\) e representa os pontos de \\(\\Omega\\) que pertencem simultaneamente à todos os \\(A_i\\)’s.\n\\(A-B\\) é a diferença entre \\(A\\) e \\(B\\), isto é, todos os elementos de \\(A\\), exceto os que estão em \\(B\\). Outra forma de notação é \\(A \\cap B^c\\), pois \\(A-B = A \\cap B^c\\)\nSe todo o elemento de \\(A\\) é também um elemento de \\(B\\), então \\(A\\) é definido como um subconjunto de \\(B\\) e escrevemos \\(A \\subset B\\) ou \\(B \\supset A\\). Interpretamos como sendo “\\(A\\) está contido em \\(B\\)” ou “\\(B\\) contém \\(A\\)”\nDois conjuntos são disjuntos ou mutuamente excludentes se a sua interseção é vazio. Assim, \\(A\\ e\\ B\\ disjuntos \\iff A \\cap B = \\emptyset\\)\n\n\n\n\nAs Leis de DeMorgan são teoremas fundamentais que descrevem relações entre a união e interesecção entre conjuntos e de seus complementos.\nO caso simples pode ser descrito abaixo:\n\\[\n\\begin{align*}\n(A \\cup B)^c = A^c \\cap B^c \\\\\n(A \\cap B)^c = A^c \\cup B^c\n\\end{align*}\n\\]\nImagine as duas frases:\n\nA: “Está chovendo”\nB: “Está frio”\n\nPortanto:\n\nA negação de “Está chovendo e está frio” (\\(A \\cap B\\)) é “Não está chovendo ou não está frio” (\\(A^c \\cup B^c\\));\nA negação de “Está chovendo ou está frio” (\\(A \\cap B\\)) é “Não está chovendo e não está frio” (\\(A^c \\cap B^c\\)).\n\nCaso geral:\n\\[\n\\begin{align*}\n\\left( \\bigcup\\limits_{i=1}^{n} A_i \\right)^c = \\bigcap\\limits_{i=1}^{n} A_i^c \\\\\n\\left( \\bigcap\\limits_{i=1}^{n} A_i \\right)^c = \\bigcup\\limits_{i=1}^{n} A_i^c\n\\end{align*}\n\\]\nVale ressaltar também que essa relação vale também quando \\(n = \\infty\\), ou seja:\n\\[\n\\begin{align*}\n\\left( \\bigcup\\limits_{i=1}^{\\infty} A_i \\right)^c = \\bigcap\\limits_{i=1}^{\\infty} A_i^c \\\\\n\\left( \\bigcap\\limits_{i=1}^{\\infty} A_i \\right)^c = \\bigcup\\limits_{i=1}^{\\infty} A_i^c\n\\end{align*}\n\\]\n\n\n\nUma maneiras mais intuitivas de visualizar essas relações entre conjuntos é via Diagramas de Venn. Abaixo, podemos ver algumas das relações acima (incluindo DeMorgan):\n\n\n\nExtraído do DeGroot e Schervish (2012)",
    "crumbs": [
      "Conceitos Básicos",
      "Experimento Aleatório, Espaço Amostral e Eventos"
    ]
  },
  {
    "objectID": "Int_Prob/experimento_aleatorio.html#operações-básicas",
    "href": "Int_Prob/experimento_aleatorio.html#operações-básicas",
    "title": "Experimento Aleatório, Espaço Amostral e Eventos",
    "section": "",
    "text": "\\(A^c\\) é o complementar de \\(A\\), isto é, todos os elementos de \\(\\Omega\\) exceto os de \\(A\\)\n\\(A_1 \\cup A_2 ... \\cup A_n\\) ou \\(\\bigcup_{i=1}^n\\)",
    "crumbs": [
      "Conceitos Básicos",
      "Experimento Aleatório, Espaço Amostral e Eventos"
    ]
  },
  {
    "objectID": "Int_Prob/experimento_aleatorio.html#operações-básicas-e-definições",
    "href": "Int_Prob/experimento_aleatorio.html#operações-básicas-e-definições",
    "title": "Experimento Aleatório, Espaço Amostral e Eventos",
    "section": "",
    "text": "\\(A^c\\) é o complementar de \\(A\\), isto é, todos os elementos de \\(\\Omega\\) exceto os de \\(A\\)\n\\(A_1 \\cup A_2 ... \\cup A_n\\) ou \\(\\bigcup\\limits_{i=1}^{n}\\) é a união de \\(A_1, A_2, ..., A_n\\) e representa os pontos de \\(\\Omega\\) que pertencem à pelo menos um \\(A_i\\)\n\\(A_1 \\cap A_2 ... \\cap A_n\\) ou \\(\\bigcap\\limits_{i=1}^{n}\\) é a intersecção de \\(A_1, A_2, ..., A_n\\) e representa os pontos de \\(\\Omega\\) que pertencem simultaneamente à todos os \\(A_i\\)’s.\n\\(A-B\\) é a diferença entre \\(A\\) e \\(B\\), isto é, todos os elementos de \\(A\\), exceto os que estão em \\(B\\). Outra forma de notação é \\(A \\cap B^c\\), pois \\(A-B = A \\cap B^c\\)\nSe todo o elemento de \\(A\\) é também um elemento de \\(B\\), então \\(A\\) é definido como um subconjunto de \\(B\\) e escrevemos \\(A \\subset B\\) ou \\(B \\supset A\\). Interpretamos como sendo “\\(A\\) está contido em \\(B\\)” ou “\\(B\\) contém \\(A\\)”",
    "crumbs": [
      "Conceitos Básicos",
      "Experimento Aleatório, Espaço Amostral e Eventos"
    ]
  },
  {
    "objectID": "Int_Prob/experimento_aleatorio.html#eventosconjuntos",
    "href": "Int_Prob/experimento_aleatorio.html#eventosconjuntos",
    "title": "Experimento Aleatório, Espaço Amostral e Eventos",
    "section": "",
    "text": "\\(A^c\\) (ou \\(\\bar{A}\\)) é o complementar de \\(A\\), isto é, todos os elementos de \\(\\Omega\\) exceto os de \\(A\\)\n\\(A_1 \\cup A_2 ... \\cup A_n\\) ou \\(\\bigcup\\limits_{i=1}^{n}A_i\\) é a união de \\(A_1, A_2, ..., A_n\\) e representa os pontos de \\(\\Omega\\) que pertencem à pelo menos um \\(A_i\\)\n\\(A_1 \\cap A_2 ... \\cap A_n\\) ou \\(\\bigcap\\limits_{i=1}^{n}A_i\\) é a intersecção de \\(A_1, A_2, ..., A_n\\) e representa os pontos de \\(\\Omega\\) que pertencem simultaneamente à todos os \\(A_i\\)’s.\n\\(A-B\\) é a diferença entre \\(A\\) e \\(B\\), isto é, todos os elementos de \\(A\\), exceto os que estão em \\(B\\). Outra forma de notação é \\(A \\cap B^c\\), pois \\(A-B = A \\cap B^c\\)\nSe todo o elemento de \\(A\\) é também um elemento de \\(B\\), então \\(A\\) é definido como um subconjunto de \\(B\\) e escrevemos \\(A \\subset B\\) ou \\(B \\supset A\\). Interpretamos como sendo “\\(A\\) está contido em \\(B\\)” ou “\\(B\\) contém \\(A\\)”\nDois conjuntos são disjuntos ou mutuamente excludentes se a sua interseção é vazio. Assim, \\(A\\ e\\ B\\ disjuntos \\iff A \\cap B = \\emptyset\\)\n\n\n\n\nAs Leis de DeMorgan são teoremas fundamentais que descrevem relações entre a união e interesecção entre conjuntos e de seus complementos.\nO caso simples pode ser descrito abaixo:\n\\[\n\\begin{align*}\n(A \\cup B)^c = A^c \\cap B^c \\\\\n(A \\cap B)^c = A^c \\cup B^c\n\\end{align*}\n\\]\nImagine as duas frases:\n\nA: “Está chovendo”\nB: “Está frio”\n\nPortanto:\n\nA negação de “Está chovendo e está frio” (\\(A \\cap B\\)) é “Não está chovendo ou não está frio” (\\(A^c \\cup B^c\\));\nA negação de “Está chovendo ou está frio” (\\(A \\cap B\\)) é “Não está chovendo e não está frio” (\\(A^c \\cap B^c\\)).\n\nCaso geral:\n\\[\n\\begin{align*}\n\\left( \\bigcup\\limits_{i=1}^{n} A_i \\right)^c = \\bigcap\\limits_{i=1}^{n} A_i^c \\\\\n\\left( \\bigcap\\limits_{i=1}^{n} A_i \\right)^c = \\bigcup\\limits_{i=1}^{n} A_i^c\n\\end{align*}\n\\]\nVale ressaltar também que essa relação vale também quando \\(n = \\infty\\), ou seja:\n\\[\n\\begin{align*}\n\\left( \\bigcup\\limits_{i=1}^{\\infty} A_i \\right)^c = \\bigcap\\limits_{i=1}^{\\infty} A_i^c \\\\\n\\left( \\bigcap\\limits_{i=1}^{\\infty} A_i \\right)^c = \\bigcup\\limits_{i=1}^{\\infty} A_i^c\n\\end{align*}\n\\]\n\n\n\nUma maneiras mais intuitivas de visualizar essas relações entre conjuntos é via Diagramas de Venn. Abaixo, podemos ver algumas das relações acima (incluindo DeMorgan):\n\n\n\nExtraído do DeGroot e Schervish (2012)",
    "crumbs": [
      "Conceitos Básicos",
      "Experimento Aleatório, Espaço Amostral e Eventos"
    ]
  },
  {
    "objectID": "Int_Prob/conceitos_de_probabilidade.html",
    "href": "Int_Prob/conceitos_de_probabilidade.html",
    "title": "Conceitos de Probabilidade",
    "section": "",
    "text": "Segundo Magalhães (2006), a definição clássica de probabilidade se refere à subconjuntos unitários equiprováveis. No caso enumerável finito temos:\n\\[\nP(A) = \\frac{Número\\ de\\ Elementos\\ em\\ A}{Número\\ total\\ de\\ Elementos\\ em\\ \\Omega}\n\\]\nSe \\(\\Omega\\) tiver uma quantidade infinita de elementos, precisamos tratar a a definição acima com o uso de limites.\n\n\n\nQuando \\(\\Omega\\) é não enumerável, o conceito se aplicará ao comprimento de intervalos, medidas de áreas ou similares, dando origem ao que chamamos de probabilidade geométrica. Por exemplo, se \\(\\Omega\\) é um intervalo onde \\(\\Omega \\in \\mathbb{R}\\), então:\n\\[\nP(A) = \\frac{Comprimento\\ de\\ A}{Comprimento\\ de\\ \\Omega}\n\\]\n\n\n\nA definição frequentista considera o limite de frequências relativas como o valor da probabilidade. Para tal, seja \\(n_{A}\\) o número de ocorrências de \\(A\\) em \\(n\\) repetições independentes do experimento em questão. Assim,\n\\[\nP(A) = \\lim_{n\\rightarrow{+\\infty}} \\frac{n_A}{n}\n\\]\n\n\n\nAs definições anteriores não são suficientes para uma formulação matemática mais rigorosa da probabilidade. Assim, na primeira metade do século XX, Kolmogorov (1933) apresentou um conjunto de axiomas matemáticos para definir probabilidade e, assim, pavimentando toda a evolução da probabilidade moderna.\nÉ importante, e ao mesmo tempo fascinante, destacar que apenas com esses três axiomas todos os teoremas da Teoria da Probabilidade são viabilizados. Assim, podemos destacar que toda a evolução da probabilidade foi pavimentada pelo trabalho de Kolmogorov (1933).\nDefinição axiomática\nAssumindo que podemos atribuir um número real \\(P(A)\\) para um evento \\(A\\):\n\nAxioma 1. \\(P(A) \\ge 0\\)\nAxioma 2. \\(P(\\Omega) = 1\\)\nAxioma 3. Se \\(A_1, A_2, ..., A_n\\) são disjuntos (2 a 2) (isto é, mutuamente excludentes), então\n\n\\[\nP\\left( \\bigcup_{k=1}^{n} A_k \\right) = \\sum_{k=1}^{n} P (A_k)\n\\]\n(Os eventos são disjuntos 2 a 2, se \\(A_i \\cap A_j = \\emptyset, \\forall i \\ne j\\))\nUma discussão mais aprofundada desses axiomas, incluindo uma versão mais conveniente para o Axioma 3 (para o caso \\(n = \\infty\\)) e uma “Axioma 4” de continuidade no vazio, que pode ser derivado dos outros axiomas, é discutida no Capítulo 1 de James (2023).\n\n\nComo comentado, os Axiomas de Kolmogorov sustentam toda a teoria da probabilidade e alguns resultados diretos são:\n\n\\(P(\\emptyset) = 0\\).\n\nProva.\n\\[\n\\begin{align*}\nP(A \\cup \\emptyset) &= P(A) + P(\\emptyset) \\\\\nP(A) &= P(A) + P(\\emptyset) \\\\\nP(\\emptyset) &= 0\n\\end{align*}\n\\]\n\nPara qualquer evento \\(A\\), \\(P(A^c) = 1-P(A)\\).\n\nProva.\n\\[\n\\begin{align*}\nP(\\Omega) &= P(A \\cup A^c) = P(A) + P(A^c) \\\\\n1 &= P(A) + P(A^c) \\overset{\\text{Isolando}}{\\implies} P(A^c) = 1-P(A)\n\\end{align*}\n\\]\n\nSe \\(A \\subset B\\), então \\(P(A) \\le P(B)\\).\n\nProva.\n\\[\n\\begin{align*}\nP(B) &= P(A \\cup (B \\cap A^c)) = P(A) + P(B \\cap A^c) \\\\\nP(B) &\\ge P(A)\n\\end{align*}\n\\]\n\nPara qualquer evento \\(A\\), \\(0 \\le P(A) \\le 1\\).\n\nProva. Como \\(A \\subset \\Omega\\), então \\(0 \\le P(A) \\le P(\\Omega) = 1\\)\n\nPara quaisquer eventos \\(A\\) e \\(B\\), \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) (Esta relação é mais fácil de visualizar via Diagrama de Venn)\n\nProva.\nPrimeiro, vamos escrever \\(P(A)\\) e \\(P(B)\\) como a soma de suas partes:\n\\[\nP(A) = P(A \\cap B^c) + P(A \\cap B) \\\\\nP(B) = P(B \\cap A^c) + P(B \\cap A)\n\\]\nAgora, vamos escrever a união como a soma de suas partes:\n\\[\nP(A \\cup B) = P(A \\cap B^c) + P(A \\cap B) + P(B \\cap A^c)\n\\]\nAgora, somando e substraindo \\(P(A \\cap B)\\):\n…\n\n\n\n\nUm Espaço de Probabilidade é um constructo que provê um modelo formal de um processo aleatório. Ele é composto por uma tríplice de valores \\((\\Omega, \\mathcal{F}, P)\\) onde\n\n\\(\\Omega\\) é todo o espaço amostral\n\\(\\mathcal{F}\\) (ou \\(\\sigma\\)-álgebra) é o espaço de eventos (domínio)\n\\(P\\) é uma função de probabilidade produz valores no intervalo \\([0,1]\\)\n\nPara um entendimento mais consistente da importância da definição de um espaço de probabilidade, bem como da importância da definição de uma \\(\\sigma\\)-álgebra, que é o domínio do espaço, recomenda-se a leitura do Capítulo 1 de James (2023), Capítulo 1 de Magalhães (2006), Capítulo 2B e 12 de Axler (2020).1 No entanto, como esta discussão é mais aprofundada, para uma visão mais geral recomendo fortemente o leitor a assitir o vídeo Bertrand’s Paradox (with 3blue1brown) - Numberphile (explicação do Exemplo 1.8 de Magalhães (2006)) para esclarecer qual o efeito da definição de diferentes espaços de probabiidade para um mesmo problema e assim produzir probabilidades significativamente diferentes dependendo da maneira como são definidos \\(\\Omega\\) e \\(\\sigma\\)-álgebra e, assim, produzindo diferentes processos geradores de eventos.\nalguns conceitos de Teoria da Medida aditividade contável\nfalr video do veritaium\nhttps://en.wikipedia.org/wiki/Banach%E2%80%93Tarski_paradox Vitali Set Axioma da escolha",
    "crumbs": [
      "Conceitos Básicos",
      "Conceitos de Probabilidade"
    ]
  },
  {
    "objectID": "Int_Prob/conceitos_de_probabilidade.html#probabilidade-axiomática",
    "href": "Int_Prob/conceitos_de_probabilidade.html#probabilidade-axiomática",
    "title": "Conceitos de Probabilidade",
    "section": "",
    "text": "As definições anteriores não são suficientes para uma formulação matemática mais rigorosa da probabilidade. Assim, na primeira metade do século XX, Kolmogorov (1933) apresentou um conjunto de axiomas matemáticos para definir probabilidade e, assim, pavimentando toda a evolução da probabilidade moderna.\nÉ importante, e ao mesmo tempo fascinante, destacar que apenas com esses três axiomas todos os teoremas da Teoria da Probabilidade são viabilizados. Assim, podemos destacar que toda a evolução da probabilidade foi pavimentada pelo trabalho de Kolmogorov (1933).\nDefinição axiomática\nAssumindo que podemos atribuir um número real \\(P(A)\\) para um evento \\(A\\):\n\nAxioma 1. \\(P(A) \\ge 0\\)\nAxioma 2. \\(P(\\Omega) = 1\\)\nAxioma 3. Se \\(A_1, A_2, ..., A_n\\) são disjuntos (2 a 2) (isto é, mutuamente excludentes), então\n\n\\[\nP\\left( \\bigcup_{k=1}^{n} A_k \\right) = \\sum_{k=1}^{n} P (A_k)\n\\]\n(Os eventos são disjuntos 2 a 2, se \\(A_i \\cap A_j = \\emptyset, \\forall i \\ne j\\))\nUma discussão mais aprofundada desses axiomas, incluindo uma versão mais conveniente para o Axioma 3 (para o caso \\(n = \\infty\\)) e uma “Axioma 4” de continuidade no vazio, que pode ser derivado dos outros axiomas, é discutida no Capítulo 1 de James (2023).\n\n\nComo comentado, os Axiomas de Kolmogorov sustentam toda a teoria da probabilidade e alguns resultados diretos são:\n\n\\(P(\\emptyset) = 0\\).\n\nProva.\n\\[\n\\begin{align*}\nP(A \\cup \\emptyset) &= P(A) + P(\\emptyset) \\\\\nP(A) &= P(A) + P(\\emptyset) \\\\\nP(\\emptyset) &= 0\n\\end{align*}\n\\]\n\nPara qualquer evento \\(A\\), \\(P(A^c) = 1-P(A)\\).\n\nProva.\n\\[\n\\begin{align*}\nP(\\Omega) &= P(A \\cup A^c) = P(A) + P(A^c) \\\\\n1 &= P(A) + P(A^c) \\overset{\\text{Isolando}}{\\implies} P(A^c) = 1-P(A)\n\\end{align*}\n\\]\n\nSe \\(A \\subset B\\), então \\(P(A) \\le P(B)\\).\n\nProva.\n\\[\n\\begin{align*}\nP(B) &= P(A \\cup (B \\cap A^c)) = P(A) + P(B \\cap A^c) \\\\\nP(B) &\\ge P(A)\n\\end{align*}\n\\]\n\nPara qualquer evento \\(A\\), \\(0 \\le P(A) \\le 1\\).\n\nProva. Como \\(A \\subset \\Omega\\), então \\(0 \\le P(A) \\le P(\\Omega) = 1\\)\n\nPara quaisquer eventos \\(A\\) e \\(B\\), \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) (Esta relação é mais fácil de visualizar via Diagrama de Venn)\n\nProva.\nPrimeiro, vamos escrever \\(P(A)\\) e \\(P(B)\\) como a soma de suas partes:\n\\[\nP(A) = P(A \\cap B^c) + P(A \\cap B) \\\\\nP(B) = P(B \\cap A^c) + P(B \\cap A)\n\\]\nAgora, vamos escrever a união como a soma de suas partes:\n\\[\nP(A \\cup B) = P(A \\cap B^c) + P(A \\cap B) + P(B \\cap A^c)\n\\]\nAgora, somando e substraindo \\(P(A \\cap B)\\):\n…",
    "crumbs": [
      "Conceitos Básicos",
      "Conceitos de Probabilidade"
    ]
  },
  {
    "objectID": "Int_Prob/conceitos_de_probabilidade.html#definição-clássica-omega-enumerável",
    "href": "Int_Prob/conceitos_de_probabilidade.html#definição-clássica-omega-enumerável",
    "title": "Conceitos de Probabilidade",
    "section": "",
    "text": "Segundo Magalhães (2006), a definição clássica de probabilidade se refere à subconjuntos unitários equiprováveis. No caso enumerável finito temos:\n\\[\nP(A) = \\frac{Número\\ de\\ Elementos\\ em\\ A}{Número\\ total\\ de\\ Elementos\\ em\\ \\Omega}\n\\]\nSe \\(\\Omega\\) tiver uma quantidade infinita de elementos, precisamos tratar a a definição acima com o uso de limites.",
    "crumbs": [
      "Conceitos Básicos",
      "Conceitos de Probabilidade"
    ]
  },
  {
    "objectID": "Int_Prob/conceitos_de_probabilidade.html#definição-clássica-geométrica-omega-não-enumerável",
    "href": "Int_Prob/conceitos_de_probabilidade.html#definição-clássica-geométrica-omega-não-enumerável",
    "title": "Conceitos de Probabilidade",
    "section": "",
    "text": "Quando \\(\\Omega\\) é não enumerável, o conceito se aplicará ao comprimento de intervalos, medidas de áreas ou similares, dando origem ao que chamamos de probabilidade geométrica. Por exemplo, se \\(\\Omega\\) é um intervalo onde \\(\\Omega \\in \\mathbb{R}\\), então:\n\\[\nP(A) = \\frac{Comprimento\\ de\\ A}{Comprimento\\ de\\ \\Omega}\n\\]",
    "crumbs": [
      "Conceitos Básicos",
      "Conceitos de Probabilidade"
    ]
  },
  {
    "objectID": "Int_Prob/conceitos_de_probabilidade.html#espaços-de-probabilidade",
    "href": "Int_Prob/conceitos_de_probabilidade.html#espaços-de-probabilidade",
    "title": "Conceitos de Probabilidade",
    "section": "",
    "text": "Um Espaço de Probabilidade é um constructo que provê um modelo formal de um processo aleatório. Ele é composto por uma tríplice de valores \\((\\Omega, \\mathcal{F}, P)\\) onde\n\n\\(\\Omega\\) é todo o espaço amostral\n\\(\\mathcal{F}\\) (ou \\(\\sigma\\)-álgebra) é o espaço de eventos (domínio)\n\\(P\\) é uma função de probabilidade produz valores no intervalo \\([0,1]\\)\n\nPara um entendimento mais consistente da importância da definição de um espaço de probabilidade, bem como da importância da definição de uma \\(\\sigma\\)-álgebra, que é o domínio do espaço, recomenda-se a leitura do Capítulo 1 de James (2023), Capítulo 1 de Magalhães (2006), Capítulo 2B e 12 de Axler (2020).1 No entanto, como esta discussão é mais aprofundada, para uma visão mais geral recomendo fortemente o leitor a assitir o vídeo Bertrand’s Paradox (with 3blue1brown) - Numberphile (explicação do Exemplo 1.8 de Magalhães (2006)) para esclarecer qual o efeito da definição de diferentes espaços de probabiidade para um mesmo problema e assim produzir probabilidades significativamente diferentes dependendo da maneira como são definidos \\(\\Omega\\) e \\(\\sigma\\)-álgebra e, assim, produzindo diferentes processos geradores de eventos.\nalguns conceitos de Teoria da Medida aditividade contável\nfalr video do veritaium\nhttps://en.wikipedia.org/wiki/Banach%E2%80%93Tarski_paradox Vitali Set Axioma da escolha",
    "crumbs": [
      "Conceitos Básicos",
      "Conceitos de Probabilidade"
    ]
  },
  {
    "objectID": "Int_Prob/conceitos_de_probabilidade.html#definição-frequentista",
    "href": "Int_Prob/conceitos_de_probabilidade.html#definição-frequentista",
    "title": "Conceitos de Probabilidade",
    "section": "",
    "text": "A definição frequentista considera o limite de frequências relativas como o valor da probabilidade. Para tal, seja \\(n_{A}\\) o número de ocorrências de \\(A\\) em \\(n\\) repetições independentes do experimento em questão. Assim,\n\\[\nP(A) = \\lim_{n\\rightarrow{+\\infty}} \\frac{n_A}{n}\n\\]",
    "crumbs": [
      "Conceitos Básicos",
      "Conceitos de Probabilidade"
    ]
  },
  {
    "objectID": "Int_Prob/conceitos_de_probabilidade.html#footnotes",
    "href": "Int_Prob/conceitos_de_probabilidade.html#footnotes",
    "title": "Conceitos de Probabilidade",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nÉ possível que esses conceitos sejam mais difíceis de assimilar do ponto de vista prático. No entanto, eles são fundamentais para garantir uma base matemática sólida para toda a teoria da probabilidade. As propriedades desejáveis de uma \\(\\sigma\\)-álgebra garantem que o espaço de probabilidade é um espaço mensurável. Por exemplo, uma das condições é a aditividade contável (isto é, se \\(A_1\\) e \\(A_2\\) são dois eventos sendo \\(A_1 \\in \\mathcal{F}\\) e \\(A_2 \\in \\mathcal{F}\\), então \\((A_1 \\cup A_2) \\in \\mathcal{F}\\)). Se essa condição não fosse satisfeita, seria possível aplicar o terceiro axioma de Kolmogorov e chegar em inconsistências como probabilidades maior que 1. Por exemplo, suponha \\(\\mathcal{F} = \\{ \\emptyset, \\Omega, \\{1\\}, \\{2\\}, \\{1,3\\}, \\{2,3\\} \\}\\) não é uma \\(\\sigma\\)-álgebra pois \\(\\{1, 2\\} \\notin \\mathcal{F}\\) e, portanto, \\(P(\\{1, 2\\})\\) é indefinido. Supondo, por exemplo que \\(P(\\{1\\}) = 0.5\\) e \\(P(\\{2\\}) = 0.7\\) Se quiséssemos aplicar o terceiro axioma de Kolmogorov concluiríamos, erroneamente, que \\(P(\\{1, 2\\}) = 1.2 &gt; 1! (Absurdo!)\\). Essa discussão possui relação com outros conceitos matemáticos como Aditividade Contável de uma \\(\\Omega\\)-álgebra, o Axioma da Escolha e o Paradoxo de Banach-Tarski. ↩︎",
    "crumbs": [
      "Conceitos Básicos",
      "Conceitos de Probabilidade"
    ]
  }
]